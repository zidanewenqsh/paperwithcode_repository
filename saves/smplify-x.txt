D:\Projects\smplify-x\collect_code.py
from pathlib import Path

def collect_and_write_files(paths, extensions, output_path):
    """
    遍历给定的路径列表，找到所有扩展名在extensions中的文件，并将它们的内容保存到指定的输出文件中。
    
    参数:
    paths: 文件或文件夹的路径列表。
    extensions: 接受的文件后缀名列表。
    output_path: 输出文件的路径。
    """
    script_path = Path(__file__).resolve()  # 获取当前脚本的绝对路径
    def collect_files(path):
        # 递归收集符合条件的文件
        for p in path.iterdir():
            if p.is_dir():
                collect_files(p)
            elif p.suffix in extensions and p.resolve() != script_path:
                files_to_save.append(p)
    
    files_to_save = []
    output_file_path = Path(output_path)
    
    # 确保输出文件的目录存在
    output_file_path.parent.mkdir(parents=True, exist_ok=True)
    
    # 遍历所有路径，收集文件
    for path_string in paths:
        path = Path(path_string)
        if path.exists():
            if path.is_dir():
                collect_files(path)
            elif path.suffix in extensions:
                files_to_save.append(path)
    
    # 将文件内容写入输出文件
    with output_file_path.open('w', encoding='utf-8') as f:
        for file_path in files_to_save:
            # 写入文件的绝对路径
            f.write(f"{file_path.resolve()}\n")
            # 写入文件内容
            with file_path.open('r', encoding='utf-8') as file:
                f.write(file.read())
                f.write("\n\n")  # 在文件内容之间加入空行以区分不同的文件内容

# 使用示例
paths = [r"D:\Projects\smplify-x\cfg_files", r"D:\Projects\smplify-x\smplifyx"]
extensions = ['.py', '.ipynb', '.md', '.yaml', '.txt']
output_path = 'smplifyx_code.txt'

collect_and_write_files(paths, extensions, output_path)


D:\Projects\smplify-x\main.py
# -*- coding: utf-8 -*-

# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is
# holder of all proprietary rights on this computer program.
# You can only use this computer program if you have closed
# a license agreement with MPG or you get the right to use the computer
# program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and
# liable to prosecution.
#
# Copyright©2019 Max-Planck-Gesellschaft zur Förderung
# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute
# for Intelligent Systems and the Max Planck Institute for Biological
# Cybernetics. All rights reserved.
#
# Contact: ps-license@tuebingen.mpg.de

from __future__ import absolute_import
from __future__ import print_function
from __future__ import division

import sys
import os

import os.path as osp

import time
import yaml
import torch

import smplx

from utils import JointMapper
from cmd_parser import parse_config
from data_parser import create_dataset
from fit_single_frame import fit_single_frame

from camera import create_camera
from prior import create_prior

torch.backends.cudnn.enabled = False


def main(**args):
    output_folder = args.pop('output_folder')
    output_folder = osp.expandvars(output_folder)
    if not osp.exists(output_folder):
        os.makedirs(output_folder)

    # Store the arguments for the current experiment
    conf_fn = osp.join(output_folder, 'conf.yaml')
    with open(conf_fn, 'w') as conf_file:
        yaml.dump(args, conf_file)

    result_folder = args.pop('result_folder', 'results')
    result_folder = osp.join(output_folder, result_folder)
    if not osp.exists(result_folder):
        os.makedirs(result_folder)

    mesh_folder = args.pop('mesh_folder', 'meshes')
    mesh_folder = osp.join(output_folder, mesh_folder)
    if not osp.exists(mesh_folder):
        os.makedirs(mesh_folder)

    out_img_folder = osp.join(output_folder, 'images')
    if not osp.exists(out_img_folder):
        os.makedirs(out_img_folder)

    float_dtype = args['float_dtype']
    if float_dtype == 'float64':
        dtype = torch.float64
    elif float_dtype == 'float32':
        dtype = torch.float64
    else:
        print('Unknown float type {}, exiting!'.format(float_dtype))
        sys.exit(-1)

    use_cuda = args.get('use_cuda', True)
    if use_cuda and not torch.cuda.is_available():
        print('CUDA is not available, exiting!')
        sys.exit(-1)

    img_folder = args.pop('img_folder', 'images')
    dataset_obj = create_dataset(img_folder=img_folder, **args)

    start = time.time()

    input_gender = args.pop('gender', 'neutral')
    gender_lbl_type = args.pop('gender_lbl_type', 'none')
    max_persons = args.pop('max_persons', -1)

    float_dtype = args.get('float_dtype', 'float32')
    if float_dtype == 'float64':
        dtype = torch.float64
    elif float_dtype == 'float32':
        dtype = torch.float32
    else:
        raise ValueError('Unknown float type {}, exiting!'.format(float_dtype))

    joint_mapper = JointMapper(dataset_obj.get_model2data())

    model_params = dict(model_path=args.get('model_folder'),
                        joint_mapper=joint_mapper,
                        create_global_orient=True,
                        create_body_pose=not args.get('use_vposer'),
                        create_betas=True,
                        create_left_hand_pose=True,
                        create_right_hand_pose=True,
                        create_expression=True,
                        create_jaw_pose=True,
                        create_leye_pose=True,
                        create_reye_pose=True,
                        create_transl=False,
                        dtype=dtype,
                        **args)

    male_model = smplx.create(gender='male', **model_params)
    # SMPL-H has no gender-neutral model
    if args.get('model_type') != 'smplh':
        neutral_model = smplx.create(gender='neutral', **model_params)
    female_model = smplx.create(gender='female', **model_params)

    # Create the camera object
    focal_length = args.get('focal_length')
    camera = create_camera(focal_length_x=focal_length,
                           focal_length_y=focal_length,
                           dtype=dtype,
                           **args)

    if hasattr(camera, 'rotation'):
        camera.rotation.requires_grad = False

    use_hands = args.get('use_hands', True)
    use_face = args.get('use_face', True)

    body_pose_prior = create_prior(
        prior_type=args.get('body_prior_type'),
        dtype=dtype,
        **args)

    jaw_prior, expr_prior = None, None
    if use_face:
        jaw_prior = create_prior(
            prior_type=args.get('jaw_prior_type'),
            dtype=dtype,
            **args)
        expr_prior = create_prior(
            prior_type=args.get('expr_prior_type', 'l2'),
            dtype=dtype, **args)

    left_hand_prior, right_hand_prior = None, None
    if use_hands:
        lhand_args = args.copy()
        lhand_args['num_gaussians'] = args.get('num_pca_comps')
        left_hand_prior = create_prior(
            prior_type=args.get('left_hand_prior_type'),
            dtype=dtype,
            use_left_hand=True,
            **lhand_args)

        rhand_args = args.copy()
        rhand_args['num_gaussians'] = args.get('num_pca_comps')
        right_hand_prior = create_prior(
            prior_type=args.get('right_hand_prior_type'),
            dtype=dtype,
            use_right_hand=True,
            **rhand_args)

    shape_prior = create_prior(
        prior_type=args.get('shape_prior_type', 'l2'),
        dtype=dtype, **args)

    angle_prior = create_prior(prior_type='angle', dtype=dtype)

    if use_cuda and torch.cuda.is_available():
        device = torch.device('cuda')

        camera = camera.to(device=device)
        female_model = female_model.to(device=device)
        male_model = male_model.to(device=device)
        if args.get('model_type') != 'smplh':
            neutral_model = neutral_model.to(device=device)
        body_pose_prior = body_pose_prior.to(device=device)
        angle_prior = angle_prior.to(device=device)
        shape_prior = shape_prior.to(device=device)
        if use_face:
            expr_prior = expr_prior.to(device=device)
            jaw_prior = jaw_prior.to(device=device)
        if use_hands:
            left_hand_prior = left_hand_prior.to(device=device)
            right_hand_prior = right_hand_prior.to(device=device)
    else:
        device = torch.device('cpu')

    # A weight for every joint of the model
    joint_weights = dataset_obj.get_joint_weights().to(device=device,
                                                       dtype=dtype)
    # Add a fake batch dimension for broadcasting
    joint_weights.unsqueeze_(dim=0)

    for idx, data in enumerate(dataset_obj):

        img = data['img']
        fn = data['fn']
        keypoints = data['keypoints']
        print('Processing: {}'.format(data['img_path']))

        curr_result_folder = osp.join(result_folder, fn)
        if not osp.exists(curr_result_folder):
            os.makedirs(curr_result_folder)
        curr_mesh_folder = osp.join(mesh_folder, fn)
        if not osp.exists(curr_mesh_folder):
            os.makedirs(curr_mesh_folder)
        for person_id in range(keypoints.shape[0]):
            if person_id >= max_persons and max_persons > 0:
                continue

            curr_result_fn = osp.join(curr_result_folder,
                                      '{:03d}.pkl'.format(person_id))
            curr_mesh_fn = osp.join(curr_mesh_folder,
                                    '{:03d}.obj'.format(person_id))

            curr_img_folder = osp.join(output_folder, 'images', fn,
                                       '{:03d}'.format(person_id))
            if not osp.exists(curr_img_folder):
                os.makedirs(curr_img_folder)

            if gender_lbl_type != 'none':
                if gender_lbl_type == 'pd' and 'gender_pd' in data:
                    gender = data['gender_pd'][person_id]
                if gender_lbl_type == 'gt' and 'gender_gt' in data:
                    gender = data['gender_gt'][person_id]
            else:
                gender = input_gender

            if gender == 'neutral':
                body_model = neutral_model
            elif gender == 'female':
                body_model = female_model
            elif gender == 'male':
                body_model = male_model

            out_img_fn = osp.join(curr_img_folder, 'output.png')

            fit_single_frame(img, keypoints[[person_id]],
                             body_model=body_model,
                             camera=camera,
                             joint_weights=joint_weights,
                             dtype=dtype,
                             output_folder=output_folder,
                             result_folder=curr_result_folder,
                             out_img_fn=out_img_fn,
                             result_fn=curr_result_fn,
                             mesh_fn=curr_mesh_fn,
                             shape_prior=shape_prior,
                             expr_prior=expr_prior,
                             body_pose_prior=body_pose_prior,
                             left_hand_prior=left_hand_prior,
                             right_hand_prior=right_hand_prior,
                             jaw_prior=jaw_prior,
                             angle_prior=angle_prior,
                             **args)

    elapsed = time.time() - start
    time_msg = time.strftime('%H hours, %M minutes, %S seconds',
                             time.gmtime(elapsed))
    print('Processing the data took: {}'.format(time_msg))


if __name__ == "__main__":
    args = parse_config()
    main(**args)


D:\Projects\smplify-x\README.md
## Expressive Body Capture: 3D Hands, Face, and Body from a Single Image

[[Project Page](https://smpl-x.is.tue.mpg.de/)] 
[[Paper](https://ps.is.tuebingen.mpg.de/uploads_file/attachment/attachment/497/SMPL-X.pdf)]
[[Supp. Mat.](https://ps.is.tuebingen.mpg.de/uploads_file/attachment/attachment/498/SMPL-X-supp.pdf)]

![SMPL-X Examples](./images/teaser_fig.png)

## Table of Contents
  * [License](#license)
  * [Description](#description)
    * [Fitting](#fitting)
    * [Different Body Models](#different-body-models)
    * [Visualizing Results](#visualizing-results)
  * [Dependencies](#dependencies)
  * [Citation](#citation)
  * [Acknowledgments](#acknowledgments)
  * [Contact](#contact)


## License

Software Copyright License for **non-commercial scientific research purposes**.
Please read carefully the [terms and conditions](https://github.com/vchoutas/smplx/blob/master/LICENSE) and any accompanying documentation before you download and/or use the SMPL-X/SMPLify-X model, data and software, (the "Model & Software"), including 3D meshes, blend weights, blend shapes, textures, software, scripts, and animations. By downloading and/or using the Model & Software (including downloading, cloning, installing, and any other use of this github repository), you acknowledge that you have read these terms and conditions, understand them, and agree to be bound by them. If you do not agree with these terms and conditions, you must not download and/or use the Model & Software. Any infringement of the terms of this agreement will automatically terminate your rights under this [License](./LICENSE).

## Disclaimer

The original images used for the figures 1 and 2 of the paper can be found in [this link](https://www.gettyimages.de/search/stack/546047069#). 
The images in the paper are used under license from gettyimages.com.
We have acquired the right to use them in the publication, but redistribution is not allowed.
Please follow the instructions on the given link to acquire right of usage.
Our results are obtained on the 483 × 724 pixels resolution of the original images.

## Description

This repository contains the fitting code used for the experiments in [Expressive Body Capture: 3D Hands, Face, and Body from a Single Image](https://smpl-x.is.tue.mpg.de/).

### Fitting 
Run the following command to execute the code:
```Shell
python smplifyx/main.py --config cfg_files/fit_smplx.yaml 
    --data_folder DATA_FOLDER 
    --output_folder OUTPUT_FOLDER 
    --visualize="True/False"
    --model_folder MODEL_FOLDER
    --vposer_ckpt VPOSER_FOLDER
    --part_segm_fn smplx_parts_segm.pkl
```
where the `DATA_FOLDER` should contain two subfolders, *images*, where the
images are located, and *keypoints*, where the OpenPose output should be
stored.

### Different Body Models

To fit [SMPL](http://smpl.is.tue.mpg.de/) or [SMPL+H](http://mano.is.tue.mpg.de), replace the *yaml* configuration file 
with either *fit_smpl.yaml* or *fit_smplx.yaml*, i.e.:
 * for SMPL:
 ```Shell
 python smplifyx/main.py --config cfg_files/fit_smpl.yaml 
    --data_folder DATA_FOLDER 
    --output_folder OUTPUT_FOLDER 
    --visualize="True/False"
    --model_folder MODEL_FOLDER
    --vposer_ckpt VPOSER_FOLDER
 ```
  * for SMPL+H:
 ```Shell
 python smplifyx/main.py --config cfg_files/fit_smplh.yaml 
    --data_folder DATA_FOLDER 
    --output_folder OUTPUT_FOLDER 
    --visualize="True/False"
    --model_folder MODEL_FOLDER
    --vposer_ckpt VPOSER_FOLDER
 ```
 
### Visualizing Results

To visualize the results produced by the method you can run the following script:
```Shell
python smplifyx/render_results.py --mesh_fns OUTPUT_MESH_FOLDER
```
where *OUTPUT_MESH_FOLDER* is the folder that contains the resulting meshes.

## Dependencies

Follow the installation instructions for each of the following before using the
fitting code.

1. [PyTorch](https://pytorch.org/)
2. [SMPL-X](https://github.com/vchoutas/smplx)
3. [VPoser](https://github.com/nghorbani/HumanBodyPrior)
4. [Homogenus](https://github.com/nghorbani/homogenus)

### Optional Dependencies

1. [PyTorch Mesh self-intersection](https://github.com/vchoutas/torch-mesh-isect) for interpenetration penalty 
   * Download the per-triangle part segmentation: [smplx_parts_segm.pkl](https://smpl-x.is.tue.mpg.de/download.php) (ctrl/cmd +F for the filename)
1. [Trimesh](https://trimsh.org/) for loading triangular meshes
1. [Pyrender](https://pyrender.readthedocs.io/) for visualization

The code has been tested with Python 3.6, CUDA 10.0, CuDNN 7.3 and PyTorch 1.0 on Ubuntu 18.04. 

## Citation

If you find this Model & Software useful in your research we would kindly ask you to cite:

```
@inproceedings{SMPL-X:2019,
  title = {Expressive Body Capture: 3D Hands, Face, and Body from a Single Image},
  author = {Pavlakos, Georgios and Choutas, Vasileios and Ghorbani, Nima and Bolkart, Timo and Osman, Ahmed A. A. and Tzionas, Dimitrios and Black, Michael J.},
  booktitle = {Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)},
  year = {2019}
}
```

## Acknowledgments

### LBFGS with Strong Wolfe Line Search

The LBFGS optimizer with Strong Wolfe Line search is taken from this [Pytorch pull request](https://github.com/pytorch/pytorch/pull/8824). Special thanks to 
[Du Phan](https://github.com/fehiepsi) for implementing this. 
We will update the repository once the pull request is merged.

## Contact
The code of this repository was implemented by [Vassilis Choutas](mailto:vassilis.choutas@tuebingen.mpg.de) and
[Georgios Pavlakos](mailto:pavlakos@seas.upenn.edu).

For questions, please contact [smplx@tue.mpg.de](mailto:smplx@tue.mpg.de). 

For commercial licensing (and all related questions for business applications), please contact [ps-licensing@tue.mpg.de](mailto:ps-licensing@tue.mpg.de).


D:\Projects\smplify-x\smplifyx\camera.py
# -*- coding: utf-8 -*-

# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is
# holder of all proprietary rights on this computer program.
# You can only use this computer program if you have closed
# a license agreement with MPG or you get the right to use the computer
# program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and
# liable to prosecution.
#
# Copyright©2019 Max-Planck-Gesellschaft zur Förderung
# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute
# for Intelligent Systems and the Max Planck Institute for Biological
# Cybernetics. All rights reserved.
#
# Contact: ps-license@tuebingen.mpg.de

from __future__ import absolute_import
from __future__ import print_function
from __future__ import division

from collections import namedtuple

import torch
import torch.nn as nn

from smplx.lbs import transform_mat


PerspParams = namedtuple('ModelOutput',
                         ['rotation', 'translation', 'center',
                          'focal_length'])


def create_camera(camera_type='persp', **kwargs):
    if camera_type.lower() == 'persp':
        return PerspectiveCamera(**kwargs)
    else:
        raise ValueError('Uknown camera type: {}'.format(camera_type))


class PerspectiveCamera(nn.Module):

    FOCAL_LENGTH = 5000

    def __init__(self, rotation=None, translation=None,
                 focal_length_x=None, focal_length_y=None,
                 batch_size=1,
                 center=None, dtype=torch.float32, **kwargs):
        super(PerspectiveCamera, self).__init__()
        self.batch_size = batch_size
        self.dtype = dtype
        # Make a buffer so that PyTorch does not complain when creating
        # the camera matrix
        self.register_buffer('zero',
                             torch.zeros([batch_size], dtype=dtype))

        if focal_length_x is None or type(focal_length_x) == float:
            focal_length_x = torch.full(
                [batch_size],
                self.FOCAL_LENGTH if focal_length_x is None else
                focal_length_x,
                dtype=dtype)

        if focal_length_y is None or type(focal_length_y) == float:
            focal_length_y = torch.full(
                [batch_size],
                self.FOCAL_LENGTH if focal_length_y is None else
                focal_length_y,
                dtype=dtype)

        self.register_buffer('focal_length_x', focal_length_x)
        self.register_buffer('focal_length_y', focal_length_y)

        if center is None:
            center = torch.zeros([batch_size, 2], dtype=dtype)
        self.register_buffer('center', center)

        if rotation is None:
            rotation = torch.eye(
                3, dtype=dtype).unsqueeze(dim=0).repeat(batch_size, 1, 1)

        rotation = nn.Parameter(rotation, requires_grad=True)
        self.register_parameter('rotation', rotation)

        if translation is None:
            translation = torch.zeros([batch_size, 3], dtype=dtype)

        translation = nn.Parameter(translation,
                                   requires_grad=True)
        self.register_parameter('translation', translation)

    def forward(self, points):
        device = points.device

        with torch.no_grad():
            camera_mat = torch.zeros([self.batch_size, 2, 2],
                                     dtype=self.dtype, device=points.device)
            camera_mat[:, 0, 0] = self.focal_length_x
            camera_mat[:, 1, 1] = self.focal_length_y

        camera_transform = transform_mat(self.rotation,
                                         self.translation.unsqueeze(dim=-1))
        homog_coord = torch.ones(list(points.shape)[:-1] + [1],
                                 dtype=points.dtype,
                                 device=device)
        # Convert the points to homogeneous coordinates
        points_h = torch.cat([points, homog_coord], dim=-1)

        projected_points = torch.einsum('bki,bji->bjk',
                                        [camera_transform, points_h])

        img_points = torch.div(projected_points[:, :, :2],
                               projected_points[:, :, 2].unsqueeze(dim=-1))
        img_points = torch.einsum('bki,bji->bjk', [camera_mat, img_points]) \
            + self.center.unsqueeze(dim=1)
        return img_points


D:\Projects\smplify-x\smplifyx\cmd_parser.py
# -*- coding: utf-8 -*-

# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is
# holder of all proprietary rights on this computer program.
# You can only use this computer program if you have closed
# a license agreement with MPG or you get the right to use the computer
# program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and
# liable to prosecution.
#
# Copyright©2019 Max-Planck-Gesellschaft zur Förderung
# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute
# for Intelligent Systems. All rights reserved.
#
# Contact: ps-license@tuebingen.mpg.de

from __future__ import absolute_import
from __future__ import print_function
from __future__ import division

import sys
import os

import configargparse


def parse_config(argv=None):
    arg_formatter = configargparse.ArgumentDefaultsHelpFormatter

    cfg_parser = configargparse.YAMLConfigFileParser
    description = 'PyTorch implementation of SMPLifyX'
    parser = configargparse.ArgParser(formatter_class=arg_formatter,
                                      config_file_parser_class=cfg_parser,
                                      description=description,
                                      prog='SMPLifyX')

    parser.add_argument('--data_folder',
                        default=os.getcwd(),
                        help='The directory that contains the data.')
    parser.add_argument('--max_persons', type=int, default=3,
                        help='The maximum number of persons to process')
    parser.add_argument('-c', '--config',
                        required=True, is_config_file=True,
                        help='config file path')
    parser.add_argument('--loss_type', default='smplify', type=str,
                        help='The type of loss to use')
    parser.add_argument('--interactive',
                        type=lambda arg: arg.lower() == 'true',
                        default=False,
                        help='Print info messages during the process')
    parser.add_argument('--save_meshes',
                        type=lambda arg: arg.lower() == 'true',
                        default=True,
                        help='Save final output meshes')
    parser.add_argument('--visualize',
                        type=lambda arg: arg.lower() == 'true',
                        default=False,
                        help='Display plots while running the optimization')
    parser.add_argument('--degrees', type=float, default=[0, 90, 180, 270],
                        nargs='*',
                        help='Degrees of rotation for rendering the final' +
                        ' result')
    parser.add_argument('--use_cuda',
                        type=lambda arg: arg.lower() == 'true',
                        default=True,
                        help='Use CUDA for the computations')
    parser.add_argument('--dataset', default='hands_cmu_gt', type=str,
                        help='The name of the dataset that will be used')
    parser.add_argument('--joints_to_ign', default=-1, type=int,
                        nargs='*',
                        help='Indices of joints to be ignored')
    parser.add_argument('--output_folder',
                        default='output',
                        type=str,
                        help='The folder where the output is stored')
    parser.add_argument('--img_folder', type=str, default='images',
                        help='The folder where the images are stored')
    parser.add_argument('--keyp_folder', type=str, default='keypoints',
                        help='The folder where the keypoints are stored')
    parser.add_argument('--summary_folder', type=str, default='summaries',
                        help='Where to store the TensorBoard summaries')
    parser.add_argument('--result_folder', type=str, default='results',
                        help='The folder with the pkls of the output' +
                        ' parameters')
    parser.add_argument('--mesh_folder', type=str, default='meshes',
                        help='The folder where the output meshes are stored')
    parser.add_argument('--gender_lbl_type', default='none',
                        choices=['none', 'gt', 'pd'], type=str,
                        help='The type of gender label to use')
    parser.add_argument('--gender', type=str,
                        default='neutral',
                        choices=['neutral', 'male', 'female'],
                        help='Use gender neutral or gender specific SMPL' +
                        'model')
    parser.add_argument('--float_dtype', type=str, default='float32',
                        help='The types of floats used')
    parser.add_argument('--model_type', default='smpl', type=str,
                        choices=['smpl', 'smplh', 'smplx'],
                        help='The type of the model that we will fit to the' +
                        ' data.')
    parser.add_argument('--camera_type', type=str, default='persp',
                        choices=['persp'],
                        help='The type of camera used')
    parser.add_argument('--optim_jaw', default=True,
                        type=lambda x: x.lower() in ['true', '1'],
                        help='Optimize over the jaw pose')
    parser.add_argument('--optim_hands', default=True,
                        type=lambda x: x.lower() in ['true', '1'],
                        help='Optimize over the hand pose')
    parser.add_argument('--optim_expression', default=True,
                        type=lambda x: x.lower() in ['true', '1'],
                        help='Optimize over the expression')
    parser.add_argument('--optim_shape', default=True,
                        type=lambda x: x.lower() in ['true', '1'],
                        help='Optimize over the shape space')

    parser.add_argument('--model_folder',
                        default='models',
                        type=str,
                        help='The directory where the models are stored.')
    parser.add_argument('--use_joints_conf', default=True,
                        type=lambda x: x.lower() in ['true', '1'],
                        help='Use the confidence scores for the optimization')
    parser.add_argument('--batch_size', type=int, default=1,
                        help='The size of the batch')
    parser.add_argument('--num_gaussians',
                        default=8,
                        type=int,
                        help='The number of gaussian for the Pose Mixture' +
                        ' Prior.')
    parser.add_argument('--use_pca', default=True,
                        type=lambda x: x.lower() in ['true', '1'],
                        help='Use the low dimensional PCA space for the hands')
    parser.add_argument('--num_pca_comps', default=6, type=int,
                        help='The number of PCA components for the hand.')
    parser.add_argument('--flat_hand_mean', default=False,
                        type=lambda arg: arg.lower() in ['true', '1'],
                        help='Use the flat hand as the mean pose')
    parser.add_argument('--body_prior_type', default='mog', type=str,
                        help='The type of prior that will be used to' +
                        ' regularize the optimization. Can be a Mixture of' +
                        ' Gaussians (mog)')
    parser.add_argument('--left_hand_prior_type', default='mog', type=str,
                        choices=['mog', 'l2', 'None'],
                        help='The type of prior that will be used to' +
                        ' regularize the optimization of the pose of the' +
                        ' left hand. Can be a Mixture of' +
                        ' Gaussians (mog)')
    parser.add_argument('--right_hand_prior_type', default='mog', type=str,
                        choices=['mog', 'l2', 'None'],
                        help='The type of prior that will be used to' +
                        ' regularize the optimization of the pose of the' +
                        ' right hand. Can be a Mixture of' +
                        ' Gaussians (mog)')
    parser.add_argument('--jaw_prior_type', default='l2', type=str,
                        choices=['l2', 'None'],
                        help='The type of prior that will be used to' +
                        ' regularize the optimization of the pose of the' +
                        ' jaw.')
    parser.add_argument('--use_vposer', default=False,
                        type=lambda arg: arg.lower() in ['true', '1'],
                        help='Use the VAE pose embedding')
    parser.add_argument('--vposer_ckpt', type=str, default='',
                        help='The path to the V-Poser checkpoint')
    # Left/Right shoulder and hips
    parser.add_argument('--init_joints_idxs', nargs='*', type=int,
                        default=[9, 12, 2, 5],
                        help='Which joints to use for initializing the camera')
    parser.add_argument('--body_tri_idxs', nargs='*',
                        #  default='5.12,2.9',
                        default=[5, 12, 2, 9],
                        type=int,
                        help='The indices of the joints used to estimate' +
                        ' the initial depth of the camera. The format' +
                        ' should be vIdx1 vIdx2 vIdx3 vIdx4')

    parser.add_argument('--prior_folder', type=str, default='prior',
                        help='The folder where the prior is stored')
    parser.add_argument('--focal_length',
                        default=5000,
                        type=float,
                        help='Value of focal length.')
    parser.add_argument('--rho',
                        default=100,
                        type=float,
                        help='Value of constant of robust loss')
    parser.add_argument('--interpenetration',
                        default=False,
                        type=lambda x: x.lower() in ['true', '1'],
                        help='Whether to use the interpenetration term')
    parser.add_argument('--penalize_outside',
                        default=False,
                        type=lambda x: x.lower() in ['true', '1'],
                        help='Penalize outside')
    parser.add_argument('--data_weights', nargs='*',
                        default=[1, ] * 5, type=float,
                        help='The weight of the data term')
    parser.add_argument('--body_pose_prior_weights',
                        default=[4.04 * 1e2, 4.04 * 1e2, 57.4, 4.78],
                        nargs='*',
                        type=float,
                        help='The weights of the body pose regularizer')
    parser.add_argument('--shape_weights',
                        default=[1e2, 5 * 1e1, 1e1, .5 * 1e1],
                        type=float, nargs='*',
                        help='The weights of the Shape regularizer')
    parser.add_argument('--expr_weights',
                        default=[1e2, 5 * 1e1, 1e1, .5 * 1e1],
                        type=float, nargs='*',
                        help='The weights of the Expressions regularizer')
    parser.add_argument('--face_joints_weights',
                        default=[0.0, 0.0, 0.0, 2.0], type=float,
                        nargs='*',
                        help='The weights for the facial keypoints' +
                        ' for each stage of the optimization')
    parser.add_argument('--hand_joints_weights',
                        default=[0.0, 0.0, 0.0, 2.0],
                        type=float, nargs='*',
                        help='The weights for the 2D joint error of the hands')
    parser.add_argument('--jaw_pose_prior_weights',
                        nargs='*',
                        help='The weights of the pose regularizer of the' +
                        ' hands')
    parser.add_argument('--hand_pose_prior_weights',
                        default=[1e2, 5 * 1e1, 1e1, .5 * 1e1],
                        type=float, nargs='*',
                        help='The weights of the pose regularizer of the' +
                        ' hands')
    parser.add_argument('--coll_loss_weights',
                        default=[0.0, 0.0, 0.0, 2.0], type=float,
                        nargs='*',
                        help='The weight for the collision term')

    parser.add_argument('--depth_loss_weight', default=1e2, type=float,
                        help='The weight for the regularizer for the' +
                        ' z coordinate of the camera translation')
    parser.add_argument('--df_cone_height', default=0.5, type=float,
                        help='The default value for the height of the cone' +
                        ' that is used to calculate the penetration distance' +
                        ' field')
    parser.add_argument('--max_collisions', default=8, type=int,
                        help='The maximum number of bounding box collisions')
    parser.add_argument('--point2plane', default=False,
                        type=lambda arg: arg.lower() in ['true', '1'],
                        help='Use point to plane distance')
    parser.add_argument('--part_segm_fn', default='', type=str,
                        help='The file with the part segmentation for the' +
                        ' faces of the model')
    parser.add_argument('--ign_part_pairs', default=None,
                        nargs='*', type=str,
                        help='Pairs of parts whose collisions will be ignored')
    parser.add_argument('--use_hands', default=False,
                        type=lambda x: x.lower() in ['true', '1'],
                        help='Use the hand keypoints in the SMPL' +
                        'optimization process')
    parser.add_argument('--use_face', default=False,
                        type=lambda x: x.lower() in ['true', '1'],
                        help='Use the facial keypoints in the optimization' +
                        ' process')
    parser.add_argument('--use_face_contour', default=False,
                        type=lambda x: x.lower() in ['true', '1'],
                        help='Use the dynamic contours of the face')
    parser.add_argument('--side_view_thsh',
                        default=25,
                        type=float,
                        help='This is thresholding value that determines' +
                        ' whether the human is captured in a side view.' +
                        'If the pixel distance between the shoulders is less' +
                        ' than this value, two initializations of SMPL fits' +
                        ' are tried.')
    parser.add_argument('--optim_type', type=str, default='adam',
                        help='The optimizer used')
    parser.add_argument('--lr', type=float, default=1e-6,
                        help='The learning rate for the algorithm')
    parser.add_argument('--gtol', type=float, default=1e-8,
                        help='The tolerance threshold for the gradient')
    parser.add_argument('--ftol', type=float, default=2e-9,
                        help='The tolerance threshold for the function')
    parser.add_argument('--maxiters', type=int, default=100,
                        help='The maximum iterations for the optimization')

    args = parser.parse_args(argv)

    args_dict = vars(args)

    assert len(args_dict['body_tri_idxs']) % 2 == 0, (
        'Number of body_tri_idxs arguments must be divisble by 2.'
        f' Got: {len(args_dict["body_tri_idxs"])}'
    )
    num_tri_idxs = len(args_dict['body_tri_idxs'])
    # Convert the list of indices to a list of pairs
    args_dict['body_tri_idxs'] = [
        (args_dict['body_tri_idxs'][ii], args_dict['body_tri_idxs'][jj])
        for ii, jj in zip(range(0, num_tri_idxs, 2), range(1, num_tri_idxs, 2))
    ]
    return args_dict


D:\Projects\smplify-x\smplifyx\data_parser.py
# -*- coding: utf-8 -*-

# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is
# holder of all proprietary rights on this computer program.
# You can only use this computer program if you have closed
# a license agreement with MPG or you get the right to use the computer
# program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and
# liable to prosecution.
#
# Copyright©2019 Max-Planck-Gesellschaft zur Förderung
# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute
# for Intelligent Systems and the Max Planck Institute for Biological
# Cybernetics. All rights reserved.
#
# Contact: ps-license@tuebingen.mpg.de

from __future__ import absolute_import
from __future__ import print_function
from __future__ import division

import sys
import os
import os.path as osp

import json

from collections import namedtuple

import cv2
import numpy as np

import torch
from torch.utils.data import Dataset


from utils import smpl_to_openpose

Keypoints = namedtuple('Keypoints',
                       ['keypoints', 'gender_gt', 'gender_pd'])

Keypoints.__new__.__defaults__ = (None,) * len(Keypoints._fields)


def create_dataset(dataset='openpose', data_folder='data', **kwargs):
    if dataset.lower() == 'openpose':
        return OpenPose(data_folder, **kwargs)
    else:
        raise ValueError('Unknown dataset: {}'.format(dataset))


def read_keypoints(keypoint_fn, use_hands=True, use_face=True,
                   use_face_contour=False):
    with open(keypoint_fn) as keypoint_file:
        data = json.load(keypoint_file)

    keypoints = []

    gender_pd = []
    gender_gt = []
    for idx, person_data in enumerate(data['people']):
        body_keypoints = np.array(person_data['pose_keypoints_2d'],
                                  dtype=np.float32)
        body_keypoints = body_keypoints.reshape([-1, 3])
        if use_hands:
            left_hand_keyp = np.array(
                person_data['hand_left_keypoints_2d'],
                dtype=np.float32).reshape([-1, 3])
            right_hand_keyp = np.array(
                person_data['hand_right_keypoints_2d'],
                dtype=np.float32).reshape([-1, 3])

            body_keypoints = np.concatenate(
                [body_keypoints, left_hand_keyp, right_hand_keyp], axis=0)
        if use_face:
            # TODO: Make parameters, 17 is the offset for the eye brows,
            # etc. 51 is the total number of FLAME compatible landmarks
            face_keypoints = np.array(
                person_data['face_keypoints_2d'],
                dtype=np.float32).reshape([-1, 3])[17: 17 + 51, :]

            contour_keyps = np.array(
                [], dtype=body_keypoints.dtype).reshape(0, 3)
            if use_face_contour:
                contour_keyps = np.array(
                    person_data['face_keypoints_2d'],
                    dtype=np.float32).reshape([-1, 3])[:17, :]

            body_keypoints = np.concatenate(
                [body_keypoints, face_keypoints, contour_keyps], axis=0)

        if 'gender_pd' in person_data:
            gender_pd.append(person_data['gender_pd'])
        if 'gender_gt' in person_data:
            gender_gt.append(person_data['gender_gt'])

        keypoints.append(body_keypoints)

    return Keypoints(keypoints=keypoints, gender_pd=gender_pd,
                     gender_gt=gender_gt)


class OpenPose(Dataset):

    NUM_BODY_JOINTS = 25
    NUM_HAND_JOINTS = 20

    def __init__(self, data_folder, img_folder='images',
                 keyp_folder='keypoints',
                 use_hands=False,
                 use_face=False,
                 dtype=torch.float32,
                 model_type='smplx',
                 joints_to_ign=None,
                 use_face_contour=False,
                 openpose_format='coco25',
                 **kwargs):
        super(OpenPose, self).__init__()

        self.use_hands = use_hands
        self.use_face = use_face
        self.model_type = model_type
        self.dtype = dtype
        self.joints_to_ign = joints_to_ign
        self.use_face_contour = use_face_contour

        self.openpose_format = openpose_format

        self.num_joints = (self.NUM_BODY_JOINTS +
                           2 * self.NUM_HAND_JOINTS * use_hands)

        self.img_folder = osp.join(data_folder, img_folder)
        self.keyp_folder = osp.join(data_folder, keyp_folder)

        self.img_paths = [osp.join(self.img_folder, img_fn)
                          for img_fn in os.listdir(self.img_folder)
                          if img_fn.endswith('.png') or
                          img_fn.endswith('.jpg') and
                          not img_fn.startswith('.')]
        self.img_paths = sorted(self.img_paths)
        self.cnt = 0

    def get_model2data(self):
        return smpl_to_openpose(self.model_type, use_hands=self.use_hands,
                                use_face=self.use_face,
                                use_face_contour=self.use_face_contour,
                                openpose_format=self.openpose_format)

    def get_left_shoulder(self):
        return 2

    def get_right_shoulder(self):
        return 5

    def get_joint_weights(self):
        # The weights for the joint terms in the optimization
        optim_weights = np.ones(self.num_joints + 2 * self.use_hands +
                                self.use_face * 51 +
                                17 * self.use_face_contour,
                                dtype=np.float32)

        # Neck, Left and right hip
        # These joints are ignored because SMPL has no neck joint and the
        # annotation of the hips is ambiguous.
        if self.joints_to_ign is not None and -1 not in self.joints_to_ign:
            optim_weights[self.joints_to_ign] = 0.
        return torch.tensor(optim_weights, dtype=self.dtype)

    def __len__(self):
        return len(self.img_paths)

    def __getitem__(self, idx):
        img_path = self.img_paths[idx]
        return self.read_item(img_path)

    def read_item(self, img_path):
        img = cv2.imread(img_path).astype(np.float32)[:, :, ::-1] / 255.0
        img_fn = osp.split(img_path)[1]
        img_fn, _ = osp.splitext(osp.split(img_path)[1])

        keypoint_fn = osp.join(self.keyp_folder,
                               img_fn + '_keypoints.json')
        keyp_tuple = read_keypoints(keypoint_fn, use_hands=self.use_hands,
                                    use_face=self.use_face,
                                    use_face_contour=self.use_face_contour)

        if len(keyp_tuple.keypoints) < 1:
            return {}
        keypoints = np.stack(keyp_tuple.keypoints)

        output_dict = {'fn': img_fn,
                       'img_path': img_path,
                       'keypoints': keypoints, 'img': img}
        if keyp_tuple.gender_gt is not None:
            if len(keyp_tuple.gender_gt) > 0:
                output_dict['gender_gt'] = keyp_tuple.gender_gt
        if keyp_tuple.gender_pd is not None:
            if len(keyp_tuple.gender_pd) > 0:
                output_dict['gender_pd'] = keyp_tuple.gender_pd
        return output_dict

    def __iter__(self):
        return self

    def __next__(self):
        return self.next()

    def next(self):
        if self.cnt >= len(self.img_paths):
            raise StopIteration

        img_path = self.img_paths[self.cnt]
        self.cnt += 1

        return self.read_item(img_path)


D:\Projects\smplify-x\smplifyx\fitting.py
# -*- coding: utf-8 -*-

# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is
# holder of all proprietary rights on this computer program.
# You can only use this computer program if you have closed
# a license agreement with MPG or you get the right to use the computer
# program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and
# liable to prosecution.
#
# Copyright©2019 Max-Planck-Gesellschaft zur Förderung
# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute
# for Intelligent Systems and the Max Planck Institute for Biological
# Cybernetics. All rights reserved.
#
# Contact: ps-license@tuebingen.mpg.de

from __future__ import absolute_import
from __future__ import print_function
from __future__ import division

import sys
import os

import time

import numpy as np

import torch
import torch.nn as nn

from mesh_viewer import MeshViewer
import utils


@torch.no_grad()
def guess_init(model,
               joints_2d,
               edge_idxs,
               focal_length=5000,
               pose_embedding=None,
               vposer=None,
               use_vposer=True,
               dtype=torch.float32,
               model_type='smpl',
               **kwargs):
    ''' Initializes the camera translation vector

        Parameters
        ----------
        model: nn.Module
            The PyTorch module of the body
        joints_2d: torch.tensor 1xJx2
            The 2D tensor of the joints
        edge_idxs: list of lists
            A list of pairs, each of which represents a limb used to estimate
            the camera translation
        focal_length: float, optional (default = 5000)
            The focal length of the camera
        pose_embedding: torch.tensor 1x32
            The tensor that contains the embedding of V-Poser that is used to
            generate the pose of the model
        dtype: torch.dtype, optional (torch.float32)
            The floating point type used
        vposer: nn.Module, optional (None)
            The PyTorch module that implements the V-Poser decoder
        Returns
        -------
        init_t: torch.tensor 1x3, dtype = torch.float32
            The vector with the estimated camera location

    '''

    body_pose = vposer.decode(
        pose_embedding, output_type='aa').view(1, -1) if use_vposer else None
    if use_vposer and model_type == 'smpl':
        wrist_pose = torch.zeros([body_pose.shape[0], 6],
                                 dtype=body_pose.dtype,
                                 device=body_pose.device)
        body_pose = torch.cat([body_pose, wrist_pose], dim=1)

    output = model(body_pose=body_pose, return_verts=False,
                   return_full_pose=False)
    joints_3d = output.joints
    joints_2d = joints_2d.to(device=joints_3d.device)

    diff3d = []
    diff2d = []
    for edge in edge_idxs:
        diff3d.append(joints_3d[:, edge[0]] - joints_3d[:, edge[1]])
        diff2d.append(joints_2d[:, edge[0]] - joints_2d[:, edge[1]])

    diff3d = torch.stack(diff3d, dim=1)
    diff2d = torch.stack(diff2d, dim=1)

    length_2d = diff2d.pow(2).sum(dim=-1).sqrt()
    length_3d = diff3d.pow(2).sum(dim=-1).sqrt()

    height2d = length_2d.mean(dim=1)
    height3d = length_3d.mean(dim=1)

    est_d = focal_length * (height3d / height2d)

    # just set the z value
    batch_size = joints_3d.shape[0]
    x_coord = torch.zeros([batch_size], device=joints_3d.device,
                          dtype=dtype)
    y_coord = x_coord.clone()
    init_t = torch.stack([x_coord, y_coord, est_d], dim=1)
    return init_t


class FittingMonitor(object):
    def __init__(self, summary_steps=1, visualize=False,
                 maxiters=100, ftol=2e-09, gtol=1e-05,
                 body_color=(1.0, 1.0, 0.9, 1.0),
                 model_type='smpl',
                 **kwargs):
        super(FittingMonitor, self).__init__()

        self.maxiters = maxiters
        self.ftol = ftol
        self.gtol = gtol

        self.visualize = visualize
        self.summary_steps = summary_steps
        self.body_color = body_color
        self.model_type = model_type

    def __enter__(self):
        self.steps = 0
        if self.visualize:
            self.mv = MeshViewer(body_color=self.body_color)
        return self

    def __exit__(self, exception_type, exception_value, traceback):
        if self.visualize:
            self.mv.close_viewer()

    def set_colors(self, vertex_color):
        batch_size = self.colors.shape[0]

        self.colors = np.tile(
            np.array(vertex_color).reshape(1, 3),
            [batch_size, 1])

    def run_fitting(self, optimizer, closure, params, body_model,
                    use_vposer=True, pose_embedding=None, vposer=None,
                    **kwargs):
        ''' Helper function for running an optimization process
            Parameters
            ----------
                optimizer: torch.optim.Optimizer
                    The PyTorch optimizer object
                closure: function
                    The function used to calculate the gradients
                params: list
                    List containing the parameters that will be optimized
                body_model: nn.Module
                    The body model PyTorch module
                use_vposer: bool
                    Flag on whether to use VPoser (default=True).
                pose_embedding: torch.tensor, BxN
                    The tensor that contains the latent pose variable.
                vposer: nn.Module
                    The VPoser module
            Returns
            -------
                loss: float
                The final loss value
        '''
        append_wrists = self.model_type == 'smpl' and use_vposer
        prev_loss = None
        for n in range(self.maxiters):
            loss = optimizer.step(closure)

            if torch.isnan(loss).sum() > 0:
                print('NaN loss value, stopping!')
                break

            if torch.isinf(loss).sum() > 0:
                print('Infinite loss value, stopping!')
                break

            if n > 0 and prev_loss is not None and self.ftol > 0:
                loss_rel_change = utils.rel_change(prev_loss, loss.item())

                if loss_rel_change <= self.ftol:
                    break

            if all([torch.abs(var.grad.view(-1).max()).item() < self.gtol
                    for var in params if var.grad is not None]):
                break

            if self.visualize and n % self.summary_steps == 0:
                body_pose = vposer.decode(
                    pose_embedding, output_type='aa').view(
                        1, -1) if use_vposer else None

                if append_wrists:
                    wrist_pose = torch.zeros([body_pose.shape[0], 6],
                                             dtype=body_pose.dtype,
                                             device=body_pose.device)
                    body_pose = torch.cat([body_pose, wrist_pose], dim=1)
                model_output = body_model(
                    return_verts=True, body_pose=body_pose)
                vertices = model_output.vertices.detach().cpu().numpy()

                self.mv.update_mesh(vertices.squeeze(),
                                    body_model.faces)

            prev_loss = loss.item()

        return prev_loss

    def create_fitting_closure(self,
                               optimizer, body_model, camera=None,
                               gt_joints=None, loss=None,
                               joints_conf=None,
                               joint_weights=None,
                               return_verts=True, return_full_pose=False,
                               use_vposer=False, vposer=None,
                               pose_embedding=None,
                               create_graph=False,
                               **kwargs):
        faces_tensor = body_model.faces_tensor.view(-1)
        append_wrists = self.model_type == 'smpl' and use_vposer

        def fitting_func(backward=True):
            if backward:
                optimizer.zero_grad()

            body_pose = vposer.decode(
                pose_embedding, output_type='aa').view(
                    1, -1) if use_vposer else None

            if append_wrists:
                wrist_pose = torch.zeros([body_pose.shape[0], 6],
                                         dtype=body_pose.dtype,
                                         device=body_pose.device)
                body_pose = torch.cat([body_pose, wrist_pose], dim=1)

            body_model_output = body_model(return_verts=return_verts,
                                           body_pose=body_pose,
                                           return_full_pose=return_full_pose)
            total_loss = loss(body_model_output, camera=camera,
                              gt_joints=gt_joints,
                              body_model_faces=faces_tensor,
                              joints_conf=joints_conf,
                              joint_weights=joint_weights,
                              pose_embedding=pose_embedding,
                              use_vposer=use_vposer,
                              **kwargs)

            if backward:
                total_loss.backward(create_graph=create_graph)

            self.steps += 1
            if self.visualize and self.steps % self.summary_steps == 0:
                model_output = body_model(return_verts=True,
                                          body_pose=body_pose)
                vertices = model_output.vertices.detach().cpu().numpy()

                self.mv.update_mesh(vertices.squeeze(),
                                    body_model.faces)

            return total_loss

        return fitting_func


def create_loss(loss_type='smplify', **kwargs):
    if loss_type == 'smplify':
        return SMPLifyLoss(**kwargs)
    elif loss_type == 'camera_init':
        return SMPLifyCameraInitLoss(**kwargs)
    else:
        raise ValueError('Unknown loss type: {}'.format(loss_type))


class SMPLifyLoss(nn.Module):

    def __init__(self, search_tree=None,
                 pen_distance=None, tri_filtering_module=None,
                 rho=100,
                 body_pose_prior=None,
                 shape_prior=None,
                 expr_prior=None,
                 angle_prior=None,
                 jaw_prior=None,
                 use_joints_conf=True,
                 use_face=True, use_hands=True,
                 left_hand_prior=None, right_hand_prior=None,
                 interpenetration=True, dtype=torch.float32,
                 data_weight=1.0,
                 body_pose_weight=0.0,
                 shape_weight=0.0,
                 bending_prior_weight=0.0,
                 hand_prior_weight=0.0,
                 expr_prior_weight=0.0, jaw_prior_weight=0.0,
                 coll_loss_weight=0.0,
                 reduction='sum',
                 **kwargs):

        super(SMPLifyLoss, self).__init__()

        self.use_joints_conf = use_joints_conf
        self.angle_prior = angle_prior

        self.robustifier = utils.GMoF(rho=rho)
        self.rho = rho

        self.body_pose_prior = body_pose_prior

        self.shape_prior = shape_prior

        self.interpenetration = interpenetration
        if self.interpenetration:
            self.search_tree = search_tree
            self.tri_filtering_module = tri_filtering_module
            self.pen_distance = pen_distance

        self.use_hands = use_hands
        if self.use_hands:
            self.left_hand_prior = left_hand_prior
            self.right_hand_prior = right_hand_prior

        self.use_face = use_face
        if self.use_face:
            self.expr_prior = expr_prior
            self.jaw_prior = jaw_prior

        self.register_buffer('data_weight',
                             torch.tensor(data_weight, dtype=dtype))
        self.register_buffer('body_pose_weight',
                             torch.tensor(body_pose_weight, dtype=dtype))
        self.register_buffer('shape_weight',
                             torch.tensor(shape_weight, dtype=dtype))
        self.register_buffer('bending_prior_weight',
                             torch.tensor(bending_prior_weight, dtype=dtype))
        if self.use_hands:
            self.register_buffer('hand_prior_weight',
                                 torch.tensor(hand_prior_weight, dtype=dtype))
        if self.use_face:
            self.register_buffer('expr_prior_weight',
                                 torch.tensor(expr_prior_weight, dtype=dtype))
            self.register_buffer('jaw_prior_weight',
                                 torch.tensor(jaw_prior_weight, dtype=dtype))
        if self.interpenetration:
            self.register_buffer('coll_loss_weight',
                                 torch.tensor(coll_loss_weight, dtype=dtype))

    def reset_loss_weights(self, loss_weight_dict):
        for key in loss_weight_dict:
            if hasattr(self, key):
                weight_tensor = getattr(self, key)
                if 'torch.Tensor' in str(type(loss_weight_dict[key])):
                    weight_tensor = loss_weight_dict[key].clone().detach()
                else:
                    weight_tensor = torch.tensor(loss_weight_dict[key],
                                                 dtype=weight_tensor.dtype,
                                                 device=weight_tensor.device)
                setattr(self, key, weight_tensor)

    def forward(self, body_model_output, camera, gt_joints, joints_conf,
                body_model_faces, joint_weights,
                use_vposer=False, pose_embedding=None,
                **kwargs):
        projected_joints = camera(body_model_output.joints)
        # Calculate the weights for each joints
        weights = (joint_weights * joints_conf
                   if self.use_joints_conf else
                   joint_weights).unsqueeze(dim=-1)

        # Calculate the distance of the projected joints from
        # the ground truth 2D detections
        joint_diff = self.robustifier(gt_joints - projected_joints)
        joint_loss = (torch.sum(weights ** 2 * joint_diff) *
                      self.data_weight ** 2)

        # Calculate the loss from the Pose prior
        if use_vposer:
            pprior_loss = (pose_embedding.pow(2).sum() *
                           self.body_pose_weight ** 2)
        else:
            pprior_loss = torch.sum(self.body_pose_prior(
                body_model_output.body_pose,
                body_model_output.betas)) * self.body_pose_weight ** 2

        shape_loss = torch.sum(self.shape_prior(
            body_model_output.betas)) * self.shape_weight ** 2
        # Calculate the prior over the joint rotations. This a heuristic used
        # to prevent extreme rotation of the elbows and knees
        body_pose = body_model_output.full_pose[:, 3:66]
        angle_prior_loss = torch.sum(
            self.angle_prior(body_pose)) * self.bending_prior_weight

        # Apply the prior on the pose space of the hand
        left_hand_prior_loss, right_hand_prior_loss = 0.0, 0.0
        if self.use_hands and self.left_hand_prior is not None:
            left_hand_prior_loss = torch.sum(
                self.left_hand_prior(
                    body_model_output.left_hand_pose)) * \
                self.hand_prior_weight ** 2

        if self.use_hands and self.right_hand_prior is not None:
            right_hand_prior_loss = torch.sum(
                self.right_hand_prior(
                    body_model_output.right_hand_pose)) * \
                self.hand_prior_weight ** 2

        expression_loss = 0.0
        jaw_prior_loss = 0.0
        if self.use_face:
            expression_loss = torch.sum(self.expr_prior(
                body_model_output.expression)) * \
                self.expr_prior_weight ** 2

            if hasattr(self, 'jaw_prior'):
                jaw_prior_loss = torch.sum(
                    self.jaw_prior(
                        body_model_output.jaw_pose.mul(
                            self.jaw_prior_weight)))

        pen_loss = 0.0
        # Calculate the loss due to interpenetration
        if (self.interpenetration and self.coll_loss_weight.item() > 0):
            batch_size = projected_joints.shape[0]
            triangles = torch.index_select(
                body_model_output.vertices, 1,
                body_model_faces).view(batch_size, -1, 3, 3)

            with torch.no_grad():
                collision_idxs = self.search_tree(triangles)

            # Remove unwanted collisions
            if self.tri_filtering_module is not None:
                collision_idxs = self.tri_filtering_module(collision_idxs)

            if collision_idxs.ge(0).sum().item() > 0:
                pen_loss = torch.sum(
                    self.coll_loss_weight *
                    self.pen_distance(triangles, collision_idxs))

        total_loss = (joint_loss + pprior_loss + shape_loss +
                      angle_prior_loss + pen_loss +
                      jaw_prior_loss + expression_loss +
                      left_hand_prior_loss + right_hand_prior_loss)
        return total_loss


class SMPLifyCameraInitLoss(nn.Module):

    def __init__(self, init_joints_idxs, trans_estimation=None,
                 reduction='sum',
                 data_weight=1.0,
                 depth_loss_weight=1e2, dtype=torch.float32,
                 **kwargs):
        super(SMPLifyCameraInitLoss, self).__init__()
        self.dtype = dtype

        if trans_estimation is not None:
            self.register_buffer(
                'trans_estimation',
                utils.to_tensor(trans_estimation, dtype=dtype))
        else:
            self.trans_estimation = trans_estimation

        self.register_buffer('data_weight',
                             torch.tensor(data_weight, dtype=dtype))
        self.register_buffer(
            'init_joints_idxs',
            utils.to_tensor(init_joints_idxs, dtype=torch.long))
        self.register_buffer('depth_loss_weight',
                             torch.tensor(depth_loss_weight, dtype=dtype))

    def reset_loss_weights(self, loss_weight_dict):
        for key in loss_weight_dict:
            if hasattr(self, key):
                weight_tensor = getattr(self, key)
                weight_tensor = torch.tensor(loss_weight_dict[key],
                                             dtype=weight_tensor.dtype,
                                             device=weight_tensor.device)
                setattr(self, key, weight_tensor)

    def forward(self, body_model_output, camera, gt_joints,
                **kwargs):

        projected_joints = camera(body_model_output.joints)

        joint_error = torch.pow(
            torch.index_select(gt_joints, 1, self.init_joints_idxs) -
            torch.index_select(projected_joints, 1, self.init_joints_idxs),
            2)
        joint_loss = torch.sum(joint_error) * self.data_weight ** 2

        depth_loss = 0.0
        if (self.depth_loss_weight.item() > 0 and self.trans_estimation is not
                None):
            depth_loss = self.depth_loss_weight ** 2 * torch.sum((
                camera.translation[:, 2] - self.trans_estimation[:, 2]).pow(2))

        return joint_loss + depth_loss


D:\Projects\smplify-x\smplifyx\fit_single_frame.py
# -*- coding: utf-8 -*-

# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is
# holder of all proprietary rights on this computer program.
# You can only use this computer program if you have closed
# a license agreement with MPG or you get the right to use the computer
# program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and
# liable to prosecution.
#
# Copyright©2019 Max-Planck-Gesellschaft zur Förderung
# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute
# for Intelligent Systems and the Max Planck Institute for Biological
# Cybernetics. All rights reserved.
#
# Contact: ps-license@tuebingen.mpg.de

from __future__ import absolute_import
from __future__ import print_function
from __future__ import division


import time
try:
    import cPickle as pickle
except ImportError:
    import pickle

import sys
import os
import os.path as osp

import numpy as np
import torch

from tqdm import tqdm

from collections import defaultdict

import cv2
import PIL.Image as pil_img

from optimizers import optim_factory

import fitting
from human_body_prior.tools.model_loader import load_vposer


def fit_single_frame(img,
                     keypoints,
                     body_model,
                     camera,
                     joint_weights,
                     body_pose_prior,
                     jaw_prior,
                     left_hand_prior,
                     right_hand_prior,
                     shape_prior,
                     expr_prior,
                     angle_prior,
                     result_fn='out.pkl',
                     mesh_fn='out.obj',
                     out_img_fn='overlay.png',
                     loss_type='smplify',
                     use_cuda=True,
                     init_joints_idxs=(9, 12, 2, 5),
                     use_face=True,
                     use_hands=True,
                     data_weights=None,
                     body_pose_prior_weights=None,
                     hand_pose_prior_weights=None,
                     jaw_pose_prior_weights=None,
                     shape_weights=None,
                     expr_weights=None,
                     hand_joints_weights=None,
                     face_joints_weights=None,
                     depth_loss_weight=1e2,
                     interpenetration=True,
                     coll_loss_weights=None,
                     df_cone_height=0.5,
                     penalize_outside=True,
                     max_collisions=8,
                     point2plane=False,
                     part_segm_fn='',
                     focal_length=5000.,
                     side_view_thsh=25.,
                     rho=100,
                     vposer_latent_dim=32,
                     vposer_ckpt='',
                     use_joints_conf=False,
                     interactive=True,
                     visualize=False,
                     save_meshes=True,
                     degrees=None,
                     batch_size=1,
                     dtype=torch.float32,
                     ign_part_pairs=None,
                     left_shoulder_idx=2,
                     right_shoulder_idx=5,
                     **kwargs):
    assert batch_size == 1, 'PyTorch L-BFGS only supports batch_size == 1'

    device = torch.device('cuda') if use_cuda else torch.device('cpu')

    if degrees is None:
        degrees = [0, 90, 180, 270]

    if data_weights is None:
        data_weights = [1, ] * 5

    if body_pose_prior_weights is None:
        body_pose_prior_weights = [4.04 * 1e2, 4.04 * 1e2, 57.4, 4.78]

    msg = (
        'Number of Body pose prior weights {}'.format(
            len(body_pose_prior_weights)) +
        ' does not match the number of data term weights {}'.format(
            len(data_weights)))
    assert (len(data_weights) ==
            len(body_pose_prior_weights)), msg

    if use_hands:
        if hand_pose_prior_weights is None:
            hand_pose_prior_weights = [1e2, 5 * 1e1, 1e1, .5 * 1e1]
        msg = ('Number of Body pose prior weights does not match the' +
               ' number of hand pose prior weights')
        assert (len(hand_pose_prior_weights) ==
                len(body_pose_prior_weights)), msg
        if hand_joints_weights is None:
            hand_joints_weights = [0.0, 0.0, 0.0, 1.0]
            msg = ('Number of Body pose prior weights does not match the' +
                   ' number of hand joint distance weights')
            assert (len(hand_joints_weights) ==
                    len(body_pose_prior_weights)), msg

    if shape_weights is None:
        shape_weights = [1e2, 5 * 1e1, 1e1, .5 * 1e1]
    msg = ('Number of Body pose prior weights = {} does not match the' +
           ' number of Shape prior weights = {}')
    assert (len(shape_weights) ==
            len(body_pose_prior_weights)), msg.format(
                len(shape_weights),
                len(body_pose_prior_weights))

    if use_face:
        if jaw_pose_prior_weights is None:
            jaw_pose_prior_weights = [[x] * 3 for x in shape_weights]
        else:
            jaw_pose_prior_weights = map(lambda x: map(float, x.split(',')),
                                         jaw_pose_prior_weights)
            jaw_pose_prior_weights = [list(w) for w in jaw_pose_prior_weights]
        msg = ('Number of Body pose prior weights does not match the' +
               ' number of jaw pose prior weights')
        assert (len(jaw_pose_prior_weights) ==
                len(body_pose_prior_weights)), msg

        if expr_weights is None:
            expr_weights = [1e2, 5 * 1e1, 1e1, .5 * 1e1]
        msg = ('Number of Body pose prior weights = {} does not match the' +
               ' number of Expression prior weights = {}')
        assert (len(expr_weights) ==
                len(body_pose_prior_weights)), msg.format(
                    len(body_pose_prior_weights),
                    len(expr_weights))

        if face_joints_weights is None:
            face_joints_weights = [0.0, 0.0, 0.0, 1.0]
        msg = ('Number of Body pose prior weights does not match the' +
               ' number of face joint distance weights')
        assert (len(face_joints_weights) ==
                len(body_pose_prior_weights)), msg

    if coll_loss_weights is None:
        coll_loss_weights = [0.0] * len(body_pose_prior_weights)
    msg = ('Number of Body pose prior weights does not match the' +
           ' number of collision loss weights')
    assert (len(coll_loss_weights) ==
            len(body_pose_prior_weights)), msg

    use_vposer = kwargs.get('use_vposer', True)
    vposer, pose_embedding = [None, ] * 2
    if use_vposer:
        pose_embedding = torch.zeros([batch_size, 32],
                                     dtype=dtype, device=device,
                                     requires_grad=True)

        vposer_ckpt = osp.expandvars(vposer_ckpt)
        vposer, _ = load_vposer(vposer_ckpt, vp_model='snapshot')
        vposer = vposer.to(device=device)
        vposer.eval()

    if use_vposer:
        body_mean_pose = torch.zeros([batch_size, vposer_latent_dim],
                                     dtype=dtype)
    else:
        body_mean_pose = body_pose_prior.get_mean().detach().cpu()

    keypoint_data = torch.tensor(keypoints, dtype=dtype)
    gt_joints = keypoint_data[:, :, :2]
    if use_joints_conf:
        joints_conf = keypoint_data[:, :, 2].reshape(1, -1)

    # Transfer the data to the correct device
    gt_joints = gt_joints.to(device=device, dtype=dtype)
    if use_joints_conf:
        joints_conf = joints_conf.to(device=device, dtype=dtype)

    # Create the search tree
    search_tree = None
    pen_distance = None
    filter_faces = None
    if interpenetration:
        from mesh_intersection.bvh_search_tree import BVH
        import mesh_intersection.loss as collisions_loss
        from mesh_intersection.filter_faces import FilterFaces

        assert use_cuda, 'Interpenetration term can only be used with CUDA'
        assert torch.cuda.is_available(), \
            'No CUDA Device! Interpenetration term can only be used' + \
            ' with CUDA'

        search_tree = BVH(max_collisions=max_collisions)

        pen_distance = \
            collisions_loss.DistanceFieldPenetrationLoss(
                sigma=df_cone_height, point2plane=point2plane,
                vectorized=True, penalize_outside=penalize_outside)

        if part_segm_fn:
            # Read the part segmentation
            part_segm_fn = os.path.expandvars(part_segm_fn)
            with open(part_segm_fn, 'rb') as faces_parents_file:
                face_segm_data = pickle.load(faces_parents_file,
                                             encoding='latin1')
            faces_segm = face_segm_data['segm']
            faces_parents = face_segm_data['parents']
            # Create the module used to filter invalid collision pairs
            filter_faces = FilterFaces(
                faces_segm=faces_segm, faces_parents=faces_parents,
                ign_part_pairs=ign_part_pairs).to(device=device)

    # Weights used for the pose prior and the shape prior
    opt_weights_dict = {'data_weight': data_weights,
                        'body_pose_weight': body_pose_prior_weights,
                        'shape_weight': shape_weights}
    if use_face:
        opt_weights_dict['face_weight'] = face_joints_weights
        opt_weights_dict['expr_prior_weight'] = expr_weights
        opt_weights_dict['jaw_prior_weight'] = jaw_pose_prior_weights
    if use_hands:
        opt_weights_dict['hand_weight'] = hand_joints_weights
        opt_weights_dict['hand_prior_weight'] = hand_pose_prior_weights
    if interpenetration:
        opt_weights_dict['coll_loss_weight'] = coll_loss_weights

    keys = opt_weights_dict.keys()
    opt_weights = [dict(zip(keys, vals)) for vals in
                   zip(*(opt_weights_dict[k] for k in keys
                         if opt_weights_dict[k] is not None))]
    for weight_list in opt_weights:
        for key in weight_list:
            weight_list[key] = torch.tensor(weight_list[key],
                                            device=device,
                                            dtype=dtype)

    # The indices of the joints used for the initialization of the camera
    init_joints_idxs = torch.tensor(init_joints_idxs, device=device)

    edge_indices = kwargs.get('body_tri_idxs')
    init_t = fitting.guess_init(body_model, gt_joints, edge_indices,
                                use_vposer=use_vposer, vposer=vposer,
                                pose_embedding=pose_embedding,
                                model_type=kwargs.get('model_type', 'smpl'),
                                focal_length=focal_length, dtype=dtype)

    camera_loss = fitting.create_loss('camera_init',
                                      trans_estimation=init_t,
                                      init_joints_idxs=init_joints_idxs,
                                      depth_loss_weight=depth_loss_weight,
                                      dtype=dtype).to(device=device)
    camera_loss.trans_estimation[:] = init_t

    loss = fitting.create_loss(loss_type=loss_type,
                               joint_weights=joint_weights,
                               rho=rho,
                               use_joints_conf=use_joints_conf,
                               use_face=use_face, use_hands=use_hands,
                               vposer=vposer,
                               pose_embedding=pose_embedding,
                               body_pose_prior=body_pose_prior,
                               shape_prior=shape_prior,
                               angle_prior=angle_prior,
                               expr_prior=expr_prior,
                               left_hand_prior=left_hand_prior,
                               right_hand_prior=right_hand_prior,
                               jaw_prior=jaw_prior,
                               interpenetration=interpenetration,
                               pen_distance=pen_distance,
                               search_tree=search_tree,
                               tri_filtering_module=filter_faces,
                               dtype=dtype,
                               **kwargs)
    loss = loss.to(device=device)

    with fitting.FittingMonitor(
            batch_size=batch_size, visualize=visualize, **kwargs) as monitor:

        img = torch.tensor(img, dtype=dtype)

        H, W, _ = img.shape

        data_weight = 1000 / H
        # The closure passed to the optimizer
        camera_loss.reset_loss_weights({'data_weight': data_weight})

        # Reset the parameters to estimate the initial translation of the
        # body model
        body_model.reset_params(body_pose=body_mean_pose)

        # If the distance between the 2D shoulders is smaller than a
        # predefined threshold then try 2 fits, the initial one and a 180
        # degree rotation
        shoulder_dist = torch.dist(gt_joints[:, left_shoulder_idx],
                                   gt_joints[:, right_shoulder_idx])
        try_both_orient = shoulder_dist.item() < side_view_thsh

        # Update the value of the translation of the camera as well as
        # the image center.
        with torch.no_grad():
            camera.translation[:] = init_t.view_as(camera.translation)
            camera.center[:] = torch.tensor([W, H], dtype=dtype) * 0.5

        # Re-enable gradient calculation for the camera translation
        camera.translation.requires_grad = True

        camera_opt_params = [camera.translation, body_model.global_orient]

        camera_optimizer, camera_create_graph = optim_factory.create_optimizer(
            camera_opt_params,
            **kwargs)

        # The closure passed to the optimizer
        fit_camera = monitor.create_fitting_closure(
            camera_optimizer, body_model, camera, gt_joints,
            camera_loss, create_graph=camera_create_graph,
            use_vposer=use_vposer, vposer=vposer,
            pose_embedding=pose_embedding,
            return_full_pose=False, return_verts=False)

        # Step 1: Optimize over the torso joints the camera translation
        # Initialize the computational graph by feeding the initial translation
        # of the camera and the initial pose of the body model.
        camera_init_start = time.time()
        cam_init_loss_val = monitor.run_fitting(camera_optimizer,
                                                fit_camera,
                                                camera_opt_params, body_model,
                                                use_vposer=use_vposer,
                                                pose_embedding=pose_embedding,
                                                vposer=vposer)

        if interactive:
            if use_cuda and torch.cuda.is_available():
                torch.cuda.synchronize()
            tqdm.write('Camera initialization done after {:.4f}'.format(
                time.time() - camera_init_start))
            tqdm.write('Camera initialization final loss {:.4f}'.format(
                cam_init_loss_val))

        # If the 2D detections/positions of the shoulder joints are too
        # close the rotate the body by 180 degrees and also fit to that
        # orientation
        if try_both_orient:
            body_orient = body_model.global_orient.detach().cpu().numpy()
            flipped_orient = cv2.Rodrigues(body_orient)[0].dot(
                cv2.Rodrigues(np.array([0., np.pi, 0]))[0])
            flipped_orient = cv2.Rodrigues(flipped_orient)[0].ravel()

            flipped_orient = torch.tensor(flipped_orient,
                                          dtype=dtype,
                                          device=device).unsqueeze(dim=0)
            orientations = [body_orient, flipped_orient]
        else:
            orientations = [body_model.global_orient.detach().cpu().numpy()]

        # store here the final error for both orientations,
        # and pick the orientation resulting in the lowest error
        results = []

        # Step 2: Optimize the full model
        final_loss_val = 0
        for or_idx, orient in enumerate(tqdm(orientations, desc='Orientation')):
            opt_start = time.time()

            new_params = defaultdict(global_orient=orient,
                                     body_pose=body_mean_pose)
            body_model.reset_params(**new_params)
            if use_vposer:
                with torch.no_grad():
                    pose_embedding.fill_(0)

            for opt_idx, curr_weights in enumerate(tqdm(opt_weights, desc='Stage')):

                body_params = list(body_model.parameters())

                final_params = list(
                    filter(lambda x: x.requires_grad, body_params))

                if use_vposer:
                    final_params.append(pose_embedding)

                body_optimizer, body_create_graph = optim_factory.create_optimizer(
                    final_params,
                    **kwargs)
                body_optimizer.zero_grad()

                curr_weights['data_weight'] = data_weight
                curr_weights['bending_prior_weight'] = (
                    3.17 * curr_weights['body_pose_weight'])
                if use_hands:
                    joint_weights[:, 25:67] = curr_weights['hand_weight']
                if use_face:
                    joint_weights[:, 67:] = curr_weights['face_weight']
                loss.reset_loss_weights(curr_weights)

                closure = monitor.create_fitting_closure(
                    body_optimizer, body_model,
                    camera=camera, gt_joints=gt_joints,
                    joints_conf=joints_conf,
                    joint_weights=joint_weights,
                    loss=loss, create_graph=body_create_graph,
                    use_vposer=use_vposer, vposer=vposer,
                    pose_embedding=pose_embedding,
                    return_verts=True, return_full_pose=True)

                if interactive:
                    if use_cuda and torch.cuda.is_available():
                        torch.cuda.synchronize()
                    stage_start = time.time()
                final_loss_val = monitor.run_fitting(
                    body_optimizer,
                    closure, final_params,
                    body_model,
                    pose_embedding=pose_embedding, vposer=vposer,
                    use_vposer=use_vposer)

                if interactive:
                    if use_cuda and torch.cuda.is_available():
                        torch.cuda.synchronize()
                    elapsed = time.time() - stage_start
                    if interactive:
                        tqdm.write('Stage {:03d} done after {:.4f} seconds'.format(
                            opt_idx, elapsed))

            if interactive:
                if use_cuda and torch.cuda.is_available():
                    torch.cuda.synchronize()
                elapsed = time.time() - opt_start
                tqdm.write(
                    'Body fitting Orientation {} done after {:.4f} seconds'.format(
                        or_idx, elapsed))
                tqdm.write('Body final loss val = {:.5f}'.format(
                    final_loss_val))

            # Get the result of the fitting process
            # Store in it the errors list in order to compare multiple
            # orientations, if they exist
            result = {'camera_' + str(key): val.detach().cpu().numpy()
                      for key, val in camera.named_parameters()}
            result.update({key: val.detach().cpu().numpy()
                           for key, val in body_model.named_parameters()})
            if use_vposer:
                result['body_pose'] = pose_embedding.detach().cpu().numpy()

            results.append({'loss': final_loss_val,
                            'result': result})

        with open(result_fn, 'wb') as result_file:
            if len(results) > 1:
                min_idx = (0 if results[0]['loss'] < results[1]['loss']
                           else 1)
            else:
                min_idx = 0
            pickle.dump(results[min_idx]['result'], result_file, protocol=2)

    if save_meshes or visualize:
        body_pose = vposer.decode(
            pose_embedding,
            output_type='aa').view(1, -1) if use_vposer else None

        model_type = kwargs.get('model_type', 'smpl')
        append_wrists = model_type == 'smpl' and use_vposer
        if append_wrists:
                wrist_pose = torch.zeros([body_pose.shape[0], 6],
                                         dtype=body_pose.dtype,
                                         device=body_pose.device)
                body_pose = torch.cat([body_pose, wrist_pose], dim=1)

        model_output = body_model(return_verts=True, body_pose=body_pose)
        vertices = model_output.vertices.detach().cpu().numpy().squeeze()

        import trimesh

        out_mesh = trimesh.Trimesh(vertices, body_model.faces, process=False)
        rot = trimesh.transformations.rotation_matrix(
            np.radians(180), [1, 0, 0])
        out_mesh.apply_transform(rot)
        out_mesh.export(mesh_fn)

    if visualize:
        import pyrender

        material = pyrender.MetallicRoughnessMaterial(
            metallicFactor=0.0,
            alphaMode='OPAQUE',
            baseColorFactor=(1.0, 1.0, 0.9, 1.0))
        mesh = pyrender.Mesh.from_trimesh(
            out_mesh,
            material=material)

        scene = pyrender.Scene(bg_color=[0.0, 0.0, 0.0, 0.0],
                               ambient_light=(0.3, 0.3, 0.3))
        scene.add(mesh, 'mesh')

        camera_center = camera.center.detach().cpu().numpy().squeeze()
        camera_transl = camera.translation.detach().cpu().numpy().squeeze()
        # Equivalent to 180 degrees around the y-axis. Transforms the fit to
        # OpenGL compatible coordinate system.
        camera_transl[0] *= -1.0

        camera_pose = np.eye(4)
        camera_pose[:3, 3] = camera_transl

        camera = pyrender.camera.IntrinsicsCamera(
            fx=focal_length, fy=focal_length,
            cx=camera_center[0], cy=camera_center[1])
        scene.add(camera, pose=camera_pose)

        # Get the lights from the viewer
        light_nodes = monitor.mv.viewer._create_raymond_lights()
        for node in light_nodes:
            scene.add_node(node)

        r = pyrender.OffscreenRenderer(viewport_width=W,
                                       viewport_height=H,
                                       point_size=1.0)
        color, _ = r.render(scene, flags=pyrender.RenderFlags.RGBA)
        color = color.astype(np.float32) / 255.0

        valid_mask = (color[:, :, -1] > 0)[:, :, np.newaxis]
        input_img = img.detach().cpu().numpy()
        output_img = (color[:, :, :-1] * valid_mask +
                      (1 - valid_mask) * input_img)

        img = pil_img.fromarray((output_img * 255).astype(np.uint8))
        img.save(out_img_fn)


D:\Projects\smplify-x\smplifyx\main.py
# -*- coding: utf-8 -*-

# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is
# holder of all proprietary rights on this computer program.
# You can only use this computer program if you have closed
# a license agreement with MPG or you get the right to use the computer
# program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and
# liable to prosecution.
#
# Copyright©2019 Max-Planck-Gesellschaft zur Förderung
# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute
# for Intelligent Systems and the Max Planck Institute for Biological
# Cybernetics. All rights reserved.
#
# Contact: ps-license@tuebingen.mpg.de

from __future__ import absolute_import
from __future__ import print_function
from __future__ import division

import sys
import os

import os.path as osp

import time
import yaml
import torch

import smplx

from utils import JointMapper
from cmd_parser import parse_config
from data_parser import create_dataset
from fit_single_frame import fit_single_frame

from camera import create_camera
from prior import create_prior

torch.backends.cudnn.enabled = False


def main(**args):
    output_folder = args.pop('output_folder')
    output_folder = osp.expandvars(output_folder)
    if not osp.exists(output_folder):
        os.makedirs(output_folder)

    # Store the arguments for the current experiment
    conf_fn = osp.join(output_folder, 'conf.yaml')
    with open(conf_fn, 'w') as conf_file:
        yaml.dump(args, conf_file)

    result_folder = args.pop('result_folder', 'results')
    result_folder = osp.join(output_folder, result_folder)
    if not osp.exists(result_folder):
        os.makedirs(result_folder)

    mesh_folder = args.pop('mesh_folder', 'meshes')
    mesh_folder = osp.join(output_folder, mesh_folder)
    if not osp.exists(mesh_folder):
        os.makedirs(mesh_folder)

    out_img_folder = osp.join(output_folder, 'images')
    if not osp.exists(out_img_folder):
        os.makedirs(out_img_folder)

    float_dtype = args['float_dtype']
    if float_dtype == 'float64':
        dtype = torch.float64
    elif float_dtype == 'float32':
        dtype = torch.float64
    else:
        print('Unknown float type {}, exiting!'.format(float_dtype))
        sys.exit(-1)

    use_cuda = args.get('use_cuda', True)
    if use_cuda and not torch.cuda.is_available():
        print('CUDA is not available, exiting!')
        sys.exit(-1)

    img_folder = args.pop('img_folder', 'images')
    dataset_obj = create_dataset(img_folder=img_folder, **args)

    start = time.time()

    input_gender = args.pop('gender', 'neutral')
    gender_lbl_type = args.pop('gender_lbl_type', 'none')
    max_persons = args.pop('max_persons', -1)

    float_dtype = args.get('float_dtype', 'float32')
    if float_dtype == 'float64':
        dtype = torch.float64
    elif float_dtype == 'float32':
        dtype = torch.float32
    else:
        raise ValueError('Unknown float type {}, exiting!'.format(float_dtype))

    joint_mapper = JointMapper(dataset_obj.get_model2data())

    model_params = dict(model_path=args.get('model_folder'),
                        joint_mapper=joint_mapper,
                        create_global_orient=True,
                        create_body_pose=not args.get('use_vposer'),
                        create_betas=True,
                        create_left_hand_pose=True,
                        create_right_hand_pose=True,
                        create_expression=True,
                        create_jaw_pose=True,
                        create_leye_pose=True,
                        create_reye_pose=True,
                        create_transl=False,
                        dtype=dtype,
                        **args)

    male_model = smplx.create(gender='male', **model_params)
    # SMPL-H has no gender-neutral model
    if args.get('model_type') != 'smplh':
        neutral_model = smplx.create(gender='neutral', **model_params)
    female_model = smplx.create(gender='female', **model_params)

    # Create the camera object
    focal_length = args.get('focal_length')
    camera = create_camera(focal_length_x=focal_length,
                           focal_length_y=focal_length,
                           dtype=dtype,
                           **args)

    if hasattr(camera, 'rotation'):
        camera.rotation.requires_grad = False

    use_hands = args.get('use_hands', True)
    use_face = args.get('use_face', True)

    body_pose_prior = create_prior(
        prior_type=args.get('body_prior_type'),
        dtype=dtype,
        **args)

    jaw_prior, expr_prior = None, None
    if use_face:
        jaw_prior = create_prior(
            prior_type=args.get('jaw_prior_type'),
            dtype=dtype,
            **args)
        expr_prior = create_prior(
            prior_type=args.get('expr_prior_type', 'l2'),
            dtype=dtype, **args)

    left_hand_prior, right_hand_prior = None, None
    if use_hands:
        lhand_args = args.copy()
        lhand_args['num_gaussians'] = args.get('num_pca_comps')
        left_hand_prior = create_prior(
            prior_type=args.get('left_hand_prior_type'),
            dtype=dtype,
            use_left_hand=True,
            **lhand_args)

        rhand_args = args.copy()
        rhand_args['num_gaussians'] = args.get('num_pca_comps')
        right_hand_prior = create_prior(
            prior_type=args.get('right_hand_prior_type'),
            dtype=dtype,
            use_right_hand=True,
            **rhand_args)

    shape_prior = create_prior(
        prior_type=args.get('shape_prior_type', 'l2'),
        dtype=dtype, **args)

    angle_prior = create_prior(prior_type='angle', dtype=dtype)

    if use_cuda and torch.cuda.is_available():
        device = torch.device('cuda')

        camera = camera.to(device=device)
        female_model = female_model.to(device=device)
        male_model = male_model.to(device=device)
        if args.get('model_type') != 'smplh':
            neutral_model = neutral_model.to(device=device)
        body_pose_prior = body_pose_prior.to(device=device)
        angle_prior = angle_prior.to(device=device)
        shape_prior = shape_prior.to(device=device)
        if use_face:
            expr_prior = expr_prior.to(device=device)
            jaw_prior = jaw_prior.to(device=device)
        if use_hands:
            left_hand_prior = left_hand_prior.to(device=device)
            right_hand_prior = right_hand_prior.to(device=device)
    else:
        device = torch.device('cpu')

    # A weight for every joint of the model
    joint_weights = dataset_obj.get_joint_weights().to(device=device,
                                                       dtype=dtype)
    # Add a fake batch dimension for broadcasting
    joint_weights.unsqueeze_(dim=0)

    for idx, data in enumerate(dataset_obj):

        img = data['img']
        fn = data['fn']
        keypoints = data['keypoints']
        print('Processing: {}'.format(data['img_path']))

        curr_result_folder = osp.join(result_folder, fn)
        if not osp.exists(curr_result_folder):
            os.makedirs(curr_result_folder)
        curr_mesh_folder = osp.join(mesh_folder, fn)
        if not osp.exists(curr_mesh_folder):
            os.makedirs(curr_mesh_folder)
        for person_id in range(keypoints.shape[0]):
            if person_id >= max_persons and max_persons > 0:
                continue

            curr_result_fn = osp.join(curr_result_folder,
                                      '{:03d}.pkl'.format(person_id))
            curr_mesh_fn = osp.join(curr_mesh_folder,
                                    '{:03d}.obj'.format(person_id))

            curr_img_folder = osp.join(output_folder, 'images', fn,
                                       '{:03d}'.format(person_id))
            if not osp.exists(curr_img_folder):
                os.makedirs(curr_img_folder)

            if gender_lbl_type != 'none':
                if gender_lbl_type == 'pd' and 'gender_pd' in data:
                    gender = data['gender_pd'][person_id]
                if gender_lbl_type == 'gt' and 'gender_gt' in data:
                    gender = data['gender_gt'][person_id]
            else:
                gender = input_gender

            if gender == 'neutral':
                body_model = neutral_model
            elif gender == 'female':
                body_model = female_model
            elif gender == 'male':
                body_model = male_model

            out_img_fn = osp.join(curr_img_folder, 'output.png')

            fit_single_frame(img, keypoints[[person_id]],
                             body_model=body_model,
                             camera=camera,
                             joint_weights=joint_weights,
                             dtype=dtype,
                             output_folder=output_folder,
                             result_folder=curr_result_folder,
                             out_img_fn=out_img_fn,
                             result_fn=curr_result_fn,
                             mesh_fn=curr_mesh_fn,
                             shape_prior=shape_prior,
                             expr_prior=expr_prior,
                             body_pose_prior=body_pose_prior,
                             left_hand_prior=left_hand_prior,
                             right_hand_prior=right_hand_prior,
                             jaw_prior=jaw_prior,
                             angle_prior=angle_prior,
                             **args)

    elapsed = time.time() - start
    time_msg = time.strftime('%H hours, %M minutes, %S seconds',
                             time.gmtime(elapsed))
    print('Processing the data took: {}'.format(time_msg))


if __name__ == "__main__":
    args = parse_config()
    main(**args)


D:\Projects\smplify-x\smplifyx\mesh_viewer.py
# -*- coding: utf-8 -*-

# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is
# holder of all proprietary rights on this computer program.
# You can only use this computer program if you have closed
# a license agreement with MPG or you get the right to use the computer
# program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and
# liable to prosecution.
#
# Copyright©2019 Max-Planck-Gesellschaft zur Förderung
# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute
# for Intelligent Systems and the Max Planck Institute for Biological
# Cybernetics. All rights reserved.
#
# Contact: ps-license@tuebingen.mpg.de

from __future__ import absolute_import
from __future__ import print_function
from __future__ import division


import numpy as np


class MeshViewer(object):

    def __init__(self, width=1200, height=800,
                 body_color=(1.0, 1.0, 0.9, 1.0),
                 registered_keys=None):
        super(MeshViewer, self).__init__()

        if registered_keys is None:
            registered_keys = dict()

        import trimesh
        import pyrender

        self.mat_constructor = pyrender.MetallicRoughnessMaterial
        self.mesh_constructor = trimesh.Trimesh
        self.trimesh_to_pymesh = pyrender.Mesh.from_trimesh
        self.transf = trimesh.transformations.rotation_matrix

        self.body_color = body_color
        self.scene = pyrender.Scene(bg_color=[0.0, 0.0, 0.0, 1.0],
                                    ambient_light=(0.3, 0.3, 0.3))

        pc = pyrender.PerspectiveCamera(yfov=np.pi / 3.0,
                                        aspectRatio=float(width) / height)
        camera_pose = np.eye(4)
        camera_pose[:3, 3] = np.array([0, 0, 3])
        self.scene.add(pc, pose=camera_pose)

        self.viewer = pyrender.Viewer(self.scene, use_raymond_lighting=True,
                                      viewport_size=(width, height),
                                      cull_faces=False,
                                      run_in_thread=True,
                                      registered_keys=registered_keys)

    def is_active(self):
        return self.viewer.is_active

    def close_viewer(self):
        if self.viewer.is_active:
            self.viewer.close_external()

    def create_mesh(self, vertices, faces, color=(0.3, 0.3, 0.3, 1.0),
                    wireframe=False):

        material = self.mat_constructor(
            metallicFactor=0.0,
            alphaMode='BLEND',
            baseColorFactor=color)

        mesh = self.mesh_constructor(vertices, faces)

        rot = self.transf(np.radians(180), [1, 0, 0])
        mesh.apply_transform(rot)

        return self.trimesh_to_pymesh(mesh, material=material)

    def update_mesh(self, vertices, faces):
        if not self.viewer.is_active:
            return

        self.viewer.render_lock.acquire()

        for node in self.scene.get_nodes():
            if node.name == 'body_mesh':
                self.scene.remove_node(node)
                break

        body_mesh = self.create_mesh(
            vertices, faces, color=self.body_color)
        self.scene.add(body_mesh, name='body_mesh')

        self.viewer.render_lock.release()


D:\Projects\smplify-x\smplifyx\optimizers\lbfgs_ls.py
# PyTorch implementation of L-BFGS with Strong Wolfe line search
# Will be removed once https://github.com/pytorch/pytorch/pull/8824
# is merged

import torch
from functools import reduce

from torch.optim import Optimizer


def _cubic_interpolate(x1, f1, g1, x2, f2, g2, bounds=None):
    # ported from https://github.com/torch/optim/blob/master/polyinterp.lua
    # Compute bounds of interpolation area
    if bounds is not None:
        xmin_bound, xmax_bound = bounds
    else:
        xmin_bound, xmax_bound = (x1, x2) if x1 <= x2 else (x2, x1)

    # Code for most common case: cubic interpolation of 2 points
    #   w/ function and derivative values for both
    # Solution in this case (where x2 is the farthest point):
    #   d1 = g1 + g2 - 3*(f1-f2)/(x1-x2);
    #   d2 = sqrt(d1^2 - g1*g2);
    #   min_pos = x2 - (x2 - x1)*((g2 + d2 - d1)/(g2 - g1 + 2*d2));
    #   t_new = min(max(min_pos,xmin_bound),xmax_bound);
    d1 = g1 + g2 - 3 * (f1 - f2) / (x1 - x2)
    d2_square = d1 ** 2 - g1 * g2
    if d2_square >= 0:
        d2 = d2_square.sqrt()
        if x1 <= x2:
            min_pos = x2 - (x2 - x1) * ((g2 + d2 - d1) / (g2 - g1 + 2 * d2))
        else:
            min_pos = x1 - (x1 - x2) * ((g1 + d2 - d1) / (g1 - g2 + 2 * d2))
        return min(max(min_pos, xmin_bound), xmax_bound)
    else:
        return (xmin_bound + xmax_bound) / 2.


def _strong_Wolfe(obj_func, x, t, d, f, g, gtd, c1=1e-4, c2=0.9, tolerance_change=1e-9,
                  max_iter=20,
                  max_ls=25):
    # ported from https://github.com/torch/optim/blob/master/lswolfe.lua
    d_norm = d.abs().max()
    g = g.clone()
    # evaluate objective and gradient using initial step
    f_new, g_new = obj_func(x, t, d)
    ls_func_evals = 1
    gtd_new = g_new.dot(d)

    # bracket an interval containing a point satisfying the Wolfe criteria
    t_prev, f_prev, g_prev, gtd_prev = 0, f, g, gtd
    done = False
    ls_iter = 0
    while ls_iter < max_ls:
        # check conditions
        if f_new > (f + c1 * t * gtd) or (ls_iter > 1 and f_new >= f_prev):
            bracket = [t_prev, t]
            bracket_f = [f_prev, f_new]
            bracket_g = [g_prev, g_new.clone()]
            bracket_gtd = [gtd_prev, gtd_new]
            break

        if abs(gtd_new) <= -c2 * gtd:
            bracket = [t]
            bracket_f = [f_new]
            bracket_g = [g_new]
            bracket_gtd = [gtd_new]
            done = True
            break

        if gtd_new >= 0:
            bracket = [t_prev, t]
            bracket_f = [f_prev, f_new]
            bracket_g = [g_prev, g_new.clone()]
            bracket_gtd = [gtd_prev, gtd_new]
            break

        # interpolate
        min_step = t + 0.01 * (t - t_prev)
        max_step = t * 10
        tmp = t
        t = _cubic_interpolate(t_prev, f_prev, gtd_prev, t, f_new, gtd_new,
                               bounds=(min_step, max_step))

        # next step
        t_prev = tmp
        f_prev = f_new
        g_prev = g_new.clone()
        gtd_prev = gtd_new
        f_new, g_new = obj_func(x, t, d)
        ls_func_evals += 1
        gtd_new = g_new.dot(d)
        ls_iter += 1

    # reached max number of iterations?
    if ls_iter == max_ls:
        bracket = [0, t]
        bracket_f = [f, f_new]
        bracket_g = [g, g_new]
        bracket_gtd = [gtd, gtd_new]

    # zoom phase: we now have a point satisfying the criteria, or
    # a bracket around it. We refine the bracket until we find the
    # exact point satisfying the criteria
    insuf_progress = False
    # find high and low points in bracket
    low_pos, high_pos = (0, 1) if bracket_f[0] <= bracket_f[-1] else (1, 0)
    while not done and ls_iter < max_iter:
        # compute new trial value
        t = _cubic_interpolate(bracket[0], bracket_f[0], bracket_gtd[0],
                               bracket[1], bracket_f[1], bracket_gtd[1])

        # test what we are making sufficient progress
        eps = 0.1 * (max(bracket) - min(bracket))
        if min(max(bracket) - t, t - min(bracket)) < eps:
            # interpolation close to boundary
            if insuf_progress or t >= max(bracket) or t <= min(bracket):
                # evaluate at 0.1 away from boundary
                if abs(t - max(bracket)) < abs(t - min(bracket)):
                    t = max(bracket) - eps
                else:
                    t = min(bracket) + eps
                insuf_progress = False
            else:
                insuf_progress = True
        else:
            insuf_progress = False

        # Evaluate new point
        f_new, g_new = obj_func(x, t, d)
        ls_func_evals += 1
        gtd_new = g_new.dot(d)
        ls_iter += 1

        if f_new > (f + c1 * t * gtd) or f_new >= bracket_f[low_pos]:
            # Armijo condition not satisfied or not lower than lowest point
            bracket[high_pos] = t
            bracket_f[high_pos] = f_new
            bracket_g[high_pos] = g_new.clone()
            bracket_gtd[high_pos] = gtd_new
            low_pos, high_pos = (0, 1) if bracket_f[0] <= bracket_f[1] else (1, 0)
        else:
            if abs(gtd_new) <= -c2 * gtd:
                # Wolfe conditions satisfied
                done = True
            elif gtd_new * (bracket[high_pos] - bracket[low_pos]) >= 0:
                # old high becomes new low
                bracket[high_pos] = bracket[low_pos]
                bracket_f[high_pos] = bracket_f[low_pos]
                bracket_g[high_pos] = bracket_g[low_pos]
                bracket_gtd[high_pos] = bracket_gtd[low_pos]

            # new point becomes new low
            bracket[low_pos] = t
            bracket_f[low_pos] = f_new
            bracket_g[low_pos] = g_new.clone()
            bracket_gtd[low_pos] = gtd_new

        # line-search bracket is so small
        if abs(bracket[1] - bracket[0]) * d_norm < tolerance_change:
            break

    # return stuff
    t = bracket[low_pos]
    f_new = bracket_f[low_pos]
    g_new = bracket_g[low_pos]
    return f_new, g_new, t, ls_func_evals


# LBFGS with strong Wolfe line search introduces in PR #8824
# Will be removed once merged with master
class LBFGS(Optimizer):
    """Implements L-BFGS algorithm, heavily inspired by `minFunc
    <https://www.cs.ubc.ca/~schmidtm/Software/minFunc.html>`.
    .. warning::
        This optimizer doesn't support per-parameter options and parameter
        groups (there can be only one).
    .. warning::
        Right now all parameters have to be on a single device. This will be
        improved in the future.
    .. note::
        This is a very memory intensive optimizer (it requires additional
        ``param_bytes * (history_size + 1)`` bytes). If it doesn't fit in memory
        try reducing the history size, or use a different algorithm.
    Arguments:
        lr (float): learning rate (default: 1)
        max_iter (int): maximal number of iterations per optimization step
            (default: 20)
        max_eval (int): maximal number of function evaluations per optimization
            step (default: max_iter * 1.25).
        tolerance_grad (float): termination tolerance on first order optimality
            (default: 1e-5).
        tolerance_change (float): termination tolerance on function
            value/parameter changes (default: 1e-9).
        history_size (int): update history size (default: 100).
        line_search_fn (str): either 'strong_Wolfe' or None (default: None).
    """

    def __init__(self, params, lr=1, max_iter=20, max_eval=None,
                 tolerance_grad=1e-5, tolerance_change=1e-9, history_size=100,
                 line_search_fn=None):
        if max_eval is None:
            max_eval = max_iter * 5 // 4
        defaults = dict(lr=lr, max_iter=max_iter, max_eval=max_eval,
                        tolerance_grad=tolerance_grad, tolerance_change=tolerance_change,
                        history_size=history_size, line_search_fn=line_search_fn)
        super(LBFGS, self).__init__(params, defaults)

        if len(self.param_groups) != 1:
            raise ValueError("LBFGS doesn't support per-parameter options "
                             "(parameter groups)")

        self._params = self.param_groups[0]['params']
        self._numel_cache = None

    def _numel(self):
        if self._numel_cache is None:
            self._numel_cache = reduce(lambda total, p: total + p.numel(), self._params, 0)
        return self._numel_cache

    def _gather_flat_grad(self):
        views = []
        for p in self._params:
            if p.grad is None:
                view = p.new(p.numel()).zero_()
            elif p.grad.is_sparse:
                view = p.grad.to_dense().view(-1)
            else:
                view = p.grad.view(-1)
            views.append(view)
        return torch.cat(views, 0)

    def _add_grad(self, step_size, update):
        offset = 0
        for p in self._params:
            numel = p.numel()
            # view as to avoid deprecated pointwise semantics
            p.data.add_(step_size, update[offset:offset + numel].view_as(p.data))
            offset += numel
        assert offset == self._numel()

    def _clone_param(self):
        return [p.clone() for p in self._params]

    def _set_param(self, params_data):
        for p, pdata in zip(self._params, params_data):
            p.data.copy_(pdata)

    def _directional_evaluate(self, closure, x, t, d):
        self._add_grad(t, d)
        loss = float(closure())
        flat_grad = self._gather_flat_grad()
        self._set_param(x)
        return loss, flat_grad

    def step(self, closure):
        """Performs a single optimization step.
        Arguments:
            closure (callable): A closure that reevaluates the model
                and returns the loss.
        """
        assert len(self.param_groups) == 1

        group = self.param_groups[0]
        lr = group['lr']
        max_iter = group['max_iter']
        max_eval = group['max_eval']
        tolerance_grad = group['tolerance_grad']
        tolerance_change = group['tolerance_change']
        line_search_fn = group['line_search_fn']
        history_size = group['history_size']

        # NOTE: LBFGS has only global state, but we register it as state for
        # the first param, because this helps with casting in load_state_dict
        state = self.state[self._params[0]]
        state.setdefault('func_evals', 0)
        state.setdefault('n_iter', 0)

        # evaluate initial f(x) and df/dx
        orig_loss = closure()
        loss = float(orig_loss)
        current_evals = 1
        state['func_evals'] += 1

        flat_grad = self._gather_flat_grad()
        opt_cond = flat_grad.abs().max() <= tolerance_grad

        # optimal condition
        if opt_cond:
            return orig_loss

        # tensors cached in state (for tracing)
        d = state.get('d')
        t = state.get('t')
        old_dirs = state.get('old_dirs')
        old_stps = state.get('old_stps')
        ro = state.get('ro')
        H_diag = state.get('H_diag')
        prev_flat_grad = state.get('prev_flat_grad')
        prev_loss = state.get('prev_loss')

        n_iter = 0
        # optimize for a max of max_iter iterations
        while n_iter < max_iter:
            # keep track of nb of iterations
            n_iter += 1
            state['n_iter'] += 1

            ############################################################
            # compute gradient descent direction
            ############################################################
            if state['n_iter'] == 1:
                d = flat_grad.neg()
                old_dirs = []
                old_stps = []
                ro = []
                H_diag = 1
            else:
                # do lbfgs update (update memory)
                y = flat_grad.sub(prev_flat_grad)
                s = d.mul(t)
                ys = y.dot(s)  # y*s
                if ys > 1e-10:
                    # updating memory
                    if len(old_dirs) == history_size:
                        # shift history by one (limited-memory)
                        old_dirs.pop(0)
                        old_stps.pop(0)
                        ro.pop(0)

                    # store new direction/step
                    old_dirs.append(y)
                    old_stps.append(s)
                    ro.append(1. / ys)

                    # update scale of initial Hessian approximation
                    H_diag = ys / y.dot(y)  # (y*y)

                # compute the approximate (L-BFGS) inverse Hessian
                # multiplied by the gradient
                num_old = len(old_dirs)

                if 'al' not in state:
                    state['al'] = [None] * history_size
                al = state['al']

                # iteration in L-BFGS loop collapsed to use just one buffer
                q = flat_grad.neg()
                for i in range(num_old - 1, -1, -1):
                    al[i] = old_stps[i].dot(q) * ro[i]
                    q.add_(-al[i], old_dirs[i])

                # multiply by initial Hessian
                # r/d is the final direction
                d = r = torch.mul(q, H_diag)
                for i in range(num_old):
                    be_i = old_dirs[i].dot(r) * ro[i]
                    r.add_(al[i] - be_i, old_stps[i])

            if prev_flat_grad is None:
                prev_flat_grad = flat_grad.clone()
            else:
                prev_flat_grad.copy_(flat_grad)
            prev_loss = loss

            ############################################################
            # compute step length
            ############################################################
            # reset initial guess for step size
            if state['n_iter'] == 1:
                t = min(1., 1. / flat_grad.abs().sum()) * lr
            else:
                t = lr

            # directional derivative
            gtd = flat_grad.dot(d)  # g * d

            # directional derivative is below tolerance
            if gtd > -tolerance_change:
                break

            # optional line search: user function
            ls_func_evals = 0
            if line_search_fn is not None:
                # perform line search, using user function
                if line_search_fn != "strong_Wolfe":
                    raise RuntimeError("only 'strong_Wolfe' is supported")
                else:
                    x_init = self._clone_param()

                    def obj_func(x, t, d):
                        return self._directional_evaluate(closure, x, t, d)
                    loss, flat_grad, t, ls_func_evals = _strong_Wolfe(obj_func, x_init, t, d,
                                                                      loss,
                                                                      flat_grad,
                                                                      gtd,
                                                                      max_iter=max_iter)
                self._add_grad(t, d)
                opt_cond = flat_grad.abs().max() <= tolerance_grad
            else:
                # no line search, simply move with fixed-step
                self._add_grad(t, d)
                if n_iter != max_iter:
                    # re-evaluate function only if not in last iteration
                    # the reason we do this: in a stochastic setting,
                    # no use to re-evaluate that function here
                    loss = float(closure())
                    flat_grad = self._gather_flat_grad()
                    opt_cond = flat_grad.abs().max() <= tolerance_grad
                    ls_func_evals = 1

            # update func eval
            current_evals += ls_func_evals
            state['func_evals'] += ls_func_evals

            ############################################################
            # check conditions
            ############################################################
            if n_iter == max_iter:
                break

            if current_evals >= max_eval:
                break

            # optimal condition
            if opt_cond:
                break

            # lack of progress
            if d.mul(t).abs().max() <= tolerance_change:
                break

            if abs(loss - prev_loss) < tolerance_change:
                break

        state['d'] = d
        state['t'] = t
        state['old_dirs'] = old_dirs
        state['old_stps'] = old_stps
        state['ro'] = ro
        state['H_diag'] = H_diag
        state['prev_flat_grad'] = prev_flat_grad
        state['prev_loss'] = prev_loss

        return orig_loss


D:\Projects\smplify-x\smplifyx\optimizers\optim_factory.py
# -*- coding: utf-8 -*-

# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is
# holder of all proprietary rights on this computer program.
# You can only use this computer program if you have closed
# a license agreement with MPG or you get the right to use the computer
# program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and
# liable to prosecution.
#
# Copyright©2019 Max-Planck-Gesellschaft zur Förderung
# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute
# for Intelligent Systems and the Max Planck Institute for Biological
# Cybernetics. All rights reserved.
#
# Contact: ps-license@tuebingen.mpg.de


from __future__ import absolute_import
from __future__ import print_function
from __future__ import division

import torch.optim as optim
from .lbfgs_ls import LBFGS as LBFGSLs


def create_optimizer(parameters, optim_type='lbfgs',
                     lr=1e-3,
                     momentum=0.9,
                     use_nesterov=True,
                     beta1=0.9,
                     beta2=0.999,
                     epsilon=1e-8,
                     use_locking=False,
                     weight_decay=0.0,
                     centered=False,
                     rmsprop_alpha=0.99,
                     maxiters=20,
                     gtol=1e-6,
                     ftol=1e-9,
                     **kwargs):
    ''' Creates the optimizer
    '''
    if optim_type == 'adam':
        return (optim.Adam(parameters, lr=lr, betas=(beta1, beta2),
                           weight_decay=weight_decay),
                False)
    elif optim_type == 'lbfgs':
        return (optim.LBFGS(parameters, lr=lr, max_iter=maxiters), False)
    elif optim_type == 'lbfgsls':
        return LBFGSLs(parameters, lr=lr, max_iter=maxiters,
                       line_search_fn='strong_Wolfe'), False
    elif optim_type == 'rmsprop':
        return (optim.RMSprop(parameters, lr=lr, epsilon=epsilon,
                              alpha=rmsprop_alpha,
                              weight_decay=weight_decay,
                              momentum=momentum, centered=centered),
                False)
    elif optim_type == 'sgd':
        return (optim.SGD(parameters, lr=lr, momentum=momentum,
                          weight_decay=weight_decay,
                          nesterov=use_nesterov),
                False)
    else:
        raise ValueError('Optimizer {} not supported!'.format(optim_type))


D:\Projects\smplify-x\smplifyx\optimizers\__init__.py
# -*- coding: utf-8 -*-

# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is
# holder of all proprietary rights on this computer program.
# You can only use this computer program if you have closed
# a license agreement with MPG or you get the right to use the computer
# program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and
# liable to prosecution.
#
# Copyright©2019 Max-Planck-Gesellschaft zur Förderung
# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute
# for Intelligent Systems and the Max Planck Institute for Biological
# Cybernetics. All rights reserved.
#
# Contact: ps-license@tuebingen.mpg.de


D:\Projects\smplify-x\smplifyx\prior.py
# -*- coding: utf-8 -*-

# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is
# holder of all proprietary rights on this computer program.
# You can only use this computer program if you have closed
# a license agreement with MPG or you get the right to use the computer
# program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and
# liable to prosecution.
#
# Copyright©2019 Max-Planck-Gesellschaft zur Förderung
# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute
# for Intelligent Systems and the Max Planck Institute for Biological
# Cybernetics. All rights reserved.
#
# Contact: ps-license@tuebingen.mpg.de

from __future__ import absolute_import
from __future__ import print_function
from __future__ import division

import sys
import os

import time
import pickle

import numpy as np

import torch
import torch.nn as nn

DEFAULT_DTYPE = torch.float32


def create_prior(prior_type, **kwargs):
    if prior_type == 'gmm':
        prior = MaxMixturePrior(**kwargs)
    elif prior_type == 'l2':
        return L2Prior(**kwargs)
    elif prior_type == 'angle':
        return SMPLifyAnglePrior(**kwargs)
    elif prior_type == 'none' or prior_type is None:
        # Don't use any pose prior
        def no_prior(*args, **kwargs):
            return 0.0
        prior = no_prior
    else:
        raise ValueError('Prior {}'.format(prior_type) + ' is not implemented')
    return prior


class SMPLifyAnglePrior(nn.Module):
    def __init__(self, dtype=torch.float32, **kwargs):
        super(SMPLifyAnglePrior, self).__init__()

        # Indices for the roration angle of
        # 55: left elbow,  90deg bend at -np.pi/2
        # 58: right elbow, 90deg bend at np.pi/2
        # 12: left knee,   90deg bend at np.pi/2
        # 15: right knee,  90deg bend at np.pi/2
        angle_prior_idxs = np.array([55, 58, 12, 15], dtype=np.int64)
        angle_prior_idxs = torch.tensor(angle_prior_idxs, dtype=torch.long)
        self.register_buffer('angle_prior_idxs', angle_prior_idxs)

        angle_prior_signs = np.array([1, -1, -1, -1],
                                     dtype=np.float32 if dtype == torch.float32
                                     else np.float64)
        angle_prior_signs = torch.tensor(angle_prior_signs,
                                         dtype=dtype)
        self.register_buffer('angle_prior_signs', angle_prior_signs)

    def forward(self, pose, with_global_pose=False):
        ''' Returns the angle prior loss for the given pose

        Args:
            pose: (Bx[23 + 1] * 3) torch tensor with the axis-angle
            representation of the rotations of the joints of the SMPL model.
        Kwargs:
            with_global_pose: Whether the pose vector also contains the global
            orientation of the SMPL model. If not then the indices must be
            corrected.
        Returns:
            A sze (B) tensor containing the angle prior loss for each element
            in the batch.
        '''
        angle_prior_idxs = self.angle_prior_idxs - (not with_global_pose) * 3
        return torch.exp(pose[:, angle_prior_idxs] *
                         self.angle_prior_signs).pow(2)


class L2Prior(nn.Module):
    def __init__(self, dtype=DEFAULT_DTYPE, reduction='sum', **kwargs):
        super(L2Prior, self).__init__()

    def forward(self, module_input, *args):
        return torch.sum(module_input.pow(2))


class MaxMixturePrior(nn.Module):

    def __init__(self, prior_folder='prior',
                 num_gaussians=6, dtype=DEFAULT_DTYPE, epsilon=1e-16,
                 use_merged=True,
                 **kwargs):
        super(MaxMixturePrior, self).__init__()

        if dtype == DEFAULT_DTYPE:
            np_dtype = np.float32
        elif dtype == torch.float64:
            np_dtype = np.float64
        else:
            print('Unknown float type {}, exiting!'.format(dtype))
            sys.exit(-1)

        self.num_gaussians = num_gaussians
        self.epsilon = epsilon
        self.use_merged = use_merged
        gmm_fn = 'gmm_{:02d}.pkl'.format(num_gaussians)

        full_gmm_fn = os.path.join(prior_folder, gmm_fn)
        if not os.path.exists(full_gmm_fn):
            print('The path to the mixture prior "{}"'.format(full_gmm_fn) +
                  ' does not exist, exiting!')
            sys.exit(-1)

        with open(full_gmm_fn, 'rb') as f:
            gmm = pickle.load(f, encoding='latin1')

        if type(gmm) == dict:
            means = gmm['means'].astype(np_dtype)
            covs = gmm['covars'].astype(np_dtype)
            weights = gmm['weights'].astype(np_dtype)
        elif 'sklearn.mixture.gmm.GMM' in str(type(gmm)):
            means = gmm.means_.astype(np_dtype)
            covs = gmm.covars_.astype(np_dtype)
            weights = gmm.weights_.astype(np_dtype)
        else:
            print('Unknown type for the prior: {}, exiting!'.format(type(gmm)))
            sys.exit(-1)

        self.register_buffer('means', torch.tensor(means, dtype=dtype))

        self.register_buffer('covs', torch.tensor(covs, dtype=dtype))

        precisions = [np.linalg.inv(cov) for cov in covs]
        precisions = np.stack(precisions).astype(np_dtype)

        self.register_buffer('precisions',
                             torch.tensor(precisions, dtype=dtype))

        # The constant term:
        sqrdets = np.array([(np.sqrt(np.linalg.det(c)))
                            for c in gmm['covars']])
        const = (2 * np.pi)**(69 / 2.)

        nll_weights = np.asarray(gmm['weights'] / (const *
                                                   (sqrdets / sqrdets.min())))
        nll_weights = torch.tensor(nll_weights, dtype=dtype).unsqueeze(dim=0)
        self.register_buffer('nll_weights', nll_weights)

        weights = torch.tensor(gmm['weights'], dtype=dtype).unsqueeze(dim=0)
        self.register_buffer('weights', weights)

        self.register_buffer('pi_term',
                             torch.log(torch.tensor(2 * np.pi, dtype=dtype)))

        cov_dets = [np.log(np.linalg.det(cov.astype(np_dtype)) + epsilon)
                    for cov in covs]
        self.register_buffer('cov_dets',
                             torch.tensor(cov_dets, dtype=dtype))

        # The dimensionality of the random variable
        self.random_var_dim = self.means.shape[1]

    def get_mean(self):
        ''' Returns the mean of the mixture '''
        mean_pose = torch.matmul(self.weights, self.means)
        return mean_pose

    def merged_log_likelihood(self, pose, betas):
        diff_from_mean = pose.unsqueeze(dim=1) - self.means

        prec_diff_prod = torch.einsum('mij,bmj->bmi',
                                      [self.precisions, diff_from_mean])
        diff_prec_quadratic = (prec_diff_prod * diff_from_mean).sum(dim=-1)

        curr_loglikelihood = 0.5 * diff_prec_quadratic - \
            torch.log(self.nll_weights)
        #  curr_loglikelihood = 0.5 * (self.cov_dets.unsqueeze(dim=0) +
        #  self.random_var_dim * self.pi_term +
        #  diff_prec_quadratic
        #  ) - torch.log(self.weights)

        min_likelihood, _ = torch.min(curr_loglikelihood, dim=1)
        return min_likelihood

    def log_likelihood(self, pose, betas, *args, **kwargs):
        ''' Create graph operation for negative log-likelihood calculation
        '''
        likelihoods = []

        for idx in range(self.num_gaussians):
            mean = self.means[idx]
            prec = self.precisions[idx]
            cov = self.covs[idx]
            diff_from_mean = pose - mean

            curr_loglikelihood = torch.einsum('bj,ji->bi',
                                              [diff_from_mean, prec])
            curr_loglikelihood = torch.einsum('bi,bi->b',
                                              [curr_loglikelihood,
                                               diff_from_mean])
            cov_term = torch.log(torch.det(cov) + self.epsilon)
            curr_loglikelihood += 0.5 * (cov_term +
                                         self.random_var_dim *
                                         self.pi_term)
            likelihoods.append(curr_loglikelihood)

        log_likelihoods = torch.stack(likelihoods, dim=1)
        min_idx = torch.argmin(log_likelihoods, dim=1)
        weight_component = self.nll_weights[:, min_idx]
        weight_component = -torch.log(weight_component)

        return weight_component + log_likelihoods[:, min_idx]

    def forward(self, pose, betas):
        if self.use_merged:
            return self.merged_log_likelihood(pose, betas)
        else:
            return self.log_likelihood(pose, betas)


D:\Projects\smplify-x\smplifyx\render_pkl.py
# -*- coding: utf-8 -*-

# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is
# holder of all proprietary rights on this computer program.
# You can only use this computer program if you have closed
# a license agreement with MPG or you get the right to use the computer
# program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and
# liable to prosecution.
#
# Copyright©2019 Max-Planck-Gesellschaft zur Förderung
# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute
# for Intelligent Systems. All rights reserved.
#
# Contact: ps-license@tuebingen.mpg.de
# Contact: Vassilis choutas, vassilis.choutas@tuebingen.mpg.de

import os
import os.path as osp

import argparse
import pickle
import torch
import smplx

from cmd_parser import parse_config
from human_body_prior.tools.model_loader import load_vposer

from utils import JointMapper
import pyrender
import trimesh

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--pkl', nargs='+', type=str, required=True,
                        help='The pkl files that will be read')

    args, remaining = parser.parse_known_args()

    pkl_paths = args.pkl

    args = parse_config(remaining)
    dtype = torch.float32
    use_cuda = args.get('use_cuda', True)
    if use_cuda and torch.cuda.is_available():
        device = torch.device('cuda')
    else:
        device = torch.device('cpu')

    model_type = args.get('model_type', 'smplx')
    print('Model type:', model_type)
    print(args.get('model_folder'))
    model_params = dict(model_path=args.get('model_folder'),
                        #  joint_mapper=joint_mapper,
                        create_global_orient=True,
                        create_body_pose=not args.get('use_vposer'),
                        create_betas=True,
                        create_left_hand_pose=True,
                        create_right_hand_pose=True,
                        create_expression=True,
                        create_jaw_pose=True,
                        create_leye_pose=True,
                        create_reye_pose=True,
                        create_transl=False,
                        dtype=dtype,
                        **args)

    model = smplx.create(**model_params)
    model = model.to(device=device)

    batch_size = args.get('batch_size', 1)
    use_vposer = args.get('use_vposer', True)
    vposer, pose_embedding = [None, ] * 2
    vposer_ckpt = args.get('vposer_ckpt', '')
    if use_vposer:
        pose_embedding = torch.zeros([batch_size, 32],
                                     dtype=dtype, device=device,
                                     requires_grad=True)

        vposer_ckpt = osp.expandvars(vposer_ckpt)
        vposer, _ = load_vposer(vposer_ckpt, vp_model='snapshot')
        vposer = vposer.to(device=device)
        vposer.eval()

    for pkl_path in pkl_paths:
        with open(pkl_path, 'rb') as f:
            data = pickle.load(f, encoding='latin1')
        if use_vposer:
            with torch.no_grad():
                pose_embedding[:] = torch.tensor(
                    data['body_pose'], device=device, dtype=dtype)

        est_params = {}
        for key, val in data.items():
            if key == 'body_pose' and use_vposer:
                body_pose = vposer.decode(
                    pose_embedding, output_type='aa').view(1, -1)
                if model_type == 'smpl':
                    wrist_pose = torch.zeros([body_pose.shape[0], 6],
                                             dtype=body_pose.dtype,
                                             device=body_pose.device)
                    body_pose = torch.cat([body_pose, wrist_pose], dim=1)
                est_params['body_pose'] = body_pose
            else:
                est_params[key] = torch.tensor(val, dtype=dtype, device=device)

        model_output = model(**est_params)
        vertices = model_output.vertices.detach().cpu().numpy().squeeze()

        out_mesh = trimesh.Trimesh(vertices, model.faces, process=False)
        material = pyrender.MetallicRoughnessMaterial(
            metallicFactor=0.0,
            alphaMode='OPAQUE',
            baseColorFactor=(1.0, 1.0, 0.9, 1.0))
        mesh = pyrender.Mesh.from_trimesh(
            out_mesh,
            material=material)

        scene = pyrender.Scene(bg_color=[0.0, 0.0, 0.0, 0.0],
                               ambient_light=(0.3, 0.3, 0.3))
        scene.add(mesh, 'mesh')
        pyrender.Viewer(scene, use_raymond_lighting=True)


D:\Projects\smplify-x\smplifyx\render_results.py
# -*- coding: utf-8 -*-

# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is
# holder of all proprietary rights on this computer program.
# You can only use this computer program if you have closed
# a license agreement with MPG or you get the right to use the computer
# program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and
# liable to prosecution.
#
# Copyright©2019 Max-Planck-Gesellschaft zur Förderung
# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute
# for Intelligent Systems and the Max Planck Institute for Biological
# Cybernetics. All rights reserved.
#
# Contact: ps-license@tuebingen.mpg.de

from __future__ import absolute_import
from __future__ import print_function
from __future__ import division

import sys
import os
import os.path as osp
import argparse

import time
import trimesh
from mesh_viewer import MeshViewer


class KeyHandler(object):
    def __init__(self, mesh_fns, verbose=False):
        self.mesh_fns = mesh_fns
        self.idx = 0
        self.verbose = verbose
        self.close = False

    def next_mesh(self, viewer):
        self.idx += 1
        self.idx = self.idx % len(self.mesh_fns)

        if self.verbose:
            print('Loading {} ...'.format(self.mesh_fns[self.idx]))

    def prev_mesh(self, viewer):
        self.idx -= 1
        self.idx = self.idx % len(self.mesh_fns)

        if self.verbose:
            print('Loading {} ...'.format(self.mesh_fns[self.idx]))

    def get_mesh_fn(self):
        return self.mesh_fns[self.idx]

    def quit_viewer(self, viewer):
        self.close = True


parser = argparse.ArgumentParser()

parser.add_argument('--mesh_fns', required=True,
                    type=str, help='The name of the result file',
                    nargs='*')
parser.add_argument('--verbose', action='store_true',
                    help='Verbosity flag')

args = parser.parse_args()

input_mesh_fns = args.mesh_fns
verbose = args.verbose

mesh_fns = []
for mesh_fn in input_mesh_fns:
    if osp.isdir(mesh_fn):
        mesh_fns += [osp.join(root, fn)
                     for (root, dirs, files) in os.walk(mesh_fn)
                     for fn in files if fn.endswith('.obj')]
    elif osp.isfile(mesh_fn):
        mesh_fns.append(mesh_fn)
mesh_fns.sort()

key_handler = KeyHandler(mesh_fns)
registered_keys = {'q': key_handler.quit_viewer,
                   '+': key_handler.next_mesh, '-': key_handler.prev_mesh}
mv = MeshViewer(registered_keys=registered_keys)

print('Press q to exit')
print('Press + to open next mesh')
print('Press - to open previous mesh')

close = False
while True:
    if not mv.is_active():
        break
    if key_handler.close:
        break

    mesh_fn = key_handler.get_mesh_fn()
    #  if prev_idx == idx:
    #  continue
    out_mesh = trimesh.load(mesh_fn)

    mv.update_mesh(out_mesh.vertices, out_mesh.faces)
    time.sleep(0.1)

mv.close_viewer()


D:\Projects\smplify-x\smplifyx\smplx\body_models.py
#  -*- coding: utf-8 -*-

# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is
# holder of all proprietary rights on this computer program.
# You can only use this computer program if you have closed
# a license agreement with MPG or you get the right to use the computer
# program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and
# liable to prosecution.
#
# Copyright©2019 Max-Planck-Gesellschaft zur Förderung
# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute
# for Intelligent Systems. All rights reserved.
#
# Contact: ps-license@tuebingen.mpg.de

from typing import Optional, Dict, Union
import os
import os.path as osp

import pickle

import numpy as np

import torch
import torch.nn as nn

from .lbs import (
    lbs, vertices2landmarks, find_dynamic_lmk_idx_and_bcoords, blend_shapes)

from .vertex_ids import vertex_ids as VERTEX_IDS
from .utils import (
    Struct, to_np, to_tensor, Tensor, Array,
    SMPLOutput,
    SMPLHOutput,
    SMPLXOutput,
    MANOOutput,
    FLAMEOutput,
    find_joint_kin_chain)
from .vertex_joint_selector import VertexJointSelector
from collections import namedtuple

TensorOutput = namedtuple('TensorOutput',
                          ['vertices', 'joints', 'betas', 'expression', 'global_orient', 'body_pose', 'left_hand_pose',
                           'right_hand_pose', 'jaw_pose', 'transl', 'full_pose'])


class SMPL(nn.Module):

    NUM_JOINTS = 23
    NUM_BODY_JOINTS = 23
    SHAPE_SPACE_DIM = 300

    def __init__(
        self, model_path: str,
        kid_template_path: str = '',
        data_struct: Optional[Struct] = None,
        create_betas: bool = True,
        betas: Optional[Tensor] = None,
        num_betas: int = 10,
        create_global_orient: bool = True,
        global_orient: Optional[Tensor] = None,
        create_body_pose: bool = True,
        body_pose: Optional[Tensor] = None,
        create_transl: bool = True,
        transl: Optional[Tensor] = None,
        dtype=torch.float32,
        batch_size: int = 1,
        joint_mapper=None,
        gender: str = 'neutral',
        age: str = 'adult',
        vertex_ids: Dict[str, int] = None,
        v_template: Optional[Union[Tensor, Array]] = None,
        **kwargs
    ) -> None:
        ''' SMPL model constructor

            Parameters
            ----------
            model_path: str
                The path to the folder or to the file where the model
                parameters are stored
            data_struct: Strct
                A struct object. If given, then the parameters of the model are
                read from the object. Otherwise, the model tries to read the
                parameters from the given `model_path`. (default = None)
            create_global_orient: bool, optional
                Flag for creating a member variable for the global orientation
                of the body. (default = True)
            global_orient: torch.tensor, optional, Bx3
                The default value for the global orientation variable.
                (default = None)
            create_body_pose: bool, optional
                Flag for creating a member variable for the pose of the body.
                (default = True)
            body_pose: torch.tensor, optional, Bx(Body Joints * 3)
                The default value for the body pose variable.
                (default = None)
            num_betas: int, optional
                Number of shape components to use
                (default = 10).
            create_betas: bool, optional
                Flag for creating a member variable for the shape space
                (default = True).
            betas: torch.tensor, optional, Bx10
                The default value for the shape member variable.
                (default = None)
            create_transl: bool, optional
                Flag for creating a member variable for the translation
                of the body. (default = True)
            transl: torch.tensor, optional, Bx3
                The default value for the transl variable.
                (default = None)
            dtype: torch.dtype, optional
                The data type for the created variables
            batch_size: int, optional
                The batch size used for creating the member variables
            joint_mapper: object, optional
                An object that re-maps the joints. Useful if one wants to
                re-order the SMPL joints to some other convention (e.g. MSCOCO)
                (default = None)
            gender: str, optional
                Which gender to load
            vertex_ids: dict, optional
                A dictionary containing the indices of the extra vertices that
                will be selected
        '''

        self.gender = gender
        self.age = age

        if data_struct is None:
            if osp.isdir(model_path):
                model_fn = 'SMPL_{}.{ext}'.format(gender.upper(), ext='pkl')
                smpl_path = os.path.join(model_path, model_fn)
            else:
                smpl_path = model_path
            assert osp.exists(smpl_path), 'Path {} does not exist!'.format(
                smpl_path)

            with open(smpl_path, 'rb') as smpl_file:
                data_struct = Struct(**pickle.load(smpl_file,
                                                   encoding='latin1'))

        super(SMPL, self).__init__()
        self.batch_size = batch_size
        shapedirs = data_struct.shapedirs
        if (shapedirs.shape[-1] < self.SHAPE_SPACE_DIM):
            print(f'WARNING: You are using a {self.name()} model, with only'
                  f' {shapedirs.shape[-1]} shape coefficients.\n'
                  f'num_betas={num_betas}, shapedirs.shape={shapedirs.shape}, '
                  f'self.SHAPE_SPACE_DIM={self.SHAPE_SPACE_DIM}')
            num_betas = min(num_betas, shapedirs.shape[-1])
        else:
            num_betas = min(num_betas, self.SHAPE_SPACE_DIM)

        if self.age == 'kid':
            v_template_smil = np.load(kid_template_path)
            v_template_smil -= np.mean(v_template_smil, axis=0)
            v_template_diff = np.expand_dims(
                v_template_smil - data_struct.v_template, axis=2)
            shapedirs = np.concatenate(
                (shapedirs[:, :, :num_betas], v_template_diff), axis=2)
            num_betas = num_betas + 1

        self._num_betas = num_betas
        shapedirs = shapedirs[:, :, :num_betas]
        # The shape components
        self.register_buffer(
            'shapedirs',
            to_tensor(to_np(shapedirs), dtype=dtype))

        if vertex_ids is None:
            # SMPL and SMPL-H share the same topology, so any extra joints can
            # be drawn from the same place
            vertex_ids = VERTEX_IDS['smplh']

        self.dtype = dtype

        self.joint_mapper = joint_mapper

        self.vertex_joint_selector = VertexJointSelector(
            vertex_ids=vertex_ids, **kwargs)

        self.faces = data_struct.f
        self.register_buffer('faces_tensor',
                             to_tensor(to_np(self.faces, dtype=np.int64),
                                       dtype=torch.long))

        if create_betas:
            if betas is None:
                default_betas = torch.zeros(
                    [batch_size, self.num_betas], dtype=dtype)
            else:
                if torch.is_tensor(betas):
                    default_betas = betas.clone().detach()
                else:
                    default_betas = torch.tensor(betas, dtype=dtype)

            self.register_parameter(
                'betas', nn.Parameter(default_betas, requires_grad=True))

        # The tensor that contains the global rotation of the model
        # It is separated from the pose of the joints in case we wish to
        # optimize only over one of them
        if create_global_orient:
            if global_orient is None:
                default_global_orient = torch.zeros(
                    [batch_size, 3], dtype=dtype)
            else:
                if torch.is_tensor(global_orient):
                    default_global_orient = global_orient.clone().detach()
                else:
                    default_global_orient = torch.tensor(
                        global_orient, dtype=dtype)

            global_orient = nn.Parameter(default_global_orient,
                                         requires_grad=True)
            self.register_parameter('global_orient', global_orient)

        if create_body_pose:
            if body_pose is None:
                default_body_pose = torch.zeros(
                    [batch_size, self.NUM_BODY_JOINTS * 3], dtype=dtype)
            else:
                if torch.is_tensor(body_pose):
                    default_body_pose = body_pose.clone().detach()
                else:
                    default_body_pose = torch.tensor(body_pose,
                                                     dtype=dtype)
            self.register_parameter(
                'body_pose',
                nn.Parameter(default_body_pose, requires_grad=True))

        if create_transl:
            if transl is None:
                default_transl = torch.zeros([batch_size, 3],
                                             dtype=dtype,
                                             requires_grad=True)
            else:
                default_transl = torch.tensor(transl, dtype=dtype)
            self.register_parameter(
                'transl', nn.Parameter(default_transl, requires_grad=True))

        if v_template is None:
            v_template = data_struct.v_template
        if not torch.is_tensor(v_template):
            v_template = to_tensor(to_np(v_template), dtype=dtype)
        # The vertices of the template model
        self.register_buffer('v_template', v_template)

        j_regressor = to_tensor(to_np(
            data_struct.J_regressor), dtype=dtype)
        self.register_buffer('J_regressor', j_regressor)

        # Pose blend shape basis: 6890 x 3 x 207, reshaped to 6890*3 x 207
        num_pose_basis = data_struct.posedirs.shape[-1]
        # 207 x 20670
        posedirs = np.reshape(data_struct.posedirs, [-1, num_pose_basis]).T
        self.register_buffer('posedirs',
                             to_tensor(to_np(posedirs), dtype=dtype))

        # indices of parents for each joints
        parents = to_tensor(to_np(data_struct.kintree_table[0])).long()
        parents[0] = -1
        self.register_buffer('parents', parents)

        lbs_weights = to_tensor(to_np(data_struct.weights), dtype=dtype)
        self.register_buffer('lbs_weights', lbs_weights)

    @property
    def num_betas(self):
        return self._num_betas

    @property
    def num_expression_coeffs(self):
        return 0

    def create_mean_pose(self, data_struct) -> Tensor:
        pass

    def name(self) -> str:
        return 'SMPL'

    @torch.no_grad()
    def reset_params(self, **params_dict) -> None:
        for param_name, param in self.named_parameters():
            if param_name in params_dict:
                param[:] = torch.tensor(params_dict[param_name])
            else:
                param.fill_(0)

    def get_num_verts(self) -> int:
        return self.v_template.shape[0]

    def get_num_faces(self) -> int:
        return self.faces.shape[0]

    def extra_repr(self) -> str:
        msg = [
            f'Gender: {self.gender.upper()}',
            f'Number of joints: {self.J_regressor.shape[0]}',
            f'Betas: {self.num_betas}',
        ]
        return '\n'.join(msg)

    def forward_shape(
        self,
        betas: Optional[Tensor] = None,
    ) -> SMPLOutput:
        betas = betas if betas is not None else self.betas
        v_shaped = self.v_template + blend_shapes(betas, self.shapedirs)
        return SMPLOutput(vertices=v_shaped, betas=betas, v_shaped=v_shaped)

    def forward(
        self,
        betas: Optional[Tensor] = None,
        body_pose: Optional[Tensor] = None,
        global_orient: Optional[Tensor] = None,
        transl: Optional[Tensor] = None,
        return_verts=True,
        return_full_pose: bool = False,
        pose2rot: bool = True,
        **kwargs
    ) -> SMPLOutput:
        ''' Forward pass for the SMPL model

            Parameters
            ----------
            global_orient: torch.tensor, optional, shape Bx3
                If given, ignore the member variable and use it as the global
                rotation of the body. Useful if someone wishes to predicts this
                with an external model. (default=None)
            betas: torch.tensor, optional, shape BxN_b
                If given, ignore the member variable `betas` and use it
                instead. For example, it can used if shape parameters
                `betas` are predicted from some external model.
                (default=None)
            body_pose: torch.tensor, optional, shape Bx(J*3)
                If given, ignore the member variable `body_pose` and use it
                instead. For example, it can used if someone predicts the
                pose of the body joints are predicted from some external model.
                It should be a tensor that contains joint rotations in
                axis-angle format. (default=None)
            transl: torch.tensor, optional, shape Bx3
                If given, ignore the member variable `transl` and use it
                instead. For example, it can used if the translation
                `transl` is predicted from some external model.
                (default=None)
            return_verts: bool, optional
                Return the vertices. (default=True)
            return_full_pose: bool, optional
                Returns the full axis-angle pose vector (default=False)

            Returns
            -------
        '''
        print("SMPL forward")
        # If no shape and pose parameters are passed along, then use the
        # ones from the module
        global_orient = (global_orient if global_orient is not None else
                         self.global_orient)
        body_pose = body_pose if body_pose is not None else self.body_pose
        betas = betas if betas is not None else self.betas

        apply_trans = transl is not None or hasattr(self, 'transl')
        if transl is None and hasattr(self, 'transl'):
            transl = self.transl

        full_pose = torch.cat([global_orient, body_pose], dim=1)

        batch_size = max(betas.shape[0], global_orient.shape[0],
                         body_pose.shape[0])

        if betas.shape[0] != batch_size:
            num_repeats = int(batch_size / betas.shape[0])
            betas = betas.expand(num_repeats, -1)

        vertices, joints = lbs(betas, full_pose, self.v_template,
                               self.shapedirs, self.posedirs,
                               self.J_regressor, self.parents,
                               self.lbs_weights, pose2rot=pose2rot)

        joints = self.vertex_joint_selector(vertices, joints)
        # Map the joints to the current dataset
        if self.joint_mapper is not None:
            joints = self.joint_mapper(joints)

        if apply_trans:
            joints += transl.unsqueeze(dim=1)
            vertices += transl.unsqueeze(dim=1)

        output = SMPLOutput(vertices=vertices if return_verts else None,
                            global_orient=global_orient,
                            body_pose=body_pose,
                            joints=joints,
                            betas=betas,
                            full_pose=full_pose if return_full_pose else None)

        return output


class SMPLLayer(SMPL):
    def __init__(
        self,
        *args,
        **kwargs
    ) -> None:
        # Just create a SMPL module without any member variables
        super(SMPLLayer, self).__init__(
            create_body_pose=False,
            create_betas=False,
            create_global_orient=False,
            create_transl=False,
            *args,
            **kwargs,
        )

    def forward(
        self,
        betas: Optional[Tensor] = None,
        body_pose: Optional[Tensor] = None,
        global_orient: Optional[Tensor] = None,
        transl: Optional[Tensor] = None,
        return_verts=True,
        return_full_pose: bool = False,
        pose2rot: bool = True,
        **kwargs
    ) -> SMPLOutput:
        ''' Forward pass for the SMPL model

            Parameters
            ----------
            global_orient: torch.tensor, optional, shape Bx3x3
                Global rotation of the body.  Useful if someone wishes to
                predicts this with an external model. It is expected to be in
                rotation matrix format.  (default=None)
            betas: torch.tensor, optional, shape BxN_b
                Shape parameters. For example, it can used if shape parameters
                `betas` are predicted from some external model.
                (default=None)
            body_pose: torch.tensor, optional, shape BxJx3x3
                Body pose. For example, it can used if someone predicts the
                pose of the body joints are predicted from some external model.
                It should be a tensor that contains joint rotations in
                rotation matrix format. (default=None)
            transl: torch.tensor, optional, shape Bx3
                Translation vector of the body.
                For example, it can used if the translation
                `transl` is predicted from some external model.
                (default=None)
            return_verts: bool, optional
                Return the vertices. (default=True)
            return_full_pose: bool, optional
                Returns the full axis-angle pose vector (default=False)

            Returns
            -------
        '''
        model_vars = [betas, global_orient, body_pose, transl]
        batch_size = 1
        for var in model_vars:
            if var is None:
                continue
            batch_size = max(batch_size, len(var))
        device, dtype = self.shapedirs.device, self.shapedirs.dtype
        if global_orient is None:
            global_orient = torch.eye(3, device=device, dtype=dtype).view(
                1, 1, 3, 3).expand(batch_size, -1, -1, -1).contiguous()
        if body_pose is None:
            body_pose = torch.eye(3, device=device, dtype=dtype).view(
                1, 1, 3, 3).expand(
                    batch_size, self.NUM_BODY_JOINTS, -1, -1).contiguous()
        if betas is None:
            betas = torch.zeros([batch_size, self.num_betas],
                                dtype=dtype, device=device)
        if transl is None:
            transl = torch.zeros([batch_size, 3], dtype=dtype, device=device)
        full_pose = torch.cat(
            [global_orient.reshape(-1, 1, 3, 3),
             body_pose.reshape(-1, self.NUM_BODY_JOINTS, 3, 3)],
            dim=1)

        vertices, joints = lbs(betas, full_pose, self.v_template,
                               self.shapedirs, self.posedirs,
                               self.J_regressor, self.parents,
                               self.lbs_weights,
                               pose2rot=False)

        joints = self.vertex_joint_selector(vertices, joints)
        # Map the joints to the current dataset
        if self.joint_mapper is not None:
            joints = self.joint_mapper(joints)

        if transl is not None:
            joints += transl.unsqueeze(dim=1)
            vertices += transl.unsqueeze(dim=1)

        output = SMPLOutput(vertices=vertices if return_verts else None,
                            global_orient=global_orient,
                            body_pose=body_pose,
                            joints=joints,
                            betas=betas,
                            full_pose=full_pose if return_full_pose else None)

        return output


class SMPLH(SMPL):

    # The hand joints are replaced by MANO
    NUM_BODY_JOINTS = SMPL.NUM_JOINTS - 2
    NUM_HAND_JOINTS = 15
    NUM_JOINTS = NUM_BODY_JOINTS + 2 * NUM_HAND_JOINTS

    def __init__(
        self, model_path,
        kid_template_path: str = '',
        data_struct: Optional[Struct] = None,
        create_left_hand_pose: bool = True,
        left_hand_pose: Optional[Tensor] = None,
        create_right_hand_pose: bool = True,
        right_hand_pose: Optional[Tensor] = None,
        use_pca: bool = True,
        num_pca_comps: int = 6,
        num_betas=16,
        flat_hand_mean: bool = False,
        batch_size: int = 1,
        gender: str = 'neutral',
        age: str = 'adult',
        dtype=torch.float32,
        vertex_ids=None,
        use_compressed: bool = True,
        ext: str = 'pkl',
        **kwargs
    ) -> None:
        ''' SMPLH model constructor

            Parameters
            ----------
            model_path: str
                The path to the folder or to the file where the model
                parameters are stored
            data_struct: Strct
                A struct object. If given, then the parameters of the model are
                read from the object. Otherwise, the model tries to read the
                parameters from the given `model_path`. (default = None)
            create_left_hand_pose: bool, optional
                Flag for creating a member variable for the pose of the left
                hand. (default = True)
            left_hand_pose: torch.tensor, optional, BxP
                The default value for the left hand pose member variable.
                (default = None)
            create_right_hand_pose: bool, optional
                Flag for creating a member variable for the pose of the right
                hand. (default = True)
            right_hand_pose: torch.tensor, optional, BxP
                The default value for the right hand pose member variable.
                (default = None)
            num_pca_comps: int, optional
                The number of PCA components to use for each hand.
                (default = 6)
            flat_hand_mean: bool, optional
                If False, then the pose of the hand is initialized to False.
            batch_size: int, optional
                The batch size used for creating the member variables
            gender: str, optional
                Which gender to load
            dtype: torch.dtype, optional
                The data type for the created variables
            vertex_ids: dict, optional
                A dictionary containing the indices of the extra vertices that
                will be selected
        '''

        self.num_pca_comps = num_pca_comps
        # If no data structure is passed, then load the data from the given
        # model folder
        if data_struct is None:
            # Load the model
            if osp.isdir(model_path):
                model_fn = 'SMPLH_{}.{ext}'.format(gender.upper(), ext=ext)
                smplh_path = os.path.join(model_path, model_fn)
            else:
                smplh_path = model_path
            assert osp.exists(smplh_path), 'Path {} does not exist!'.format(
                smplh_path)

            if ext == 'pkl':
                with open(smplh_path, 'rb') as smplh_file:
                    model_data = pickle.load(smplh_file, encoding='latin1')
            elif ext == 'npz':
                model_data = np.load(smplh_path, allow_pickle=True)
            else:
                raise ValueError('Unknown extension: {}'.format(ext))
            data_struct = Struct(**model_data)

        if vertex_ids is None:
            vertex_ids = VERTEX_IDS['smplh']

        super(SMPLH, self).__init__(
            model_path=model_path,
            kid_template_path=kid_template_path,
            data_struct=data_struct,
            num_betas=num_betas,
            batch_size=batch_size, vertex_ids=vertex_ids, gender=gender, age=age,
            use_compressed=use_compressed, dtype=dtype, ext=ext, **kwargs)

        self.use_pca = use_pca
        self.num_pca_comps = num_pca_comps
        self.flat_hand_mean = flat_hand_mean

        left_hand_components = data_struct.hands_componentsl[:num_pca_comps]
        right_hand_components = data_struct.hands_componentsr[:num_pca_comps]

        self.np_left_hand_components = left_hand_components
        self.np_right_hand_components = right_hand_components
        if self.use_pca:
            self.register_buffer(
                'left_hand_components',
                torch.tensor(left_hand_components, dtype=dtype))
            self.register_buffer(
                'right_hand_components',
                torch.tensor(right_hand_components, dtype=dtype))

        if self.flat_hand_mean:
            left_hand_mean = np.zeros_like(data_struct.hands_meanl)
        else:
            left_hand_mean = data_struct.hands_meanl

        if self.flat_hand_mean:
            right_hand_mean = np.zeros_like(data_struct.hands_meanr)
        else:
            right_hand_mean = data_struct.hands_meanr

        self.register_buffer('left_hand_mean',
                             to_tensor(left_hand_mean, dtype=self.dtype))
        self.register_buffer('right_hand_mean',
                             to_tensor(right_hand_mean, dtype=self.dtype))

        # Create the buffers for the pose of the left hand
        hand_pose_dim = num_pca_comps if use_pca else 3 * self.NUM_HAND_JOINTS
        if create_left_hand_pose:
            if left_hand_pose is None:
                default_lhand_pose = torch.zeros([batch_size, hand_pose_dim],
                                                 dtype=dtype)
            else:
                default_lhand_pose = torch.tensor(left_hand_pose, dtype=dtype)

            left_hand_pose_param = nn.Parameter(default_lhand_pose,
                                                requires_grad=True)
            self.register_parameter('left_hand_pose',
                                    left_hand_pose_param)

        if create_right_hand_pose:
            if right_hand_pose is None:
                default_rhand_pose = torch.zeros([batch_size, hand_pose_dim],
                                                 dtype=dtype)
            else:
                default_rhand_pose = torch.tensor(right_hand_pose, dtype=dtype)

            right_hand_pose_param = nn.Parameter(default_rhand_pose,
                                                 requires_grad=True)
            self.register_parameter('right_hand_pose',
                                    right_hand_pose_param)

        # Create the buffer for the mean pose.
        pose_mean_tensor = self.create_mean_pose(
            data_struct, flat_hand_mean=flat_hand_mean)
        if not torch.is_tensor(pose_mean_tensor):
            pose_mean_tensor = torch.tensor(pose_mean_tensor, dtype=dtype)
        self.register_buffer('pose_mean', pose_mean_tensor)

    def create_mean_pose(self, data_struct, flat_hand_mean=False):
        # Create the array for the mean pose. If flat_hand is false, then use
        # the mean that is given by the data, rather than the flat open hand
        global_orient_mean = torch.zeros([3], dtype=self.dtype)
        body_pose_mean = torch.zeros([self.NUM_BODY_JOINTS * 3],
                                     dtype=self.dtype)

        pose_mean = torch.cat([global_orient_mean, body_pose_mean,
                               self.left_hand_mean,
                               self.right_hand_mean], dim=0)
        return pose_mean

    def name(self) -> str:
        return 'SMPL+H'

    def extra_repr(self):
        msg = super(SMPLH, self).extra_repr()
        msg = [msg]
        if self.use_pca:
            msg.append(f'Number of PCA components: {self.num_pca_comps}')
        msg.append(f'Flat hand mean: {self.flat_hand_mean}')
        return '\n'.join(msg)

    def forward(
        self,
        betas: Optional[Tensor] = None,
        global_orient: Optional[Tensor] = None,
        body_pose: Optional[Tensor] = None,
        left_hand_pose: Optional[Tensor] = None,
        right_hand_pose: Optional[Tensor] = None,
        transl: Optional[Tensor] = None,
        return_verts: bool = True,
        return_full_pose: bool = False,
        pose2rot: bool = True,
        **kwargs
    ) -> SMPLHOutput:
        '''
        '''
        print('SMPLH forward')
        # If no shape and pose parameters are passed along, then use the
        # ones from the module
        global_orient = (global_orient if global_orient is not None else
                         self.global_orient)
        body_pose = body_pose if body_pose is not None else self.body_pose
        betas = betas if betas is not None else self.betas
        left_hand_pose = (left_hand_pose if left_hand_pose is not None else
                          self.left_hand_pose)
        right_hand_pose = (right_hand_pose if right_hand_pose is not None else
                           self.right_hand_pose)

        apply_trans = transl is not None or hasattr(self, 'transl')
        if transl is None:
            if hasattr(self, 'transl'):
                transl = self.transl

        if self.use_pca:
            left_hand_pose = torch.einsum(
                'bi,ij->bj', [left_hand_pose, self.left_hand_components])
            right_hand_pose = torch.einsum(
                'bi,ij->bj', [right_hand_pose, self.right_hand_components])

        full_pose = torch.cat([global_orient, body_pose,
                               left_hand_pose,
                               right_hand_pose], dim=1)
        full_pose += self.pose_mean

        vertices, joints = lbs(betas, full_pose, self.v_template,
                               self.shapedirs, self.posedirs,
                               self.J_regressor, self.parents,
                               self.lbs_weights, pose2rot=pose2rot)

        # Add any extra joints that might be needed
        joints = self.vertex_joint_selector(vertices, joints)
        if self.joint_mapper is not None:
            joints = self.joint_mapper(joints)

        if apply_trans:
            joints += transl.unsqueeze(dim=1)
            vertices += transl.unsqueeze(dim=1)

        output = SMPLHOutput(vertices=vertices if return_verts else None,
                             joints=joints,
                             betas=betas,
                             global_orient=global_orient,
                             body_pose=body_pose,
                             left_hand_pose=left_hand_pose,
                             right_hand_pose=right_hand_pose,
                             full_pose=full_pose if return_full_pose else None)

        return output


class SMPLHLayer(SMPLH):

    def __init__(
        self, *args, **kwargs
    ) -> None:
        ''' SMPL+H as a layer model constructor
        '''
        super(SMPLHLayer, self).__init__(
            create_global_orient=False,
            create_body_pose=False,
            create_left_hand_pose=False,
            create_right_hand_pose=False,
            create_betas=False,
            create_transl=False,
            *args,
            **kwargs)

    def forward(
        self,
        betas: Optional[Tensor] = None,
        global_orient: Optional[Tensor] = None,
        body_pose: Optional[Tensor] = None,
        left_hand_pose: Optional[Tensor] = None,
        right_hand_pose: Optional[Tensor] = None,
        transl: Optional[Tensor] = None,
        return_verts: bool = True,
        return_full_pose: bool = False,
        pose2rot: bool = True,
        **kwargs
    ) -> SMPLHOutput:
        ''' Forward pass for the SMPL+H model

            Parameters
            ----------
            global_orient: torch.tensor, optional, shape Bx3x3
                Global rotation of the body. Useful if someone wishes to
                predicts this with an external model. It is expected to be in
                rotation matrix format. (default=None)
            betas: torch.tensor, optional, shape BxN_b
                Shape parameters. For example, it can used if shape parameters
                `betas` are predicted from some external model.
                (default=None)
            body_pose: torch.tensor, optional, shape BxJx3x3
                If given, ignore the member variable `body_pose` and use it
                instead. For example, it can used if someone predicts the
                pose of the body joints are predicted from some external model.
                It should be a tensor that contains joint rotations in
                rotation matrix format. (default=None)
            left_hand_pose: torch.tensor, optional, shape Bx15x3x3
                If given, contains the pose of the left hand.
                It should be a tensor that contains joint rotations in
                rotation matrix format. (default=None)
            right_hand_pose: torch.tensor, optional, shape Bx15x3x3
                If given, contains the pose of the right hand.
                It should be a tensor that contains joint rotations in
                rotation matrix format. (default=None)
            transl: torch.tensor, optional, shape Bx3
                Translation vector of the body.
                For example, it can used if the translation
                `transl` is predicted from some external model.
                (default=None)
            return_verts: bool, optional
                Return the vertices. (default=True)
            return_full_pose: bool, optional
                Returns the full axis-angle pose vector (default=False)

            Returns
            -------
        '''
        model_vars = [betas, global_orient, body_pose, transl, left_hand_pose,
                      right_hand_pose]
        batch_size = 1
        for var in model_vars:
            if var is None:
                continue
            batch_size = max(batch_size, len(var))
        device, dtype = self.shapedirs.device, self.shapedirs.dtype
        if global_orient is None:
            global_orient = torch.eye(3, device=device, dtype=dtype).view(
                1, 1, 3, 3).expand(batch_size, -1, -1, -1).contiguous()
        if body_pose is None:
            body_pose = torch.eye(3, device=device, dtype=dtype).view(
                1, 1, 3, 3).expand(batch_size, 21, -1, -1).contiguous()
        if left_hand_pose is None:
            left_hand_pose = torch.eye(3, device=device, dtype=dtype).view(
                1, 1, 3, 3).expand(batch_size, 15, -1, -1).contiguous()
        if right_hand_pose is None:
            right_hand_pose = torch.eye(3, device=device, dtype=dtype).view(
                1, 1, 3, 3).expand(batch_size, 15, -1, -1).contiguous()
        if betas is None:
            betas = torch.zeros([batch_size, self.num_betas],
                                dtype=dtype, device=device)
        if transl is None:
            transl = torch.zeros([batch_size, 3], dtype=dtype, device=device)

        # Concatenate all pose vectors
        full_pose = torch.cat(
            [global_orient.reshape(-1, 1, 3, 3),
             body_pose.reshape(-1, self.NUM_BODY_JOINTS, 3, 3),
             left_hand_pose.reshape(-1, self.NUM_HAND_JOINTS, 3, 3),
             right_hand_pose.reshape(-1, self.NUM_HAND_JOINTS, 3, 3)],
            dim=1)

        vertices, joints = lbs(betas, full_pose, self.v_template,
                               self.shapedirs, self.posedirs,
                               self.J_regressor, self.parents,
                               self.lbs_weights, pose2rot=False)

        # Add any extra joints that might be needed
        joints = self.vertex_joint_selector(vertices, joints)
        if self.joint_mapper is not None:
            joints = self.joint_mapper(joints)

        if transl is not None:
            joints += transl.unsqueeze(dim=1)
            vertices += transl.unsqueeze(dim=1)

        output = SMPLHOutput(vertices=vertices if return_verts else None,
                             joints=joints,
                             betas=betas,
                             global_orient=global_orient,
                             body_pose=body_pose,
                             left_hand_pose=left_hand_pose,
                             right_hand_pose=right_hand_pose,
                             full_pose=full_pose if return_full_pose else None)

        return output


class SMPLX(SMPLH):
    '''
    SMPL-X (SMPL eXpressive) is a unified body model, with shape parameters
    trained jointly for the face, hands and body.
    SMPL-X uses standard vertex based linear blend skinning with learned
    corrective blend shapes, has N=10475 vertices and K=54 joints,
    which includes joints for the neck, jaw, eyeballs and fingers.
    '''

    NUM_BODY_JOINTS = SMPLH.NUM_BODY_JOINTS
    NUM_HAND_JOINTS = 15
    NUM_FACE_JOINTS = 3
    NUM_JOINTS = NUM_BODY_JOINTS + 2 * NUM_HAND_JOINTS + NUM_FACE_JOINTS
    EXPRESSION_SPACE_DIM = 100
    NECK_IDX = 12

    def __init__(
        self, model_path: str,
        kid_template_path: str = '',
        num_expression_coeffs: int = 10,
        create_expression: bool = True,
        expression: Optional[Tensor] = None,
        create_jaw_pose: bool = True,
        jaw_pose: Optional[Tensor] = None,
        create_leye_pose: bool = True,
        leye_pose: Optional[Tensor] = None,
        create_reye_pose=True,
        reye_pose: Optional[Tensor] = None,
        use_face_contour: bool = False,
        batch_size: int = 1,
        gender: str = 'neutral',
        age: str = 'adult',
        dtype=torch.float32,
        ext: str = 'npz',
        **kwargs
    ) -> None:
        ''' SMPLX model constructor

            Parameters
            ----------
            model_path: str
                The path to the folder or to the file where the model
                parameters are stored
            num_expression_coeffs: int, optional
                Number of expression components to use
                (default = 10).
            create_expression: bool, optional
                Flag for creating a member variable for the expression space
                (default = True).
            expression: torch.tensor, optional, Bx10
                The default value for the expression member variable.
                (default = None)
            create_jaw_pose: bool, optional
                Flag for creating a member variable for the jaw pose.
                (default = False)
            jaw_pose: torch.tensor, optional, Bx3
                The default value for the jaw pose variable.
                (default = None)
            create_leye_pose: bool, optional
                Flag for creating a member variable for the left eye pose.
                (default = False)
            leye_pose: torch.tensor, optional, Bx10
                The default value for the left eye pose variable.
                (default = None)
            create_reye_pose: bool, optional
                Flag for creating a member variable for the right eye pose.
                (default = False)
            reye_pose: torch.tensor, optional, Bx10
                The default value for the right eye pose variable.
                (default = None)
            use_face_contour: bool, optional
                Whether to compute the keypoints that form the facial contour
            batch_size: int, optional
                The batch size used for creating the member variables
            gender: str, optional
                Which gender to load
            dtype: torch.dtype
                The data type for the created variables
        '''

        # Load the model
        if osp.isdir(model_path):
            model_fn = 'SMPLX_{}.{ext}'.format(gender.upper(), ext=ext)
            smplx_path = os.path.join(model_path, model_fn)
        else:
            smplx_path = model_path
        assert osp.exists(smplx_path), 'Path {} does not exist!'.format(
            smplx_path)

        if ext == 'pkl':
            with open(smplx_path, 'rb') as smplx_file:
                model_data = pickle.load(smplx_file, encoding='latin1')
        elif ext == 'npz':
            model_data = np.load(smplx_path, allow_pickle=True)
        else:
            raise ValueError('Unknown extension: {}'.format(ext))

        data_struct = Struct(**model_data)

        super(SMPLX, self).__init__(
            model_path=model_path,
            kid_template_path=kid_template_path,
            data_struct=data_struct,
            dtype=dtype,
            batch_size=batch_size,
            vertex_ids=VERTEX_IDS['smplx'],
            gender=gender, age=age, ext=ext,
            **kwargs)

        lmk_faces_idx = data_struct.lmk_faces_idx
        self.register_buffer('lmk_faces_idx',
                             torch.tensor(lmk_faces_idx, dtype=torch.long))
        lmk_bary_coords = data_struct.lmk_bary_coords
        self.register_buffer('lmk_bary_coords',
                             torch.tensor(lmk_bary_coords, dtype=dtype))

        self.use_face_contour = use_face_contour
        if self.use_face_contour:
            dynamic_lmk_faces_idx = data_struct.dynamic_lmk_faces_idx
            dynamic_lmk_faces_idx = torch.tensor(
                dynamic_lmk_faces_idx,
                dtype=torch.long)
            self.register_buffer('dynamic_lmk_faces_idx',
                                 dynamic_lmk_faces_idx)

            dynamic_lmk_bary_coords = data_struct.dynamic_lmk_bary_coords
            dynamic_lmk_bary_coords = torch.tensor(
                dynamic_lmk_bary_coords, dtype=dtype)
            self.register_buffer('dynamic_lmk_bary_coords',
                                 dynamic_lmk_bary_coords)

            neck_kin_chain = find_joint_kin_chain(self.NECK_IDX, self.parents)
            self.register_buffer(
                'neck_kin_chain',
                torch.tensor(neck_kin_chain, dtype=torch.long))

        if create_jaw_pose:
            if jaw_pose is None:
                default_jaw_pose = torch.zeros([batch_size, 3], dtype=dtype)
            else:
                default_jaw_pose = torch.tensor(jaw_pose, dtype=dtype)
            jaw_pose_param = nn.Parameter(default_jaw_pose,
                                          requires_grad=True)
            self.register_parameter('jaw_pose', jaw_pose_param)

        if create_leye_pose:
            if leye_pose is None:
                default_leye_pose = torch.zeros([batch_size, 3], dtype=dtype)
            else:
                default_leye_pose = torch.tensor(leye_pose, dtype=dtype)
            leye_pose_param = nn.Parameter(default_leye_pose,
                                           requires_grad=True)
            self.register_parameter('leye_pose', leye_pose_param)

        if create_reye_pose:
            if reye_pose is None:
                default_reye_pose = torch.zeros([batch_size, 3], dtype=dtype)
            else:
                default_reye_pose = torch.tensor(reye_pose, dtype=dtype)
            reye_pose_param = nn.Parameter(default_reye_pose,
                                           requires_grad=True)
            self.register_parameter('reye_pose', reye_pose_param)

        shapedirs = data_struct.shapedirs
        if len(shapedirs.shape) < 3:
            shapedirs = shapedirs[:, :, None]
        if (shapedirs.shape[-1] < self.SHAPE_SPACE_DIM +
                self.EXPRESSION_SPACE_DIM):
            print(f'WARNING: You are using a {self.name()} model, with only'
                  ' 10 shape and 10 expression coefficients.')
            expr_start_idx = 10
            expr_end_idx = 20
            num_expression_coeffs = min(num_expression_coeffs, 10)
        else:
            expr_start_idx = self.SHAPE_SPACE_DIM
            expr_end_idx = self.SHAPE_SPACE_DIM + num_expression_coeffs
            num_expression_coeffs = min(
                num_expression_coeffs, self.EXPRESSION_SPACE_DIM)

        self._num_expression_coeffs = num_expression_coeffs

        expr_dirs = shapedirs[:, :, expr_start_idx:expr_end_idx]
        self.register_buffer(
            'expr_dirs', to_tensor(to_np(expr_dirs), dtype=dtype))

        if create_expression:
            if expression is None:
                default_expression = torch.zeros(
                    [batch_size, self.num_expression_coeffs], dtype=dtype)
            else:
                default_expression = torch.tensor(expression, dtype=dtype)
            expression_param = nn.Parameter(default_expression,
                                            requires_grad=True)
            self.register_parameter('expression', expression_param)

    def name(self) -> str:
        return 'SMPL-X'

    @property
    def num_expression_coeffs(self):
        return self._num_expression_coeffs

    def create_mean_pose(self, data_struct, flat_hand_mean=False):
        # Create the array for the mean pose. If flat_hand is false, then use
        # the mean that is given by the data, rather than the flat open hand
        global_orient_mean = torch.zeros([3], dtype=self.dtype)
        body_pose_mean = torch.zeros([self.NUM_BODY_JOINTS * 3],
                                     dtype=self.dtype)
        jaw_pose_mean = torch.zeros([3], dtype=self.dtype)
        leye_pose_mean = torch.zeros([3], dtype=self.dtype)
        reye_pose_mean = torch.zeros([3], dtype=self.dtype)

        pose_mean = np.concatenate([global_orient_mean, body_pose_mean,
                                    jaw_pose_mean,
                                    leye_pose_mean, reye_pose_mean,
                                    self.left_hand_mean, self.right_hand_mean],
                                   axis=0)

        return pose_mean

    def extra_repr(self):
        msg = super(SMPLX, self).extra_repr()
        msg = [
            msg,
            f'Number of Expression Coefficients: {self.num_expression_coeffs}'
        ]
        return '\n'.join(msg)

    def forward(
        self,
        betas: Optional[Tensor] = None,
        global_orient: Optional[Tensor] = None,
        body_pose: Optional[Tensor] = None,
        left_hand_pose: Optional[Tensor] = None,
        right_hand_pose: Optional[Tensor] = None,
        transl: Optional[Tensor] = None,
        expression: Optional[Tensor] = None,
        jaw_pose: Optional[Tensor] = None,
        leye_pose: Optional[Tensor] = None,
        reye_pose: Optional[Tensor] = None,
        return_verts: bool = True,
        return_full_pose: bool = False,
        pose2rot: bool = True,
        return_shaped: bool = True,
        **kwargs
    ) -> SMPLXOutput:
        '''
        Forward pass for the SMPLX model

            Parameters
            ----------
            global_orient: torch.tensor, optional, shape Bx3
                If given, ignore the member variable and use it as the global
                rotation of the body. Useful if someone wishes to predicts this
                with an external model. (default=None)
            betas: torch.tensor, optional, shape BxN_b
                If given, ignore the member variable `betas` and use it
                instead. For example, it can used if shape parameters
                `betas` are predicted from some external model.
                (default=None)
            expression: torch.tensor, optional, shape BxN_e
                If given, ignore the member variable `expression` and use it
                instead. For example, it can used if expression parameters
                `expression` are predicted from some external model.
            body_pose: torch.tensor, optional, shape Bx(J*3)
                If given, ignore the member variable `body_pose` and use it
                instead. For example, it can used if someone predicts the
                pose of the body joints are predicted from some external model.
                It should be a tensor that contains joint rotations in
                axis-angle format. (default=None)
            left_hand_pose: torch.tensor, optional, shape BxP
                If given, ignore the member variable `left_hand_pose` and
                use this instead. It should either contain PCA coefficients or
                joint rotations in axis-angle format.
            right_hand_pose: torch.tensor, optional, shape BxP
                If given, ignore the member variable `right_hand_pose` and
                use this instead. It should either contain PCA coefficients or
                joint rotations in axis-angle format.
            jaw_pose: torch.tensor, optional, shape Bx3
                If given, ignore the member variable `jaw_pose` and
                use this instead. It should either joint rotations in
                axis-angle format.
            transl: torch.tensor, optional, shape Bx3
                If given, ignore the member variable `transl` and use it
                instead. For example, it can used if the translation
                `transl` is predicted from some external model.
                (default=None)
            return_verts: bool, optional
                Return the vertices. (default=True)
            return_full_pose: bool, optional
                Returns the full axis-angle pose vector (default=False)

            Returns
            -------
                output: ModelOutput
                A named tuple of type `ModelOutput`
        '''
        print("SMPLX forward")
        # If no shape and pose parameters are passed along, then use the
        # ones from the module
        global_orient = (global_orient if global_orient is not None else
                         self.global_orient)
        body_pose = body_pose if body_pose is not None else self.body_pose
        betas = betas if betas is not None else self.betas

        left_hand_pose = (left_hand_pose if left_hand_pose is not None else
                          self.left_hand_pose)
        right_hand_pose = (right_hand_pose if right_hand_pose is not None else
                           self.right_hand_pose)
        jaw_pose = jaw_pose if jaw_pose is not None else self.jaw_pose
        leye_pose = leye_pose if leye_pose is not None else self.leye_pose
        reye_pose = reye_pose if reye_pose is not None else self.reye_pose
        expression = expression if expression is not None else self.expression

        apply_trans = transl is not None or hasattr(self, 'transl')
        if transl is None:
            if hasattr(self, 'transl'):
                transl = self.transl

        if self.use_pca:
            left_hand_pose = torch.einsum(
                'bi,ij->bj', [left_hand_pose, self.left_hand_components])
            right_hand_pose = torch.einsum(
                'bi,ij->bj', [right_hand_pose, self.right_hand_components])

        full_pose = torch.cat([global_orient.reshape(-1, 1, 3),
                               body_pose.reshape(-1, self.NUM_BODY_JOINTS, 3),
                               jaw_pose.reshape(-1, 1, 3),
                               leye_pose.reshape(-1, 1, 3),
                               reye_pose.reshape(-1, 1, 3),
                               left_hand_pose.reshape(-1, 15, 3),
                               right_hand_pose.reshape(-1, 15, 3)],
                              dim=1).reshape(-1, 165)

        # Add the mean pose of the model. Does not affect the body, only the
        # hands when flat_hand_mean == False
        full_pose += self.pose_mean

        batch_size = max(betas.shape[0], global_orient.shape[0],
                         body_pose.shape[0])
        # Concatenate the shape and expression coefficients
        scale = int(batch_size / betas.shape[0])
        if scale > 1:
            betas = betas.expand(scale, -1)
            expression = expression.expand(scale, -1)
        shape_components = torch.cat([betas, expression], dim=-1)

        shapedirs = torch.cat([self.shapedirs, self.expr_dirs], dim=-1)

        vertices, joints = lbs(shape_components, full_pose, self.v_template,
                               shapedirs, self.posedirs,
                               self.J_regressor, self.parents,
                               self.lbs_weights, pose2rot=pose2rot,
                               )

        lmk_faces_idx = self.lmk_faces_idx.unsqueeze(
            dim=0).expand(batch_size, -1).contiguous()
        lmk_bary_coords = self.lmk_bary_coords.unsqueeze(dim=0).repeat(
            self.batch_size, 1, 1)
        if self.use_face_contour:
            lmk_idx_and_bcoords = find_dynamic_lmk_idx_and_bcoords(
                vertices, full_pose, self.dynamic_lmk_faces_idx,
                self.dynamic_lmk_bary_coords,
                self.neck_kin_chain,
                pose2rot=True,
            )
            dyn_lmk_faces_idx, dyn_lmk_bary_coords = lmk_idx_and_bcoords

            lmk_faces_idx = torch.cat([lmk_faces_idx,
                                       dyn_lmk_faces_idx], 1)
            lmk_bary_coords = torch.cat(
                [lmk_bary_coords.expand(batch_size, -1, -1),
                 dyn_lmk_bary_coords], 1)

        landmarks = vertices2landmarks(vertices, self.faces_tensor,
                                       lmk_faces_idx,
                                       lmk_bary_coords)

        # Add any extra joints that might be needed
        joints = self.vertex_joint_selector(vertices, joints)
        # Add the landmarks to the joints
        joints = torch.cat([joints, landmarks], dim=1)
        # Map the joints to the current dataset

        if self.joint_mapper is not None:
            joints = self.joint_mapper(joints=joints, vertices=vertices)

        if apply_trans:
            joints += transl.unsqueeze(dim=1)
            vertices += transl.unsqueeze(dim=1)

        v_shaped = None
        if return_shaped:
            v_shaped = self.v_template + blend_shapes(betas, self.shapedirs)
        else:
            v_shaped = Tensor(0)
        output = SMPLXOutput(vertices=vertices if return_verts else None,
                              joints=joints,
                              betas=betas,
                              expression=expression,
                              global_orient=global_orient,
                              transl=transl,
                              body_pose=body_pose,
                              left_hand_pose=left_hand_pose,
                              right_hand_pose=right_hand_pose,
                              jaw_pose=jaw_pose,
                              v_shaped=v_shaped,
                              full_pose=full_pose if return_full_pose else None)
        return output


class SMPLXLayer(SMPLX):
    def __init__(
        self,
        *args,
        **kwargs
    ) -> None:
        # Just create a SMPLX module without any member variables
        super(SMPLXLayer, self).__init__(
            create_global_orient=False,
            create_body_pose=False,
            create_left_hand_pose=False,
            create_right_hand_pose=False,
            create_jaw_pose=False,
            create_leye_pose=False,
            create_reye_pose=False,
            create_betas=False,
            create_expression=False,
            create_transl=False,
            *args, **kwargs,
        )

    def forward(
        self,
        betas: Optional[Tensor] = None,
        global_orient: Optional[Tensor] = None,
        body_pose: Optional[Tensor] = None,
        left_hand_pose: Optional[Tensor] = None,
        right_hand_pose: Optional[Tensor] = None,
        transl: Optional[Tensor] = None,
        expression: Optional[Tensor] = None,
        jaw_pose: Optional[Tensor] = None,
        leye_pose: Optional[Tensor] = None,
        reye_pose: Optional[Tensor] = None,
        return_verts: bool = True,
        return_full_pose: bool = True,
        **kwargs
    ) -> TensorOutput:
        '''
        Forward pass for the SMPLX model

            Parameters
            ----------
            global_orient: torch.tensor, optional, shape Bx3x3
                If given, ignore the member variable and use it as the global
                rotation of the body. Useful if someone wishes to predicts this
                with an external model. It is expected to be in rotation matrix
                format. (default=None)
            betas: torch.tensor, optional, shape BxN_b
                If given, ignore the member variable `betas` and use it
                instead. For example, it can used if shape parameters
                `betas` are predicted from some external model.
                (default=None)
            expression: torch.tensor, optional, shape BxN_e
                Expression coefficients.
                For example, it can used if expression parameters
                `expression` are predicted from some external model.
            body_pose: torch.tensor, optional, shape BxJx3x3
                If given, ignore the member variable `body_pose` and use it
                instead. For example, it can used if someone predicts the
                pose of the body joints are predicted from some external model.
                It should be a tensor that contains joint rotations in
                rotation matrix format. (default=None)
            left_hand_pose: torch.tensor, optional, shape Bx15x3x3
                If given, contains the pose of the left hand.
                It should be a tensor that contains joint rotations in
                rotation matrix format. (default=None)
            right_hand_pose: torch.tensor, optional, shape Bx15x3x3
                If given, contains the pose of the right hand.
                It should be a tensor that contains joint rotations in
                rotation matrix format. (default=None)
            jaw_pose: torch.tensor, optional, shape Bx3x3
                Jaw pose. It should either joint rotations in
                rotation matrix format.
            transl: torch.tensor, optional, shape Bx3
                Translation vector of the body.
                For example, it can used if the translation
                `transl` is predicted from some external model.
                (default=None)
            return_verts: bool, optional
                Return the vertices. (default=True)
            return_full_pose: bool, optional
                Returns the full pose vector (default=False)
            Returns
            -------
                output: ModelOutput
                A data class that contains the posed vertices and joints
        '''
        device, dtype = self.shapedirs.device, self.shapedirs.dtype

        model_vars = [betas, global_orient, body_pose, transl,
                      expression, left_hand_pose, right_hand_pose, jaw_pose]
        batch_size = 1
        for var in model_vars:
            if var is None:
                continue
            batch_size = max(batch_size, len(var))

        if global_orient is None:
            global_orient = torch.eye(3, device=device, dtype=dtype).view(
                1, 1, 3, 3).expand(batch_size, -1, -1, -1).contiguous()
        if body_pose is None:
            body_pose = torch.eye(3, device=device, dtype=dtype).view(
                1, 1, 3, 3).expand(
                    batch_size, self.NUM_BODY_JOINTS, -1, -1).contiguous()
        if left_hand_pose is None:
            left_hand_pose = torch.eye(3, device=device, dtype=dtype).view(
                1, 1, 3, 3).expand(batch_size, 15, -1, -1).contiguous()
        if right_hand_pose is None:
            right_hand_pose = torch.eye(3, device=device, dtype=dtype).view(
                1, 1, 3, 3).expand(batch_size, 15, -1, -1).contiguous()
        if jaw_pose is None:
            jaw_pose = torch.eye(3, device=device, dtype=dtype).view(
                1, 1, 3, 3).expand(batch_size, -1, -1, -1).contiguous()
        if leye_pose is None:
            leye_pose = torch.eye(3, device=device, dtype=dtype).view(
                1, 1, 3, 3).expand(batch_size, -1, -1, -1).contiguous()
        if reye_pose is None:
            reye_pose = torch.eye(3, device=device, dtype=dtype).view(
                1, 1, 3, 3).expand(batch_size, -1, -1, -1).contiguous()
        if expression is None:
            expression = torch.zeros([batch_size, self.num_expression_coeffs],
                                     dtype=dtype, device=device)
        if betas is None:
            betas = torch.zeros([batch_size, self.num_betas],
                                dtype=dtype, device=device)
        if transl is None:
            transl = torch.zeros([batch_size, 3], dtype=dtype, device=device)

        # Concatenate all pose vectors
        full_pose = torch.cat(
            [global_orient.reshape(-1, 1, 3, 3),
             body_pose.reshape(-1, self.NUM_BODY_JOINTS, 3, 3),
             jaw_pose.reshape(-1, 1, 3, 3),
             leye_pose.reshape(-1, 1, 3, 3),
             reye_pose.reshape(-1, 1, 3, 3),
             left_hand_pose.reshape(-1, self.NUM_HAND_JOINTS, 3, 3),
             right_hand_pose.reshape(-1, self.NUM_HAND_JOINTS, 3, 3)],
            dim=1)
        shape_components = torch.cat([betas, expression], dim=-1)

        shapedirs = torch.cat([self.shapedirs, self.expr_dirs], dim=-1)

        vertices, joints = lbs(shape_components, full_pose, self.v_template,
                               shapedirs, self.posedirs,
                               self.J_regressor, self.parents,
                               self.lbs_weights,
                               pose2rot=False,
                               )

        lmk_faces_idx = self.lmk_faces_idx.unsqueeze(
            dim=0).expand(batch_size, -1).contiguous()
        lmk_bary_coords = self.lmk_bary_coords.unsqueeze(dim=0).repeat(
            batch_size, 1, 1)
        if self.use_face_contour:
            lmk_idx_and_bcoords = find_dynamic_lmk_idx_and_bcoords(
                vertices, full_pose,
                self.dynamic_lmk_faces_idx,
                self.dynamic_lmk_bary_coords,
                self.neck_kin_chain,
                pose2rot=False,
            )
            dyn_lmk_faces_idx, dyn_lmk_bary_coords = lmk_idx_and_bcoords

            lmk_faces_idx = torch.cat([lmk_faces_idx, dyn_lmk_faces_idx], 1)
            lmk_bary_coords = torch.cat(
                [lmk_bary_coords.expand(batch_size, -1, -1),
                 dyn_lmk_bary_coords], 1)

        landmarks = vertices2landmarks(vertices, self.faces_tensor,
                                       lmk_faces_idx,
                                       lmk_bary_coords)

        # Add any extra joints that might be needed
        joints = self.vertex_joint_selector(vertices, joints)
        # Add the landmarks to the joints
        joints = torch.cat([joints, landmarks], dim=1)
        # Map the joints to the current dataset

        if self.joint_mapper is not None:
            joints = self.joint_mapper(joints=joints, vertices=vertices)

        if transl is not None:
            joints += transl.unsqueeze(dim=1)
            vertices += transl.unsqueeze(dim=1)

        output = TensorOutput(vertices=vertices if return_verts else Tensor(0),
                              joints=joints,
                              betas=betas,
                              expression=expression,
                              global_orient=global_orient,
                              body_pose=body_pose,
                              left_hand_pose=left_hand_pose,
                              right_hand_pose=right_hand_pose,
                              jaw_pose=jaw_pose,
                              transl=transl if transl != None else Tensor(0),
                              full_pose=full_pose if return_full_pose else Tensor(0))

        return output


class MANO(SMPL):
    # The hand joints are replaced by MANO
    NUM_BODY_JOINTS = 1
    NUM_HAND_JOINTS = 15
    NUM_JOINTS = NUM_BODY_JOINTS + NUM_HAND_JOINTS

    def __init__(
        self,
        model_path: str,
        is_rhand: bool = True,
        data_struct: Optional[Struct] = None,
        create_hand_pose: bool = True,
        hand_pose: Optional[Tensor] = None,
        use_pca: bool = True,
        num_pca_comps: int = 6,
        flat_hand_mean: bool = False,
        batch_size: int = 1,
        dtype=torch.float32,
        vertex_ids=None,
        use_compressed: bool = True,
        ext: str = 'pkl',
        **kwargs
    ) -> None:
        ''' MANO model constructor

            Parameters
            ----------
            model_path: str
                The path to the folder or to the file where the model
                parameters are stored
            data_struct: Strct
                A struct object. If given, then the parameters of the model are
                read from the object. Otherwise, the model tries to read the
                parameters from the given `model_path`. (default = None)
            create_hand_pose: bool, optional
                Flag for creating a member variable for the pose of the right
                hand. (default = True)
            hand_pose: torch.tensor, optional, BxP
                The default value for the right hand pose member variable.
                (default = None)
            num_pca_comps: int, optional
                The number of PCA components to use for each hand.
                (default = 6)
            flat_hand_mean: bool, optional
                If False, then the pose of the hand is initialized to False.
            batch_size: int, optional
                The batch size used for creating the member variables
            dtype: torch.dtype, optional
                The data type for the created variables
            vertex_ids: dict, optional
                A dictionary containing the indices of the extra vertices that
                will be selected
        '''

        self.num_pca_comps = num_pca_comps
        self.is_rhand = is_rhand
        # If no data structure is passed, then load the data from the given
        # model folder
        if data_struct is None:
            # Load the model
            if osp.isdir(model_path):
                model_fn = 'MANO_{}.{ext}'.format(
                    'RIGHT' if is_rhand else 'LEFT', ext=ext)
                mano_path = os.path.join(model_path, model_fn)
            else:
                mano_path = model_path
                self.is_rhand = True if 'RIGHT' in os.path.basename(
                    model_path) else False
            assert osp.exists(mano_path), 'Path {} does not exist!'.format(
                mano_path)

            if ext == 'pkl':
                with open(mano_path, 'rb') as mano_file:
                    model_data = pickle.load(mano_file, encoding='latin1')
            elif ext == 'npz':
                model_data = np.load(mano_path, allow_pickle=True)
            else:
                raise ValueError('Unknown extension: {}'.format(ext))
            data_struct = Struct(**model_data)

        if vertex_ids is None:
            vertex_ids = VERTEX_IDS['smplh']

        super(MANO, self).__init__(
            model_path=model_path, data_struct=data_struct,
            batch_size=batch_size, vertex_ids=vertex_ids,
            use_compressed=use_compressed, dtype=dtype, ext=ext, **kwargs)

        # add only MANO tips to the extra joints
        self.vertex_joint_selector.extra_joints_idxs = to_tensor(
            list(VERTEX_IDS['mano'].values()), dtype=torch.long)

        self.use_pca = use_pca
        self.num_pca_comps = num_pca_comps
        if self.num_pca_comps == 45:
            self.use_pca = False
        self.flat_hand_mean = flat_hand_mean

        hand_components = data_struct.hands_components[:num_pca_comps]

        self.np_hand_components = hand_components

        if self.use_pca:
            self.register_buffer(
                'hand_components',
                torch.tensor(hand_components, dtype=dtype))

        if self.flat_hand_mean:
            hand_mean = np.zeros_like(data_struct.hands_mean)
        else:
            hand_mean = data_struct.hands_mean

        self.register_buffer('hand_mean',
                             to_tensor(hand_mean, dtype=self.dtype))

        # Create the buffers for the pose of the left hand
        hand_pose_dim = num_pca_comps if use_pca else 3 * self.NUM_HAND_JOINTS
        if create_hand_pose:
            if hand_pose is None:
                default_hand_pose = torch.zeros([batch_size, hand_pose_dim],
                                                dtype=dtype)
            else:
                default_hand_pose = torch.tensor(hand_pose, dtype=dtype)

            hand_pose_param = nn.Parameter(default_hand_pose,
                                           requires_grad=True)
            self.register_parameter('hand_pose',
                                    hand_pose_param)

        # Create the buffer for the mean pose.
        pose_mean = self.create_mean_pose(
            data_struct, flat_hand_mean=flat_hand_mean)
        pose_mean_tensor = pose_mean.clone().to(dtype)
        # pose_mean_tensor = torch.tensor(pose_mean, dtype=dtype)
        self.register_buffer('pose_mean', pose_mean_tensor)

    def name(self) -> str:
        return 'MANO'

    def create_mean_pose(self, data_struct, flat_hand_mean=False):
        # Create the array for the mean pose. If flat_hand is false, then use
        # the mean that is given by the data, rather than the flat open hand
        global_orient_mean = torch.zeros([3], dtype=self.dtype)
        pose_mean = torch.cat([global_orient_mean, self.hand_mean], dim=0)
        return pose_mean

    def extra_repr(self):
        msg = [super(MANO, self).extra_repr()]
        if self.use_pca:
            msg.append(f'Number of PCA components: {self.num_pca_comps}')
        msg.append(f'Flat hand mean: {self.flat_hand_mean}')
        return '\n'.join(msg)

    def forward(
        self,
        betas: Optional[Tensor] = None,
        global_orient: Optional[Tensor] = None,
        hand_pose: Optional[Tensor] = None,
        transl: Optional[Tensor] = None,
        return_verts: bool = True,
        return_full_pose: bool = False,
        **kwargs
    ) -> MANOOutput:
        ''' Forward pass for the MANO model
        '''
        # If no shape and pose parameters are passed along, then use the
        # ones from the module
        global_orient = (global_orient if global_orient is not None else
                         self.global_orient)
        betas = betas if betas is not None else self.betas
        hand_pose = (hand_pose if hand_pose is not None else
                     self.hand_pose)

        apply_trans = transl is not None or hasattr(self, 'transl')
        if transl is None:
            if hasattr(self, 'transl'):
                transl = self.transl

        if self.use_pca:
            hand_pose = torch.einsum(
                'bi,ij->bj', [hand_pose, self.hand_components])

        full_pose = torch.cat([global_orient, hand_pose], dim=1)
        full_pose += self.pose_mean

        vertices, joints = lbs(betas, full_pose, self.v_template,
                               self.shapedirs, self.posedirs,
                               self.J_regressor, self.parents,
                               self.lbs_weights, pose2rot=True,
                               )

        # # Add pre-selected extra joints that might be needed
        # joints = self.vertex_joint_selector(vertices, joints)

        if self.joint_mapper is not None:
            joints = self.joint_mapper(joints)

        if apply_trans:
            joints = joints + transl.unsqueeze(dim=1)
            vertices = vertices + transl.unsqueeze(dim=1)

        output = MANOOutput(vertices=vertices if return_verts else None,
                            joints=joints if return_verts else None,
                            betas=betas,
                            global_orient=global_orient,
                            hand_pose=hand_pose,
                            full_pose=full_pose if return_full_pose else None)

        return output


class MANOLayer(MANO):
    def __init__(self, *args, **kwargs) -> None:
        ''' MANO as a layer model constructor
        '''
        super(MANOLayer, self).__init__(
            create_global_orient=False,
            create_hand_pose=False,
            create_betas=False,
            create_transl=False,
            *args, **kwargs)

    def name(self) -> str:
        return 'MANO'

    def forward(
        self,
        betas: Optional[Tensor] = None,
        global_orient: Optional[Tensor] = None,
        hand_pose: Optional[Tensor] = None,
        transl: Optional[Tensor] = None,
        return_verts: bool = True,
        return_full_pose: bool = False,
        **kwargs
    ) -> MANOOutput:
        ''' Forward pass for the MANO model
        '''
        device, dtype = self.shapedirs.device, self.shapedirs.dtype
        if global_orient is None:
            batch_size = 1
            global_orient = torch.eye(3, device=device, dtype=dtype).view(
                1, 1, 3, 3).expand(batch_size, -1, -1, -1).contiguous()
        else:
            batch_size = global_orient.shape[0]
        if hand_pose is None:
            hand_pose = torch.eye(3, device=device, dtype=dtype).view(
                1, 1, 3, 3).expand(batch_size, 15, -1, -1).contiguous()
        if betas is None:
            betas = torch.zeros(
                [batch_size, self.num_betas], dtype=dtype, device=device)
        if transl is None:
            transl = torch.zeros([batch_size, 3], dtype=dtype, device=device)

        full_pose = torch.cat([global_orient, hand_pose], dim=1)
        vertices, joints = lbs(betas, full_pose, self.v_template,
                               self.shapedirs, self.posedirs,
                               self.J_regressor, self.parents,
                               self.lbs_weights, pose2rot=False)

        if self.joint_mapper is not None:
            joints = self.joint_mapper(joints)

        if transl is not None:
            joints = joints + transl.unsqueeze(dim=1)
            vertices = vertices + transl.unsqueeze(dim=1)

        output = MANOOutput(
            vertices=vertices if return_verts else None,
            joints=joints if return_verts else None,
            betas=betas,
            global_orient=global_orient,
            hand_pose=hand_pose,
            full_pose=full_pose if return_full_pose else None)

        return output


class FLAME(SMPL):
    NUM_JOINTS = 5
    SHAPE_SPACE_DIM = 300
    EXPRESSION_SPACE_DIM = 100
    NECK_IDX = 0

    def __init__(
        self,
        model_path: str,
        data_struct=None,
        num_expression_coeffs=10,
        create_expression: bool = True,
        expression: Optional[Tensor] = None,
        create_neck_pose: bool = True,
        neck_pose: Optional[Tensor] = None,
        create_jaw_pose: bool = True,
        jaw_pose: Optional[Tensor] = None,
        create_leye_pose: bool = True,
        leye_pose: Optional[Tensor] = None,
        create_reye_pose=True,
        reye_pose: Optional[Tensor] = None,
        use_face_contour=False,
        batch_size: int = 1,
        gender: str = 'neutral',
        dtype: torch.dtype = torch.float32,
        ext='pkl',
        **kwargs
    ) -> None:
        ''' FLAME model constructor

            Parameters
            ----------
            model_path: str
                The path to the folder or to the file where the model
                parameters are stored
            num_expression_coeffs: int, optional
                Number of expression components to use
                (default = 10).
            create_expression: bool, optional
                Flag for creating a member variable for the expression space
                (default = True).
            expression: torch.tensor, optional, Bx10
                The default value for the expression member variable.
                (default = None)
            create_neck_pose: bool, optional
                Flag for creating a member variable for the neck pose.
                (default = False)
            neck_pose: torch.tensor, optional, Bx3
                The default value for the neck pose variable.
                (default = None)
            create_jaw_pose: bool, optional
                Flag for creating a member variable for the jaw pose.
                (default = False)
            jaw_pose: torch.tensor, optional, Bx3
                The default value for the jaw pose variable.
                (default = None)
            create_leye_pose: bool, optional
                Flag for creating a member variable for the left eye pose.
                (default = False)
            leye_pose: torch.tensor, optional, Bx10
                The default value for the left eye pose variable.
                (default = None)
            create_reye_pose: bool, optional
                Flag for creating a member variable for the right eye pose.
                (default = False)
            reye_pose: torch.tensor, optional, Bx10
                The default value for the right eye pose variable.
                (default = None)
            use_face_contour: bool, optional
                Whether to compute the keypoints that form the facial contour
            batch_size: int, optional
                The batch size used for creating the member variables
            gender: str, optional
                Which gender to load
            dtype: torch.dtype
                The data type for the created variables
        '''
        model_fn = f'FLAME_{gender.upper()}.{ext}'
        flame_path = os.path.join(model_path, model_fn)
        assert osp.exists(flame_path), 'Path {} does not exist!'.format(
            flame_path)
        if ext == 'npz':
            file_data = np.load(flame_path, allow_pickle=True)
        elif ext == 'pkl':
            with open(flame_path, 'rb') as smpl_file:
                file_data = pickle.load(smpl_file, encoding='latin1')
        else:
            raise ValueError('Unknown extension: {}'.format(ext))
        data_struct = Struct(**file_data)

        super(FLAME, self).__init__(
            model_path=model_path,
            data_struct=data_struct,
            dtype=dtype,
            batch_size=batch_size,
            gender=gender,
            ext=ext,
            **kwargs)

        self.use_face_contour = use_face_contour

        self.vertex_joint_selector.extra_joints_idxs = to_tensor(
            [], dtype=torch.long)

        if create_neck_pose:
            if neck_pose is None:
                default_neck_pose = torch.zeros([batch_size, 3], dtype=dtype)
            else:
                default_neck_pose = torch.tensor(neck_pose, dtype=dtype)
            neck_pose_param = nn.Parameter(
                default_neck_pose, requires_grad=True)
            self.register_parameter('neck_pose', neck_pose_param)

        if create_jaw_pose:
            if jaw_pose is None:
                default_jaw_pose = torch.zeros([batch_size, 3], dtype=dtype)
            else:
                default_jaw_pose = torch.tensor(jaw_pose, dtype=dtype)
            jaw_pose_param = nn.Parameter(default_jaw_pose,
                                          requires_grad=True)
            self.register_parameter('jaw_pose', jaw_pose_param)

        if create_leye_pose:
            if leye_pose is None:
                default_leye_pose = torch.zeros([batch_size, 3], dtype=dtype)
            else:
                default_leye_pose = torch.tensor(leye_pose, dtype=dtype)
            leye_pose_param = nn.Parameter(default_leye_pose,
                                           requires_grad=True)
            self.register_parameter('leye_pose', leye_pose_param)

        if create_reye_pose:
            if reye_pose is None:
                default_reye_pose = torch.zeros([batch_size, 3], dtype=dtype)
            else:
                default_reye_pose = torch.tensor(reye_pose, dtype=dtype)
            reye_pose_param = nn.Parameter(default_reye_pose,
                                           requires_grad=True)
            self.register_parameter('reye_pose', reye_pose_param)

        shapedirs = data_struct.shapedirs
        if len(shapedirs.shape) < 3:
            shapedirs = shapedirs[:, :, None]
        if (shapedirs.shape[-1] < self.SHAPE_SPACE_DIM +
                self.EXPRESSION_SPACE_DIM):
            print(f'WARNING: You are using a {self.name()} model, with only'
                  ' 10 shape and 10 expression coefficients.')
            expr_start_idx = 10
            expr_end_idx = 20
            num_expression_coeffs = min(num_expression_coeffs, 10)
        else:
            expr_start_idx = self.SHAPE_SPACE_DIM
            expr_end_idx = self.SHAPE_SPACE_DIM + num_expression_coeffs
            num_expression_coeffs = min(
                num_expression_coeffs, self.EXPRESSION_SPACE_DIM)

        self._num_expression_coeffs = num_expression_coeffs

        expr_dirs = shapedirs[:, :, expr_start_idx:expr_end_idx]
        self.register_buffer(
            'expr_dirs', to_tensor(to_np(expr_dirs), dtype=dtype))

        if create_expression:
            if expression is None:
                default_expression = torch.zeros(
                    [batch_size, self.num_expression_coeffs], dtype=dtype)
            else:
                default_expression = torch.tensor(expression, dtype=dtype)
            expression_param = nn.Parameter(default_expression,
                                            requires_grad=True)
            self.register_parameter('expression', expression_param)

        # The pickle file that contains the barycentric coordinates for
        # regressing the landmarks
        landmark_bcoord_filename = osp.join(
            model_path, 'flame_static_embedding.pkl')

        with open(landmark_bcoord_filename, 'rb') as fp:
            landmarks_data = pickle.load(fp, encoding='latin1')

        lmk_faces_idx = landmarks_data['lmk_face_idx'].astype(np.int64)
        self.register_buffer('lmk_faces_idx',
                             torch.tensor(lmk_faces_idx, dtype=torch.long))
        lmk_bary_coords = landmarks_data['lmk_b_coords']
        self.register_buffer('lmk_bary_coords',
                             torch.tensor(lmk_bary_coords, dtype=dtype))
        if self.use_face_contour:
            face_contour_path = os.path.join(
                model_path, 'flame_dynamic_embedding.npy')
            contour_embeddings = np.load(face_contour_path,
                                         allow_pickle=True,
                                         encoding='latin1')[()]

            dynamic_lmk_faces_idx = np.array(
                contour_embeddings['lmk_face_idx'], dtype=np.int64)
            dynamic_lmk_faces_idx = torch.tensor(
                dynamic_lmk_faces_idx,
                dtype=torch.long)
            self.register_buffer('dynamic_lmk_faces_idx',
                                 dynamic_lmk_faces_idx)

            dynamic_lmk_b_coords = torch.tensor(
                contour_embeddings['lmk_b_coords'], dtype=dtype)
            self.register_buffer(
                'dynamic_lmk_bary_coords', dynamic_lmk_b_coords)

            neck_kin_chain = find_joint_kin_chain(self.NECK_IDX, self.parents)
            self.register_buffer(
                'neck_kin_chain',
                torch.tensor(neck_kin_chain, dtype=torch.long))

    @property
    def num_expression_coeffs(self):
        return self._num_expression_coeffs

    def name(self) -> str:
        return 'FLAME'

    def extra_repr(self):
        msg = [
            super(FLAME, self).extra_repr(),
            f'Number of Expression Coefficients: {self.num_expression_coeffs}',
            f'Use face contour: {self.use_face_contour}',
        ]
        return '\n'.join(msg)

    def forward(
        self,
        betas: Optional[Tensor] = None,
        global_orient: Optional[Tensor] = None,
        neck_pose: Optional[Tensor] = None,
        transl: Optional[Tensor] = None,
        expression: Optional[Tensor] = None,
        jaw_pose: Optional[Tensor] = None,
        leye_pose: Optional[Tensor] = None,
        reye_pose: Optional[Tensor] = None,
        return_verts: bool = True,
        return_full_pose: bool = False,
        pose2rot: bool = True,
        **kwargs
    ) -> FLAMEOutput:
        '''
        Forward pass for the SMPLX model

            Parameters
            ----------
            global_orient: torch.tensor, optional, shape Bx3
                If given, ignore the member variable and use it as the global
                rotation of the body. Useful if someone wishes to predicts this
                with an external model. (default=None)
            betas: torch.tensor, optional, shape Bx10
                If given, ignore the member variable `betas` and use it
                instead. For example, it can used if shape parameters
                `betas` are predicted from some external model.
                (default=None)
            expression: torch.tensor, optional, shape Bx10
                If given, ignore the member variable `expression` and use it
                instead. For example, it can used if expression parameters
                `expression` are predicted from some external model.
            jaw_pose: torch.tensor, optional, shape Bx3
                If given, ignore the member variable `jaw_pose` and
                use this instead. It should either joint rotations in
                axis-angle format.
            jaw_pose: torch.tensor, optional, shape Bx3
                If given, ignore the member variable `jaw_pose` and
                use this instead. It should either joint rotations in
                axis-angle format.
            transl: torch.tensor, optional, shape Bx3
                If given, ignore the member variable `transl` and use it
                instead. For example, it can used if the translation
                `transl` is predicted from some external model.
                (default=None)
            return_verts: bool, optional
                Return the vertices. (default=True)
            return_full_pose: bool, optional
                Returns the full axis-angle pose vector (default=False)

            Returns
            -------
                output: ModelOutput
                A named tuple of type `ModelOutput`
        '''

        # If no shape and pose parameters are passed along, then use the
        # ones from the module
        global_orient = (global_orient if global_orient is not None else
                         self.global_orient)
        jaw_pose = jaw_pose if jaw_pose is not None else self.jaw_pose
        neck_pose = neck_pose if neck_pose is not None else self.neck_pose

        leye_pose = leye_pose if leye_pose is not None else self.leye_pose
        reye_pose = reye_pose if reye_pose is not None else self.reye_pose

        betas = betas if betas is not None else self.betas
        expression = expression if expression is not None else self.expression

        apply_trans = transl is not None or hasattr(self, 'transl')
        if transl is None:
            if hasattr(self, 'transl'):
                transl = self.transl

        full_pose = torch.cat(
            [global_orient, neck_pose, jaw_pose, leye_pose, reye_pose], dim=1)

        batch_size = max(betas.shape[0], global_orient.shape[0],
                         jaw_pose.shape[0])
        # Concatenate the shape and expression coefficients
        scale = int(batch_size / betas.shape[0])
        if scale > 1:
            betas = betas.expand(scale, -1)
            expression = expression.expand(scale, -1)
        shape_components = torch.cat([betas, expression], dim=-1)
        shapedirs = torch.cat([self.shapedirs, self.expr_dirs], dim=-1)

        vertices, joints = lbs(shape_components, full_pose, self.v_template,
                               shapedirs, self.posedirs,
                               self.J_regressor, self.parents,
                               self.lbs_weights, pose2rot=pose2rot,
                               )

        lmk_faces_idx = self.lmk_faces_idx.unsqueeze(
            dim=0).expand(batch_size, -1).contiguous()
        lmk_bary_coords = self.lmk_bary_coords.unsqueeze(dim=0).repeat(
            batch_size, 1, 1)
        if self.use_face_contour:
            lmk_idx_and_bcoords = find_dynamic_lmk_idx_and_bcoords(
                vertices, full_pose, self.dynamic_lmk_faces_idx,
                self.dynamic_lmk_bary_coords,
                self.neck_kin_chain,
                pose2rot=True,
            )
            dyn_lmk_faces_idx, dyn_lmk_bary_coords = lmk_idx_and_bcoords
            lmk_faces_idx = torch.cat([lmk_faces_idx,
                                       dyn_lmk_faces_idx], 1)
            lmk_bary_coords = torch.cat(
                [lmk_bary_coords.expand(batch_size, -1, -1),
                 dyn_lmk_bary_coords], 1)

        landmarks = vertices2landmarks(vertices, self.faces_tensor,
                                       lmk_faces_idx,
                                       lmk_bary_coords)

        # Add any extra joints that might be needed
        joints = self.vertex_joint_selector(vertices, joints)
        # Add the landmarks to the joints
        joints = torch.cat([joints, landmarks], dim=1)

        # Map the joints to the current dataset
        if self.joint_mapper is not None:
            joints = self.joint_mapper(joints=joints, vertices=vertices)

        if apply_trans:
            joints += transl.unsqueeze(dim=1)
            vertices += transl.unsqueeze(dim=1)

        output = FLAMEOutput(vertices=vertices if return_verts else None,
                             joints=joints,
                             betas=betas,
                             expression=expression,
                             global_orient=global_orient,
                             neck_pose=neck_pose,
                             jaw_pose=jaw_pose,
                             full_pose=full_pose if return_full_pose else None)
        return output


class FLAMELayer(FLAME):
    def __init__(self, *args, **kwargs) -> None:
        ''' FLAME as a layer model constructor '''
        super(FLAMELayer, self).__init__(
            create_betas=False,
            create_expression=False,
            create_global_orient=False,
            create_neck_pose=False,
            create_jaw_pose=False,
            create_leye_pose=False,
            create_reye_pose=False,
            *args,
            **kwargs)

    def forward(
        self,
        betas: Optional[Tensor] = None,
        global_orient: Optional[Tensor] = None,
        neck_pose: Optional[Tensor] = None,
        transl: Optional[Tensor] = None,
        expression: Optional[Tensor] = None,
        jaw_pose: Optional[Tensor] = None,
        leye_pose: Optional[Tensor] = None,
        reye_pose: Optional[Tensor] = None,
        return_verts: bool = True,
        return_full_pose: bool = False,
        pose2rot: bool = True,
        **kwargs
    ) -> FLAMEOutput:
        '''
        Forward pass for the SMPLX model

            Parameters
            ----------
            global_orient: torch.tensor, optional, shape Bx3x3
                Global rotation of the body. Useful if someone wishes to
                predicts this with an external model. It is expected to be in
                rotation matrix format. (default=None)
            betas: torch.tensor, optional, shape BxN_b
                Shape parameters. For example, it can used if shape parameters
                `betas` are predicted from some external model.
                (default=None)
            expression: torch.tensor, optional, shape BxN_e
                If given, ignore the member variable `expression` and use it
                instead. For example, it can used if expression parameters
                `expression` are predicted from some external model.
            jaw_pose: torch.tensor, optional, shape Bx3x3
                Jaw pose. It should either joint rotations in
                rotation matrix format.
            transl: torch.tensor, optional, shape Bx3
                Translation vector of the body.
                For example, it can used if the translation
                `transl` is predicted from some external model.
                (default=None)
            return_verts: bool, optional
                Return the vertices. (default=True)
            return_full_pose: bool, optional
                Returns the full axis-angle pose vector (default=False)

            Returns
            -------
                output: ModelOutput
                A named tuple of type `ModelOutput`
        '''
        device, dtype = self.shapedirs.device, self.shapedirs.dtype
        if global_orient is None:
            batch_size = 1
            global_orient = torch.eye(3, device=device, dtype=dtype).view(
                1, 1, 3, 3).expand(batch_size, -1, -1, -1).contiguous()
        else:
            batch_size = global_orient.shape[0]
        if neck_pose is None:
            neck_pose = torch.eye(3, device=device, dtype=dtype).view(
                1, 1, 3, 3).expand(batch_size, 1, -1, -1).contiguous()
        if jaw_pose is None:
            jaw_pose = torch.eye(3, device=device, dtype=dtype).view(
                1, 1, 3, 3).expand(batch_size, -1, -1, -1).contiguous()
        if leye_pose is None:
            leye_pose = torch.eye(3, device=device, dtype=dtype).view(
                1, 1, 3, 3).expand(batch_size, -1, -1, -1).contiguous()
        if reye_pose is None:
            reye_pose = torch.eye(3, device=device, dtype=dtype).view(
                1, 1, 3, 3).expand(batch_size, -1, -1, -1).contiguous()
        if betas is None:
            betas = torch.zeros([batch_size, self.num_betas],
                                dtype=dtype, device=device)
        if expression is None:
            expression = torch.zeros([batch_size, self.num_expression_coeffs],
                                     dtype=dtype, device=device)
        if transl is None:
            transl = torch.zeros([batch_size, 3], dtype=dtype, device=device)

        full_pose = torch.cat(
            [global_orient, neck_pose, jaw_pose, leye_pose, reye_pose], dim=1)

        shape_components = torch.cat([betas, expression], dim=-1)
        shapedirs = torch.cat([self.shapedirs, self.expr_dirs], dim=-1)

        vertices, joints = lbs(shape_components, full_pose, self.v_template,
                               shapedirs, self.posedirs,
                               self.J_regressor, self.parents,
                               self.lbs_weights, pose2rot=False,
                               )

        lmk_faces_idx = self.lmk_faces_idx.unsqueeze(
            dim=0).expand(batch_size, -1).contiguous()
        lmk_bary_coords = self.lmk_bary_coords.unsqueeze(dim=0).repeat(
            batch_size, 1, 1)
        if self.use_face_contour:
            lmk_idx_and_bcoords = find_dynamic_lmk_idx_and_bcoords(
                vertices, full_pose, self.dynamic_lmk_faces_idx,
                self.dynamic_lmk_bary_coords,
                self.neck_kin_chain,
                pose2rot=False,
            )
            dyn_lmk_faces_idx, dyn_lmk_bary_coords = lmk_idx_and_bcoords
            lmk_faces_idx = torch.cat([lmk_faces_idx,
                                       dyn_lmk_faces_idx], 1)
            lmk_bary_coords = torch.cat(
                [lmk_bary_coords.expand(batch_size, -1, -1),
                 dyn_lmk_bary_coords], 1)

        landmarks = vertices2landmarks(vertices, self.faces_tensor,
                                       lmk_faces_idx,
                                       lmk_bary_coords)

        # Add any extra joints that might be needed
        joints = self.vertex_joint_selector(vertices, joints)
        # Add the landmarks to the joints
        joints = torch.cat([joints, landmarks], dim=1)

        # Map the joints to the current dataset
        if self.joint_mapper is not None:
            joints = self.joint_mapper(joints=joints, vertices=vertices)

        joints += transl.unsqueeze(dim=1)
        vertices += transl.unsqueeze(dim=1)

        output = FLAMEOutput(vertices=vertices if return_verts else None,
                             joints=joints,
                             betas=betas,
                             expression=expression,
                             global_orient=global_orient,
                             neck_pose=neck_pose,
                             jaw_pose=jaw_pose,
                             full_pose=full_pose if return_full_pose else None)
        return output


def build_layer(
    model_path: str,
    model_type: str = 'smpl',
    **kwargs
) -> Union[SMPLLayer, SMPLHLayer, SMPLXLayer, MANOLayer, FLAMELayer]:
    ''' Method for creating a model from a path and a model type

        Parameters
        ----------
        model_path: str
            Either the path to the model you wish to load or a folder,
            where each subfolder contains the differents types, i.e.:
            model_path:
            |
            |-- smpl
                |-- SMPL_FEMALE
                |-- SMPL_NEUTRAL
                |-- SMPL_MALE
            |-- smplh
                |-- SMPLH_FEMALE
                |-- SMPLH_MALE
            |-- smplx
                |-- SMPLX_FEMALE
                |-- SMPLX_NEUTRAL
                |-- SMPLX_MALE
            |-- mano
                |-- MANO RIGHT
                |-- MANO LEFT
            |-- flame
                |-- FLAME_FEMALE
                |-- FLAME_MALE
                |-- FLAME_NEUTRAL

        model_type: str, optional
            When model_path is a folder, then this parameter specifies  the
            type of model to be loaded
        **kwargs: dict
            Keyword arguments

        Returns
        -------
            body_model: nn.Module
                The PyTorch module that implements the corresponding body model
        Raises
        ------
            ValueError: In case the model type is not one of SMPL, SMPLH,
            SMPLX, MANO or FLAME
    '''

    if osp.isdir(model_path):
        model_path = os.path.join(model_path, model_type)
    else:
        model_type = osp.basename(model_path).split('_')[0].lower()

    if model_type.lower() == 'smpl':
        return SMPLLayer(model_path, **kwargs)
    elif model_type.lower() == 'smplh':
        return SMPLHLayer(model_path, **kwargs)
    elif model_type.lower() == 'smplx':
        return SMPLXLayer(model_path, **kwargs)
    elif 'mano' in model_type.lower():
        return MANOLayer(model_path, **kwargs)
    elif 'flame' in model_type.lower():
        return FLAMELayer(model_path, **kwargs)
    else:
        raise ValueError(f'Unknown model type {model_type}, exiting!')


def create(
    model_path: str,
    model_type: str = 'smpl',
    **kwargs
) -> Union[SMPL, SMPLH, SMPLX, MANO, FLAME]:
    ''' Method for creating a model from a path and a model type

        Parameters
        ----------
        model_path: str
            Either the path to the model you wish to load or a folder,
            where each subfolder contains the differents types, i.e.:
            model_path:
            |
            |-- smpl
                |-- SMPL_FEMALE
                |-- SMPL_NEUTRAL
                |-- SMPL_MALE
            |-- smplh
                |-- SMPLH_FEMALE
                |-- SMPLH_MALE
            |-- smplx
                |-- SMPLX_FEMALE
                |-- SMPLX_NEUTRAL
                |-- SMPLX_MALE
            |-- mano
                |-- MANO RIGHT
                |-- MANO LEFT

        model_type: str, optional
            When model_path is a folder, then this parameter specifies  the
            type of model to be loaded
        **kwargs: dict
            Keyword arguments

        Returns
        -------
            body_model: nn.Module
                The PyTorch module that implements the corresponding body model
        Raises
        ------
            ValueError: In case the model type is not one of SMPL, SMPLH,
            SMPLX, MANO or FLAME
    '''

    # If it's a folder, assume
    if osp.isdir(model_path):
        model_path = os.path.join(model_path, model_type)
    else:
        model_type = osp.basename(model_path).split('_')[0].lower()

    if model_type.lower() == 'smpl':
        return SMPL(model_path, **kwargs)
    elif model_type.lower() == 'smplh':
        return SMPLH(model_path, **kwargs)
    elif model_type.lower() == 'smplx':
        return SMPLX(model_path, **kwargs)
    elif 'mano' in model_type.lower():
        return MANO(model_path, **kwargs)
    elif 'flame' in model_type.lower():
        return FLAME(model_path, **kwargs)
    else:
        raise ValueError(f'Unknown model type {model_type}, exiting!')


D:\Projects\smplify-x\smplifyx\smplx\joint_names.py
# -*- coding: utf-8 -*-

# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is
# holder of all proprietary rights on this computer program.
# You can only use this computer program if you have closed
# a license agreement with MPG or you get the right to use the computer
# program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and
# liable to prosecution.
#
# Copyright©2019 Max-Planck-Gesellschaft zur Förderung
# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute
# for Intelligent Systems. All rights reserved.
#
# Contact: ps-license@tuebingen.mpg.de

import numpy as np

JOINT_NAMES = [
    "pelvis",
    "left_hip",
    "right_hip",
    "spine1",
    "left_knee",
    "right_knee",
    "spine2",
    "left_ankle",
    "right_ankle",
    "spine3",
    "left_foot",
    "right_foot",
    "neck",
    "left_collar",
    "right_collar",
    "head",
    "left_shoulder",
    "right_shoulder",
    "left_elbow",
    "right_elbow",
    "left_wrist",
    "right_wrist",
    "jaw",
    "left_eye_smplhf",
    "right_eye_smplhf",
    "left_index1",
    "left_index2",
    "left_index3",
    "left_middle1",
    "left_middle2",
    "left_middle3",
    "left_pinky1",
    "left_pinky2",
    "left_pinky3",
    "left_ring1",
    "left_ring2",
    "left_ring3",
    "left_thumb1",
    "left_thumb2",
    "left_thumb3",
    "right_index1",
    "right_index2",
    "right_index3",
    "right_middle1",
    "right_middle2",
    "right_middle3",
    "right_pinky1",
    "right_pinky2",
    "right_pinky3",
    "right_ring1",
    "right_ring2",
    "right_ring3",
    "right_thumb1",
    "right_thumb2",
    "right_thumb3",
    "nose",
    "right_eye",
    "left_eye",
    "right_ear",
    "left_ear",
    "left_big_toe",
    "left_small_toe",
    "left_heel",
    "right_big_toe",
    "right_small_toe",
    "right_heel",
    "left_thumb",
    "left_index",
    "left_middle",
    "left_ring",
    "left_pinky",
    "right_thumb",
    "right_index",
    "right_middle",
    "right_ring",
    "right_pinky",
    "right_eye_brow1",
    "right_eye_brow2",
    "right_eye_brow3",
    "right_eye_brow4",
    "right_eye_brow5",
    "left_eye_brow5",
    "left_eye_brow4",
    "left_eye_brow3",
    "left_eye_brow2",
    "left_eye_brow1",
    "nose1",
    "nose2",
    "nose3",
    "nose4",
    "right_nose_2",
    "right_nose_1",
    "nose_middle",
    "left_nose_1",
    "left_nose_2",
    "right_eye1",
    "right_eye2",
    "right_eye3",
    "right_eye4",
    "right_eye5",
    "right_eye6",
    "left_eye4",
    "left_eye3",
    "left_eye2",
    "left_eye1",
    "left_eye6",
    "left_eye5",
    "right_mouth_1",
    "right_mouth_2",
    "right_mouth_3",
    "mouth_top",
    "left_mouth_3",
    "left_mouth_2",
    "left_mouth_1",
    "left_mouth_5",  # 59 in OpenPose output
    "left_mouth_4",  # 58 in OpenPose output
    "mouth_bottom",
    "right_mouth_4",
    "right_mouth_5",
    "right_lip_1",
    "right_lip_2",
    "lip_top",
    "left_lip_2",
    "left_lip_1",
    "left_lip_3",
    "lip_bottom",
    "right_lip_3",
    # Face contour
    "right_contour_1",
    "right_contour_2",
    "right_contour_3",
    "right_contour_4",
    "right_contour_5",
    "right_contour_6",
    "right_contour_7",
    "right_contour_8",
    "contour_middle",
    "left_contour_8",
    "left_contour_7",
    "left_contour_6",
    "left_contour_5",
    "left_contour_4",
    "left_contour_3",
    "left_contour_2",
    "left_contour_1",
]


SMPLH_JOINT_NAMES = [
    "pelvis",
    "left_hip",
    "right_hip",
    "spine1",
    "left_knee",
    "right_knee",
    "spine2",
    "left_ankle",
    "right_ankle",
    "spine3",
    "left_foot",
    "right_foot",
    "neck",
    "left_collar",
    "right_collar",
    "head",
    "left_shoulder",
    "right_shoulder",
    "left_elbow",
    "right_elbow",
    "left_wrist",
    "right_wrist",
    "left_index1",
    "left_index2",
    "left_index3",
    "left_middle1",
    "left_middle2",
    "left_middle3",
    "left_pinky1",
    "left_pinky2",
    "left_pinky3",
    "left_ring1",
    "left_ring2",
    "left_ring3",
    "left_thumb1",
    "left_thumb2",
    "left_thumb3",
    "right_index1",
    "right_index2",
    "right_index3",
    "right_middle1",
    "right_middle2",
    "right_middle3",
    "right_pinky1",
    "right_pinky2",
    "right_pinky3",
    "right_ring1",
    "right_ring2",
    "right_ring3",
    "right_thumb1",
    "right_thumb2",
    "right_thumb3",
    "nose",
    "right_eye",
    "left_eye",
    "right_ear",
    "left_ear",
    "left_big_toe",
    "left_small_toe",
    "left_heel",
    "right_big_toe",
    "right_small_toe",
    "right_heel",
    "left_thumb",
    "left_index",
    "left_middle",
    "left_ring",
    "left_pinky",
    "right_thumb",
    "right_index",
    "right_middle",
    "right_ring",
    "right_pinky",
]

SMPL_JOINT_NAMES = [
    "pelvis",
    "left_hip",
    "right_hip",
    "spine1",
    "left_knee",
    "right_knee",
    "spine2",
    "left_ankle",
    "right_ankle",
    "spine3",
    "left_foot",
    "right_foot",
    "neck",
    "left_collar",
    "right_collar",
    "head",
    "left_shoulder",
    "right_shoulder",
    "left_elbow",
    "right_elbow",
    "left_wrist",
    "right_wrist",
    "left_hand",
    "right_hand",
]


class Body:
    """
    Class for storing a single body pose.
    """

    def __init__(self, joints, joint_names):
        assert joints.ndim > 1
        assert joints.shape[0] == len(joint_names)
        self.joints = {}
        for i, j in enumerate(joint_names):
            self.joints[j] = joints[i]

    @staticmethod
    def from_smpl(joints):
        """
        Create a Body object from SMPL joints.
        """
        return Body(joints, SMPL_JOINT_NAMES)

    @staticmethod
    def from_smplh(joints):
        """
        Create a Body object from SMPLH joints.
        """
        return Body(joints, SMPLH_JOINT_NAMES)

    def _as(self, joint_names):
        """
        Return a Body object with the specified joint names.
        """
        joint_list = []
        for j in joint_names:
            if j not in self.joints:
                joint_list.append(np.zeros_like(self.joints["spine1"]))
            else:
                joint_list.append(self.joints[j])
        return np.stack(joint_list, axis=0)

    def as_smpl(self):
        """
        Convert the body to SMPL joints.
        """
        return self._as(SMPL_JOINT_NAMES)

    def as_smplh(self):
        """
        Convert the body to SMPLH joints.
        """
        return self._as(SMPLH_JOINT_NAMES)


D:\Projects\smplify-x\smplifyx\smplx\lbs.py
# -*- coding: utf-8 -*-

# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is
# holder of all proprietary rights on this computer program.
# You can only use this computer program if you have closed
# a license agreement with MPG or you get the right to use the computer
# program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and
# liable to prosecution.
#
# Copyright©2019 Max-Planck-Gesellschaft zur Förderung
# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute
# for Intelligent Systems. All rights reserved.
#
# Contact: ps-license@tuebingen.mpg.de

from __future__ import absolute_import
from __future__ import print_function
from __future__ import division

from typing import Tuple, List
import numpy as np

import torch
import torch.nn.functional as F

from .utils import rot_mat_to_euler, Tensor


def find_dynamic_lmk_idx_and_bcoords(
    vertices: Tensor,
    pose: Tensor,
    dynamic_lmk_faces_idx: Tensor,
    dynamic_lmk_b_coords: Tensor,
    neck_kin_chain: List[int],
    pose2rot: bool = True,
) -> Tuple[Tensor, Tensor]:
    ''' Compute the faces, barycentric coordinates for the dynamic landmarks


        To do so, we first compute the rotation of the neck around the y-axis
        and then use a pre-computed look-up table to find the faces and the
        barycentric coordinates that will be used.

        Special thanks to Soubhik Sanyal (soubhik.sanyal@tuebingen.mpg.de)
        for providing the original TensorFlow implementation and for the LUT.

        Parameters
        ----------
        vertices: torch.tensor BxVx3, dtype = torch.float32
            The tensor of input vertices
        pose: torch.tensor Bx(Jx3), dtype = torch.float32
            The current pose of the body model
        dynamic_lmk_faces_idx: torch.tensor L, dtype = torch.long
            The look-up table from neck rotation to faces
        dynamic_lmk_b_coords: torch.tensor Lx3, dtype = torch.float32
            The look-up table from neck rotation to barycentric coordinates
        neck_kin_chain: list
            A python list that contains the indices of the joints that form the
            kinematic chain of the neck.
        dtype: torch.dtype, optional

        Returns
        -------
        dyn_lmk_faces_idx: torch.tensor, dtype = torch.long
            A tensor of size BxL that contains the indices of the faces that
            will be used to compute the current dynamic landmarks.
        dyn_lmk_b_coords: torch.tensor, dtype = torch.float32
            A tensor of size BxL that contains the indices of the faces that
            will be used to compute the current dynamic landmarks.
    '''

    dtype = vertices.dtype
    batch_size = vertices.shape[0]

    if pose2rot:
        aa_pose = torch.index_select(pose.view(batch_size, -1, 3), 1,
                                     neck_kin_chain)
        rot_mats = batch_rodrigues(
            aa_pose.view(-1, 3)).view(batch_size, -1, 3, 3)
    else:
        rot_mats = torch.index_select(
            pose.view(batch_size, -1, 3, 3), 1, neck_kin_chain)

    rel_rot_mat = torch.eye(
        3, device=vertices.device, dtype=dtype).unsqueeze_(dim=0).repeat(
            batch_size, 1, 1)
    for idx in range(len(neck_kin_chain)):
        rel_rot_mat = torch.bmm(rot_mats[:, idx], rel_rot_mat)

    y_rot_angle = torch.round(
        torch.clamp(-rot_mat_to_euler(rel_rot_mat) * 180.0 / np.pi,
                    max=39)).to(dtype=torch.long)
    neg_mask = y_rot_angle.lt(0).to(dtype=torch.long)
    mask = y_rot_angle.lt(-39).to(dtype=torch.long)
    neg_vals = mask * 78 + (1 - mask) * (39 - y_rot_angle)
    y_rot_angle = (neg_mask * neg_vals +
                   (1 - neg_mask) * y_rot_angle)

    dyn_lmk_faces_idx = torch.index_select(dynamic_lmk_faces_idx,
                                           0, y_rot_angle)
    dyn_lmk_b_coords = torch.index_select(dynamic_lmk_b_coords,
                                          0, y_rot_angle)

    return dyn_lmk_faces_idx, dyn_lmk_b_coords


def vertices2landmarks(
    vertices: Tensor,
    faces: Tensor,
    lmk_faces_idx: Tensor,
    lmk_bary_coords: Tensor
) -> Tensor:
    ''' Calculates landmarks by barycentric interpolation

        Parameters
        ----------
        vertices: torch.tensor BxVx3, dtype = torch.float32
            The tensor of input vertices
        faces: torch.tensor Fx3, dtype = torch.long
            The faces of the mesh
        lmk_faces_idx: torch.tensor L, dtype = torch.long
            The tensor with the indices of the faces used to calculate the
            landmarks.
        lmk_bary_coords: torch.tensor Lx3, dtype = torch.float32
            The tensor of barycentric coordinates that are used to interpolate
            the landmarks

        Returns
        -------
        landmarks: torch.tensor BxLx3, dtype = torch.float32
            The coordinates of the landmarks for each mesh in the batch
    '''
    # Extract the indices of the vertices for each face
    # BxLx3
    batch_size, num_verts = vertices.shape[:2]
    device = vertices.device

    lmk_faces = torch.index_select(faces, 0, lmk_faces_idx.view(-1).to(torch.long)).view(
        batch_size, -1, 3)
                        #The '.to(torch.long)'.
                        # added to make the trace work in c++,
                        # otherwise you get a runtime error in c++:
                        # 'index_select(): Expected dtype int32 or int64 for index'

    lmk_faces += torch.arange(
        batch_size, dtype=torch.long, device=device).view(-1, 1, 1) * num_verts

    lmk_vertices = vertices.view(-1, 3)[lmk_faces].view(
        batch_size, -1, 3, 3)

    landmarks = torch.einsum('blfi,blf->bli', [lmk_vertices, lmk_bary_coords])
    return landmarks


def lbs(
    betas: Tensor,
    pose: Tensor,
    v_template: Tensor,
    shapedirs: Tensor,
    posedirs: Tensor,
    J_regressor: Tensor,
    parents: Tensor,
    lbs_weights: Tensor,
    pose2rot: bool = True,
) -> Tuple[Tensor, Tensor]:
    ''' Performs Linear Blend Skinning with the given shape and pose parameters

        Parameters
        ----------
        betas : torch.tensor BxNB
            The tensor of shape parameters
        pose : torch.tensor Bx(J + 1) * 3
            The pose parameters in axis-angle format
        v_template torch.tensor BxVx3
            The template mesh that will be deformed
        shapedirs : torch.tensor 1xNB
            The tensor of PCA shape displacements
        posedirs : torch.tensor Px(V * 3)
            The pose PCA coefficients
        J_regressor : torch.tensor JxV
            The regressor array that is used to calculate the joints from
            the position of the vertices
        parents: torch.tensor J
            The array that describes the kinematic tree for the model
        lbs_weights: torch.tensor N x V x (J + 1)
            The linear blend skinning weights that represent how much the
            rotation matrix of each part affects each vertex
        pose2rot: bool, optional
            Flag on whether to convert the input pose tensor to rotation
            matrices. The default value is True. If False, then the pose tensor
            should already contain rotation matrices and have a size of
            Bx(J + 1)x9
        dtype: torch.dtype, optional

        Returns
        -------
        verts: torch.tensor BxVx3
            The vertices of the mesh after applying the shape and pose
            displacements.
        joints: torch.tensor BxJx3
            The joints of the model
    '''

    batch_size = max(betas.shape[0], pose.shape[0])
    device, dtype = betas.device, betas.dtype

    # Add shape contribution
    v_shaped = v_template + blend_shapes(betas, shapedirs)

    # Get the joints
    # NxJx3 array
    J = vertices2joints(J_regressor, v_shaped)

    # 3. Add pose blend shapes
    # N x J x 3 x 3
    ident = torch.eye(3, dtype=dtype, device=device)
    if pose2rot:
        rot_mats = batch_rodrigues(pose.view(-1, 3)).view(
            [batch_size, -1, 3, 3])

        pose_feature = (rot_mats[:, 1:, :, :] - ident).view([batch_size, -1])
        # (N x P) x (P, V * 3) -> N x V x 3
        pose_offsets = torch.matmul(
            pose_feature, posedirs).view(batch_size, -1, 3)
    else:
        pose_feature = pose[:, 1:].view(batch_size, -1, 3, 3) - ident
        rot_mats = pose.view(batch_size, -1, 3, 3)

        pose_offsets = torch.matmul(pose_feature.view(batch_size, -1),
                                    posedirs).view(batch_size, -1, 3)

    v_posed = pose_offsets + v_shaped
    # 4. Get the global joint location
    J_transformed, A = batch_rigid_transform(rot_mats, J, parents, dtype=dtype)

    # 5. Do skinning:
    # W is N x V x (J + 1)
    W = lbs_weights.unsqueeze(dim=0).expand([batch_size, -1, -1])
    # (N x V x (J + 1)) x (N x (J + 1) x 16)
    num_joints = J_regressor.shape[0]
    T = torch.matmul(W, A.view(batch_size, num_joints, 16)) \
        .view(batch_size, -1, 4, 4)

    homogen_coord = torch.ones([batch_size, v_posed.shape[1], 1],
                               dtype=dtype, device=device)
    v_posed_homo = torch.cat([v_posed, homogen_coord], dim=2)
    v_homo = torch.matmul(T, torch.unsqueeze(v_posed_homo, dim=-1))

    verts = v_homo[:, :, :3, 0]

    return verts, J_transformed


def vertices2joints(J_regressor: Tensor, vertices: Tensor) -> Tensor:
    ''' Calculates the 3D joint locations from the vertices

    Parameters
    ----------
    J_regressor : torch.tensor JxV
        The regressor array that is used to calculate the joints from the
        position of the vertices
    vertices : torch.tensor BxVx3
        The tensor of mesh vertices

    Returns
    -------
    torch.tensor BxJx3
        The location of the joints
    '''

    return torch.einsum('bik,ji->bjk', [vertices, J_regressor])


def blend_shapes(betas: Tensor, shape_disps: Tensor) -> Tensor:
    ''' Calculates the per vertex displacement due to the blend shapes


    Parameters
    ----------
    betas : torch.tensor Bx(num_betas)
        Blend shape coefficients
    shape_disps: torch.tensor Vx3x(num_betas)
        Blend shapes

    Returns
    -------
    torch.tensor BxVx3
        The per-vertex displacement due to shape deformation
    '''

    # Displacement[b, m, k] = sum_{l} betas[b, l] * shape_disps[m, k, l]
    # i.e. Multiply each shape displacement by its corresponding beta and
    # then sum them.
    blend_shape = torch.einsum('bl,mkl->bmk', [betas, shape_disps])
    return blend_shape


def batch_rodrigues(
    rot_vecs: Tensor,
    epsilon: float = 1e-8,
) -> Tensor:
    ''' Calculates the rotation matrices for a batch of rotation vectors
        Parameters
        ----------
        rot_vecs: torch.tensor Nx3
            array of N axis-angle vectors
        Returns
        -------
        R: torch.tensor Nx3x3
            The rotation matrices for the given axis-angle parameters
    '''

    batch_size = rot_vecs.shape[0]
    device, dtype = rot_vecs.device, rot_vecs.dtype

    angle = torch.norm(rot_vecs + 1e-8, dim=1, keepdim=True)
    rot_dir = rot_vecs / angle

    cos = torch.unsqueeze(torch.cos(angle), dim=1)
    sin = torch.unsqueeze(torch.sin(angle), dim=1)

    # Bx1 arrays
    rx, ry, rz = torch.split(rot_dir, 1, dim=1)
    K = torch.zeros((batch_size, 3, 3), dtype=dtype, device=device)

    zeros = torch.zeros((batch_size, 1), dtype=dtype, device=device)
    K = torch.cat([zeros, -rz, ry, rz, zeros, -rx, -ry, rx, zeros], dim=1) \
        .view((batch_size, 3, 3))

    ident = torch.eye(3, dtype=dtype, device=device).unsqueeze(dim=0)
    rot_mat = ident + sin * K + (1 - cos) * torch.bmm(K, K)
    return rot_mat


def transform_mat(R: Tensor, t: Tensor) -> Tensor:
    ''' Creates a batch of transformation matrices
        Args:
            - R: Bx3x3 array of a batch of rotation matrices
            - t: Bx3x1 array of a batch of translation vectors
        Returns:
            - T: Bx4x4 Transformation matrix
    '''
    # No padding left or right, only add an extra row
    return torch.cat([F.pad(R, [0, 0, 0, 1]),
                      F.pad(t, [0, 0, 0, 1], value=1)], dim=2)


def batch_rigid_transform(
    rot_mats: Tensor,
    joints: Tensor,
    parents: Tensor,
    dtype=torch.float32
) -> Tensor:
    """
    Applies a batch of rigid transformations to the joints

    Parameters
    ----------
    rot_mats : torch.tensor BxNx3x3
        Tensor of rotation matrices
    joints : torch.tensor BxNx3
        Locations of joints
    parents : torch.tensor BxN
        The kinematic tree of each object
    dtype : torch.dtype, optional:
        The data type of the created tensors, the default is torch.float32

    Returns
    -------
    posed_joints : torch.tensor BxNx3
        The locations of the joints after applying the pose rotations
    rel_transforms : torch.tensor BxNx4x4
        The relative (with respect to the root joint) rigid transformations
        for all the joints
    """

    joints = torch.unsqueeze(joints, dim=-1)

    rel_joints = joints.clone()
    rel_joints[:, 1:] -= joints[:, parents[1:]]

    transforms_mat = transform_mat(
        rot_mats.reshape(-1, 3, 3),
        rel_joints.reshape(-1, 3, 1)).reshape(-1, joints.shape[1], 4, 4)

    transform_chain = [transforms_mat[:, 0]]
    for i in range(1, parents.shape[0]):
        # Subtract the joint location at the rest pose
        # No need for rotation, since it's identity when at rest
        curr_res = torch.matmul(transform_chain[parents[i]],
                                transforms_mat[:, i])
        transform_chain.append(curr_res)

    transforms = torch.stack(transform_chain, dim=1)

    # The last column of the transformations contains the posed joints
    posed_joints = transforms[:, :, :3, 3]

    joints_homogen = F.pad(joints, [0, 0, 0, 1])

    rel_transforms = transforms - F.pad(
        torch.matmul(transforms, joints_homogen), [3, 0, 0, 0, 0, 0, 0, 0])

    return posed_joints, rel_transforms


D:\Projects\smplify-x\smplifyx\smplx\utils.py
# -*- coding: utf-8 -*-

# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is
# holder of all proprietary rights on this computer program.
# You can only use this computer program if you have closed
# a license agreement with MPG or you get the right to use the computer
# program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and
# liable to prosecution.
#
# Copyright©2019 Max-Planck-Gesellschaft zur Förderung
# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute
# for Intelligent Systems. All rights reserved.
#
# Contact: ps-license@tuebingen.mpg.de

from typing import NewType, Union, Optional
from dataclasses import dataclass, asdict, fields
import numpy as np
import torch

Tensor = NewType('Tensor', torch.Tensor)
Array = NewType('Array', np.ndarray)


@dataclass
class ModelOutput:
    vertices: Optional[Tensor] = None
    joints: Optional[Tensor] = None
    full_pose: Optional[Tensor] = None
    global_orient: Optional[Tensor] = None
    transl: Optional[Tensor] = None
    v_shaped: Optional[Tensor] = None

    def __getitem__(self, key):
        return getattr(self, key)

    def get(self, key, default=None):
        return getattr(self, key, default)

    def __iter__(self):
        return self.keys()

    def keys(self):
        keys = [t.name for t in fields(self)]
        return iter(keys)

    def values(self):
        values = [getattr(self, t.name) for t in fields(self)]
        return iter(values)

    def items(self):
        data = [(t.name, getattr(self, t.name)) for t in fields(self)]
        return iter(data)


@dataclass
class SMPLOutput(ModelOutput):
    betas: Optional[Tensor] = None
    body_pose: Optional[Tensor] = None


@dataclass
class SMPLHOutput(SMPLOutput):
    left_hand_pose: Optional[Tensor] = None
    right_hand_pose: Optional[Tensor] = None
    transl: Optional[Tensor] = None


@dataclass
class SMPLXOutput(SMPLHOutput):
    expression: Optional[Tensor] = None
    jaw_pose: Optional[Tensor] = None


@dataclass
class MANOOutput(ModelOutput):
    betas: Optional[Tensor] = None
    hand_pose: Optional[Tensor] = None


@dataclass
class FLAMEOutput(ModelOutput):
    betas: Optional[Tensor] = None
    expression: Optional[Tensor] = None
    jaw_pose: Optional[Tensor] = None
    neck_pose: Optional[Tensor] = None


def find_joint_kin_chain(joint_id, kinematic_tree):
    kin_chain = []
    curr_idx = joint_id
    while curr_idx != -1:
        kin_chain.append(curr_idx)
        curr_idx = kinematic_tree[curr_idx]
    return kin_chain


def to_tensor(
        array: Union[Array, Tensor], dtype=torch.float32
) -> Tensor:
    if torch.is_tensor(array):
        return array
    else:
        return torch.tensor(array, dtype=dtype)


class Struct(object):
    def __init__(self, **kwargs):
        for key, val in kwargs.items():
            setattr(self, key, val)


def to_np(array, dtype=np.float32):
    if 'scipy.sparse' in str(type(array)):
        array = array.todense()
    return np.array(array, dtype=dtype)


def rot_mat_to_euler(rot_mats):
    # Calculates rotation matrix to euler angles
    # Careful for extreme cases of eular angles like [0.0, pi, 0.0]

    sy = torch.sqrt(rot_mats[:, 0, 0] * rot_mats[:, 0, 0] +
                    rot_mats[:, 1, 0] * rot_mats[:, 1, 0])
    return torch.atan2(-rot_mats[:, 2, 0], sy)


D:\Projects\smplify-x\smplifyx\smplx\vertex_ids.py
# -*- coding: utf-8 -*-

# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is
# holder of all proprietary rights on this computer program.
# You can only use this computer program if you have closed
# a license agreement with MPG or you get the right to use the computer
# program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and
# liable to prosecution.
#
# Copyright©2019 Max-Planck-Gesellschaft zur Förderung
# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute
# for Intelligent Systems. All rights reserved.
#
# Contact: ps-license@tuebingen.mpg.de

from __future__ import print_function
from __future__ import absolute_import
from __future__ import division

# Joint name to vertex mapping. SMPL/SMPL-H/SMPL-X vertices that correspond to
# MSCOCO and OpenPose joints
vertex_ids = {
    'smplh': {
        'nose':		    332,
        'reye':		    6260,
        'leye':		    2800,
        'rear':		    4071,
        'lear':		    583,
        'rthumb':		6191,
        'rindex':		5782,
        'rmiddle':		5905,
        'rring':		6016,
        'rpinky':		6133,
        'lthumb':		2746,
        'lindex':		2319,
        'lmiddle':		2445,
        'lring':		2556,
        'lpinky':		2673,
        'LBigToe':		3216,
        'LSmallToe':	3226,
        'LHeel':		3387,
        'RBigToe':		6617,
        'RSmallToe':    6624,
        'RHeel':		6787
    },
    'smplx': {
        'nose':		    9120,
        'reye':		    9929,
        'leye':		    9448,
        'rear':		    616,
        'lear':		    6,
        'rthumb':		8079,
        'rindex':		7669,
        'rmiddle':		7794,
        'rring':		7905,
        'rpinky':		8022,
        'lthumb':		5361,
        'lindex':		4933,
        'lmiddle':		5058,
        'lring':		5169,
        'lpinky':		5286,
        'LBigToe':		5770,
        'LSmallToe':    5780,
        'LHeel':		8846,
        'RBigToe':		8463,
        'RSmallToe': 	8474,
        'RHeel':  		8635
    },
    'mano': {
            'thumb':		744,
            'index':		320,
            'middle':		443,
            'ring':		    554,
            'pinky':		671,
        }
}


D:\Projects\smplify-x\smplifyx\smplx\vertex_joint_selector.py
# -*- coding: utf-8 -*-

# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is
# holder of all proprietary rights on this computer program.
# You can only use this computer program if you have closed
# a license agreement with MPG or you get the right to use the computer
# program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and
# liable to prosecution.
#
# Copyright©2019 Max-Planck-Gesellschaft zur Förderung
# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute
# for Intelligent Systems. All rights reserved.
#
# Contact: ps-license@tuebingen.mpg.de

from __future__ import absolute_import
from __future__ import print_function
from __future__ import division

import numpy as np

import torch
import torch.nn as nn

from .utils import to_tensor


class VertexJointSelector(nn.Module):

    def __init__(self, vertex_ids=None,
                 use_hands=True,
                 use_feet_keypoints=True, **kwargs):
        super(VertexJointSelector, self).__init__()

        extra_joints_idxs = []

        face_keyp_idxs = np.array([
            vertex_ids['nose'],
            vertex_ids['reye'],
            vertex_ids['leye'],
            vertex_ids['rear'],
            vertex_ids['lear']], dtype=np.int64)

        extra_joints_idxs = np.concatenate([extra_joints_idxs,
                                            face_keyp_idxs])

        if use_feet_keypoints:
            feet_keyp_idxs = np.array([vertex_ids['LBigToe'],
                                       vertex_ids['LSmallToe'],
                                       vertex_ids['LHeel'],
                                       vertex_ids['RBigToe'],
                                       vertex_ids['RSmallToe'],
                                       vertex_ids['RHeel']], dtype=np.int32)

            extra_joints_idxs = np.concatenate(
                [extra_joints_idxs, feet_keyp_idxs])

        if use_hands:
            self.tip_names = ['thumb', 'index', 'middle', 'ring', 'pinky']

            tips_idxs = []
            for hand_id in ['l', 'r']:
                for tip_name in self.tip_names:
                    tips_idxs.append(vertex_ids[hand_id + tip_name])

            extra_joints_idxs = np.concatenate(
                [extra_joints_idxs, tips_idxs])

        self.register_buffer('extra_joints_idxs',
                             to_tensor(extra_joints_idxs, dtype=torch.long))

    def forward(self, vertices, joints):
        extra_joints = torch.index_select(vertices, 1, self.extra_joints_idxs.to(torch.long)) #The '.to(torch.long)'.
                                                                                            # added to make the trace work in c++,
                                                                                            # otherwise you get a runtime error in c++:
                                                                                            # 'index_select(): Expected dtype int32 or int64 for index'
        joints = torch.cat([joints, extra_joints], dim=1)

        return joints


D:\Projects\smplify-x\smplifyx\smplx\__init__.py
# -*- coding: utf-8 -*-

# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is
# holder of all proprietary rights on this computer program.
# You can only use this computer program if you have closed
# a license agreement with MPG or you get the right to use the computer
# program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and
# liable to prosecution.
#
# Copyright©2019 Max-Planck-Gesellschaft zur Förderung
# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute
# for Intelligent Systems. All rights reserved.
#
# Contact: ps-license@tuebingen.mpg.de

from .body_models import (
    create,
    SMPL,
    SMPLH,
    SMPLX,
    MANO,
    FLAME,
    build_layer,
    SMPLLayer,
    SMPLHLayer,
    SMPLXLayer,
    MANOLayer,
    FLAMELayer,
)


D:\Projects\smplify-x\smplifyx\utils.py
# -*- coding: utf-8 -*-

# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is
# holder of all proprietary rights on this computer program.
# You can only use this computer program if you have closed
# a license agreement with MPG or you get the right to use the computer
# program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and
# liable to prosecution.
#
# Copyright©2019 Max-Planck-Gesellschaft zur Förderung
# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute
# for Intelligent Systems and the Max Planck Institute for Biological
# Cybernetics. All rights reserved.
#
# Contact: ps-license@tuebingen.mpg.de

from __future__ import absolute_import
from __future__ import print_function
from __future__ import division


import numpy as np

import torch
import torch.nn as nn


def to_tensor(tensor, dtype=torch.float32):
    if torch.Tensor == type(tensor):
        return tensor.clone().detach()
    else:
        return torch.tensor(tensor, dtype)


def rel_change(prev_val, curr_val):
    return (prev_val - curr_val) / max([np.abs(prev_val), np.abs(curr_val), 1])


def max_grad_change(grad_arr):
    return grad_arr.abs().max()


class JointMapper(nn.Module):
    def __init__(self, joint_maps=None):
        super(JointMapper, self).__init__()
        if joint_maps is None:
            self.joint_maps = joint_maps
        else:
            self.register_buffer('joint_maps',
                                 torch.tensor(joint_maps, dtype=torch.long))

    def forward(self, joints, **kwargs):
        if self.joint_maps is None:
            return joints
        else:
            return torch.index_select(joints, 1, self.joint_maps)


class GMoF(nn.Module):
    def __init__(self, rho=1):
        super(GMoF, self).__init__()
        self.rho = rho

    def extra_repr(self):
        return 'rho = {}'.format(self.rho)

    def forward(self, residual):
        squared_res = residual ** 2
        dist = torch.div(squared_res, squared_res + self.rho ** 2)
        return self.rho ** 2 * dist


def smpl_to_openpose(model_type='smplx', use_hands=True, use_face=True,
                     use_face_contour=False, openpose_format='coco25'):
    ''' Returns the indices of the permutation that maps OpenPose to SMPL

        Parameters
        ----------
        model_type: str, optional
            The type of SMPL-like model that is used. The default mapping
            returned is for the SMPLX model
        use_hands: bool, optional
            Flag for adding to the returned permutation the mapping for the
            hand keypoints. Defaults to True
        use_face: bool, optional
            Flag for adding to the returned permutation the mapping for the
            face keypoints. Defaults to True
        use_face_contour: bool, optional
            Flag for appending the facial contour keypoints. Defaults to False
        openpose_format: bool, optional
            The output format of OpenPose. For now only COCO-25 and COCO-19 is
            supported. Defaults to 'coco25'

    '''
    if openpose_format.lower() == 'coco25':
        if model_type == 'smpl':
            return np.array([24, 12, 17, 19, 21, 16, 18, 20, 0, 2, 5, 8, 1, 4,
                             7, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34],
                            dtype=np.int32)
        elif model_type == 'smplh':
            body_mapping = np.array([52, 12, 17, 19, 21, 16, 18, 20, 0, 2, 5,
                                     8, 1, 4, 7, 53, 54, 55, 56, 57, 58, 59,
                                     60, 61, 62], dtype=np.int32)
            mapping = [body_mapping]
            if use_hands:
                lhand_mapping = np.array([20, 34, 35, 36, 63, 22, 23, 24, 64,
                                          25, 26, 27, 65, 31, 32, 33, 66, 28,
                                          29, 30, 67], dtype=np.int32)
                rhand_mapping = np.array([21, 49, 50, 51, 68, 37, 38, 39, 69,
                                          40, 41, 42, 70, 46, 47, 48, 71, 43,
                                          44, 45, 72], dtype=np.int32)
                mapping += [lhand_mapping, rhand_mapping]
            return np.concatenate(mapping)
        # SMPLX
        elif model_type == 'smplx':
            body_mapping = np.array([55, 12, 17, 19, 21, 16, 18, 20, 0, 2, 5,
                                     8, 1, 4, 7, 56, 57, 58, 59, 60, 61, 62,
                                     63, 64, 65], dtype=np.int32)
            mapping = [body_mapping]
            if use_hands:
                lhand_mapping = np.array([20, 37, 38, 39, 66, 25, 26, 27,
                                          67, 28, 29, 30, 68, 34, 35, 36, 69,
                                          31, 32, 33, 70], dtype=np.int32)
                rhand_mapping = np.array([21, 52, 53, 54, 71, 40, 41, 42, 72,
                                          43, 44, 45, 73, 49, 50, 51, 74, 46,
                                          47, 48, 75], dtype=np.int32)

                mapping += [lhand_mapping, rhand_mapping]
            if use_face:
                #  end_idx = 127 + 17 * use_face_contour
                face_mapping = np.arange(76, 127 + 17 * use_face_contour,
                                         dtype=np.int32)
                mapping += [face_mapping]

            return np.concatenate(mapping)
        else:
            raise ValueError('Unknown model type: {}'.format(model_type))
    elif openpose_format == 'coco19':
        if model_type == 'smpl':
            return np.array([24, 12, 17, 19, 21, 16, 18, 20, 0, 2, 5, 8,
                             1, 4, 7, 25, 26, 27, 28],
                            dtype=np.int32)
        elif model_type == 'smplh':
            body_mapping = np.array([52, 12, 17, 19, 21, 16, 18, 20, 0, 2, 5,
                                     8, 1, 4, 7, 53, 54, 55, 56],
                                    dtype=np.int32)
            mapping = [body_mapping]
            if use_hands:
                lhand_mapping = np.array([20, 34, 35, 36, 57, 22, 23, 24, 58,
                                          25, 26, 27, 59, 31, 32, 33, 60, 28,
                                          29, 30, 61], dtype=np.int32)
                rhand_mapping = np.array([21, 49, 50, 51, 62, 37, 38, 39, 63,
                                          40, 41, 42, 64, 46, 47, 48, 65, 43,
                                          44, 45, 66], dtype=np.int32)
                mapping += [lhand_mapping, rhand_mapping]
            return np.concatenate(mapping)
        # SMPLX
        elif model_type == 'smplx':
            body_mapping = np.array([55, 12, 17, 19, 21, 16, 18, 20, 0, 2, 5,
                                     8, 1, 4, 7, 56, 57, 58, 59],
                                    dtype=np.int32)
            mapping = [body_mapping]
            if use_hands:
                lhand_mapping = np.array([20, 37, 38, 39, 60, 25, 26, 27,
                                          61, 28, 29, 30, 62, 34, 35, 36, 63,
                                          31, 32, 33, 64], dtype=np.int32)
                rhand_mapping = np.array([21, 52, 53, 54, 65, 40, 41, 42, 66,
                                          43, 44, 45, 67, 49, 50, 51, 68, 46,
                                          47, 48, 69], dtype=np.int32)

                mapping += [lhand_mapping, rhand_mapping]
            if use_face:
                face_mapping = np.arange(70, 70 + 51 +
                                         17 * use_face_contour,
                                         dtype=np.int32)
                mapping += [face_mapping]

            return np.concatenate(mapping)
        else:
            raise ValueError('Unknown model type: {}'.format(model_type))
    else:
        raise ValueError('Unknown joint format: {}'.format(openpose_format))


D:\Projects\smplify-x\smplifyx\__init__.py
# -*- coding: utf-8 -*-

# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is
# holder of all proprietary rights on this computer program.
# You can only use this computer program if you have closed
# a license agreement with MPG or you get the right to use the computer
# program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and
# liable to prosecution.
#
# Copyright©2019 Max-Planck-Gesellschaft zur Förderung
# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute
# for Intelligent Systems and the Max Planck Institute for Biological
# Cybernetics. All rights reserved.
#
# Contact: ps-license@tuebingen.mpg.de

import fitting
import data_parser

from fit_single_frame import fit_single_frame


D:\Projects\smplify-x\smplx\body_models.py
#  -*- coding: utf-8 -*-

# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is
# holder of all proprietary rights on this computer program.
# You can only use this computer program if you have closed
# a license agreement with MPG or you get the right to use the computer
# program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and
# liable to prosecution.
#
# Copyright©2019 Max-Planck-Gesellschaft zur Förderung
# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute
# for Intelligent Systems. All rights reserved.
#
# Contact: ps-license@tuebingen.mpg.de

from typing import Optional, Dict, Union
import os
import os.path as osp

import pickle

import numpy as np

import torch
import torch.nn as nn

from .lbs import (
    lbs, vertices2landmarks, find_dynamic_lmk_idx_and_bcoords, blend_shapes)

from .vertex_ids import vertex_ids as VERTEX_IDS
from .utils import (
    Struct, to_np, to_tensor, Tensor, Array,
    SMPLOutput,
    SMPLHOutput,
    SMPLXOutput,
    MANOOutput,
    FLAMEOutput,
    find_joint_kin_chain)
from .vertex_joint_selector import VertexJointSelector
from collections import namedtuple

TensorOutput = namedtuple('TensorOutput',
                          ['vertices', 'joints', 'betas', 'expression', 'global_orient', 'body_pose', 'left_hand_pose',
                           'right_hand_pose', 'jaw_pose', 'transl', 'full_pose'])


class SMPL(nn.Module):

    NUM_JOINTS = 23
    NUM_BODY_JOINTS = 23
    SHAPE_SPACE_DIM = 300

    def __init__(
        self, model_path: str,
        kid_template_path: str = '',
        data_struct: Optional[Struct] = None,
        create_betas: bool = True,
        betas: Optional[Tensor] = None,
        num_betas: int = 10,
        create_global_orient: bool = True,
        global_orient: Optional[Tensor] = None,
        create_body_pose: bool = True,
        body_pose: Optional[Tensor] = None,
        create_transl: bool = True,
        transl: Optional[Tensor] = None,
        dtype=torch.float32,
        batch_size: int = 1,
        joint_mapper=None,
        gender: str = 'neutral',
        age: str = 'adult',
        vertex_ids: Dict[str, int] = None,
        v_template: Optional[Union[Tensor, Array]] = None,
        **kwargs
    ) -> None:
        ''' SMPL model constructor

            Parameters
            ----------
            model_path: str
                The path to the folder or to the file where the model
                parameters are stored
            data_struct: Strct
                A struct object. If given, then the parameters of the model are
                read from the object. Otherwise, the model tries to read the
                parameters from the given `model_path`. (default = None)
            create_global_orient: bool, optional
                Flag for creating a member variable for the global orientation
                of the body. (default = True)
            global_orient: torch.tensor, optional, Bx3
                The default value for the global orientation variable.
                (default = None)
            create_body_pose: bool, optional
                Flag for creating a member variable for the pose of the body.
                (default = True)
            body_pose: torch.tensor, optional, Bx(Body Joints * 3)
                The default value for the body pose variable.
                (default = None)
            num_betas: int, optional
                Number of shape components to use
                (default = 10).
            create_betas: bool, optional
                Flag for creating a member variable for the shape space
                (default = True).
            betas: torch.tensor, optional, Bx10
                The default value for the shape member variable.
                (default = None)
            create_transl: bool, optional
                Flag for creating a member variable for the translation
                of the body. (default = True)
            transl: torch.tensor, optional, Bx3
                The default value for the transl variable.
                (default = None)
            dtype: torch.dtype, optional
                The data type for the created variables
            batch_size: int, optional
                The batch size used for creating the member variables
            joint_mapper: object, optional
                An object that re-maps the joints. Useful if one wants to
                re-order the SMPL joints to some other convention (e.g. MSCOCO)
                (default = None)
            gender: str, optional
                Which gender to load
            vertex_ids: dict, optional
                A dictionary containing the indices of the extra vertices that
                will be selected
        '''

        self.gender = gender
        self.age = age

        if data_struct is None:
            if osp.isdir(model_path):
                model_fn = 'SMPL_{}.{ext}'.format(gender.upper(), ext='pkl')
                smpl_path = os.path.join(model_path, model_fn)
            else:
                smpl_path = model_path
            assert osp.exists(smpl_path), 'Path {} does not exist!'.format(
                smpl_path)

            with open(smpl_path, 'rb') as smpl_file:
                data_struct = Struct(**pickle.load(smpl_file,
                                                   encoding='latin1'))

        super(SMPL, self).__init__()
        self.batch_size = batch_size
        shapedirs = data_struct.shapedirs
        if (shapedirs.shape[-1] < self.SHAPE_SPACE_DIM):
            print(f'WARNING: You are using a {self.name()} model, with only'
                  f' {shapedirs.shape[-1]} shape coefficients.\n'
                  f'num_betas={num_betas}, shapedirs.shape={shapedirs.shape}, '
                  f'self.SHAPE_SPACE_DIM={self.SHAPE_SPACE_DIM}')
            num_betas = min(num_betas, shapedirs.shape[-1])
        else:
            num_betas = min(num_betas, self.SHAPE_SPACE_DIM)

        if self.age == 'kid':
            v_template_smil = np.load(kid_template_path)
            v_template_smil -= np.mean(v_template_smil, axis=0)
            v_template_diff = np.expand_dims(
                v_template_smil - data_struct.v_template, axis=2)
            shapedirs = np.concatenate(
                (shapedirs[:, :, :num_betas], v_template_diff), axis=2)
            num_betas = num_betas + 1

        self._num_betas = num_betas
        shapedirs = shapedirs[:, :, :num_betas]
        # The shape components
        self.register_buffer(
            'shapedirs',
            to_tensor(to_np(shapedirs), dtype=dtype))

        if vertex_ids is None:
            # SMPL and SMPL-H share the same topology, so any extra joints can
            # be drawn from the same place
            vertex_ids = VERTEX_IDS['smplh']

        self.dtype = dtype

        self.joint_mapper = joint_mapper

        self.vertex_joint_selector = VertexJointSelector(
            vertex_ids=vertex_ids, **kwargs)

        self.faces = data_struct.f
        self.register_buffer('faces_tensor',
                             to_tensor(to_np(self.faces, dtype=np.int64),
                                       dtype=torch.long))

        if create_betas:
            if betas is None:
                default_betas = torch.zeros(
                    [batch_size, self.num_betas], dtype=dtype)
            else:
                if torch.is_tensor(betas):
                    default_betas = betas.clone().detach()
                else:
                    default_betas = torch.tensor(betas, dtype=dtype)

            self.register_parameter(
                'betas', nn.Parameter(default_betas, requires_grad=True))

        # The tensor that contains the global rotation of the model
        # It is separated from the pose of the joints in case we wish to
        # optimize only over one of them
        if create_global_orient:
            if global_orient is None:
                default_global_orient = torch.zeros(
                    [batch_size, 3], dtype=dtype)
            else:
                if torch.is_tensor(global_orient):
                    default_global_orient = global_orient.clone().detach()
                else:
                    default_global_orient = torch.tensor(
                        global_orient, dtype=dtype)

            global_orient = nn.Parameter(default_global_orient,
                                         requires_grad=True)
            self.register_parameter('global_orient', global_orient)

        if create_body_pose:
            if body_pose is None:
                default_body_pose = torch.zeros(
                    [batch_size, self.NUM_BODY_JOINTS * 3], dtype=dtype)
            else:
                if torch.is_tensor(body_pose):
                    default_body_pose = body_pose.clone().detach()
                else:
                    default_body_pose = torch.tensor(body_pose,
                                                     dtype=dtype)
            self.register_parameter(
                'body_pose',
                nn.Parameter(default_body_pose, requires_grad=True))

        if create_transl:
            if transl is None:
                default_transl = torch.zeros([batch_size, 3],
                                             dtype=dtype,
                                             requires_grad=True)
            else:
                default_transl = torch.tensor(transl, dtype=dtype)
            self.register_parameter(
                'transl', nn.Parameter(default_transl, requires_grad=True))

        if v_template is None:
            v_template = data_struct.v_template
        if not torch.is_tensor(v_template):
            v_template = to_tensor(to_np(v_template), dtype=dtype)
        # The vertices of the template model
        self.register_buffer('v_template', v_template)

        j_regressor = to_tensor(to_np(
            data_struct.J_regressor), dtype=dtype)
        self.register_buffer('J_regressor', j_regressor)

        # Pose blend shape basis: 6890 x 3 x 207, reshaped to 6890*3 x 207
        num_pose_basis = data_struct.posedirs.shape[-1]
        # 207 x 20670
        posedirs = np.reshape(data_struct.posedirs, [-1, num_pose_basis]).T
        self.register_buffer('posedirs',
                             to_tensor(to_np(posedirs), dtype=dtype))

        # indices of parents for each joints
        parents = to_tensor(to_np(data_struct.kintree_table[0])).long()
        parents[0] = -1
        self.register_buffer('parents', parents)

        lbs_weights = to_tensor(to_np(data_struct.weights), dtype=dtype)
        self.register_buffer('lbs_weights', lbs_weights)

    @property
    def num_betas(self):
        return self._num_betas

    @property
    def num_expression_coeffs(self):
        return 0

    def create_mean_pose(self, data_struct) -> Tensor:
        pass

    def name(self) -> str:
        return 'SMPL'

    @torch.no_grad()
    def reset_params(self, **params_dict) -> None:
        for param_name, param in self.named_parameters():
            if param_name in params_dict:
                param[:] = torch.tensor(params_dict[param_name])
            else:
                param.fill_(0)

    def get_num_verts(self) -> int:
        return self.v_template.shape[0]

    def get_num_faces(self) -> int:
        return self.faces.shape[0]

    def extra_repr(self) -> str:
        msg = [
            f'Gender: {self.gender.upper()}',
            f'Number of joints: {self.J_regressor.shape[0]}',
            f'Betas: {self.num_betas}',
        ]
        return '\n'.join(msg)

    def forward_shape(
        self,
        betas: Optional[Tensor] = None,
    ) -> SMPLOutput:
        betas = betas if betas is not None else self.betas
        v_shaped = self.v_template + blend_shapes(betas, self.shapedirs)
        return SMPLOutput(vertices=v_shaped, betas=betas, v_shaped=v_shaped)

    def forward(
        self,
        betas: Optional[Tensor] = None,
        body_pose: Optional[Tensor] = None,
        global_orient: Optional[Tensor] = None,
        transl: Optional[Tensor] = None,
        return_verts=True,
        return_full_pose: bool = False,
        pose2rot: bool = True,
        **kwargs
    ) -> SMPLOutput:
        ''' Forward pass for the SMPL model

            Parameters
            ----------
            global_orient: torch.tensor, optional, shape Bx3
                If given, ignore the member variable and use it as the global
                rotation of the body. Useful if someone wishes to predicts this
                with an external model. (default=None)
            betas: torch.tensor, optional, shape BxN_b
                If given, ignore the member variable `betas` and use it
                instead. For example, it can used if shape parameters
                `betas` are predicted from some external model.
                (default=None)
            body_pose: torch.tensor, optional, shape Bx(J*3)
                If given, ignore the member variable `body_pose` and use it
                instead. For example, it can used if someone predicts the
                pose of the body joints are predicted from some external model.
                It should be a tensor that contains joint rotations in
                axis-angle format. (default=None)
            transl: torch.tensor, optional, shape Bx3
                If given, ignore the member variable `transl` and use it
                instead. For example, it can used if the translation
                `transl` is predicted from some external model.
                (default=None)
            return_verts: bool, optional
                Return the vertices. (default=True)
            return_full_pose: bool, optional
                Returns the full axis-angle pose vector (default=False)

            Returns
            -------
        '''
        print("SMPL forward")
        # If no shape and pose parameters are passed along, then use the
        # ones from the module
        global_orient = (global_orient if global_orient is not None else
                         self.global_orient)
        body_pose = body_pose if body_pose is not None else self.body_pose
        betas = betas if betas is not None else self.betas

        apply_trans = transl is not None or hasattr(self, 'transl')
        if transl is None and hasattr(self, 'transl'):
            transl = self.transl

        full_pose = torch.cat([global_orient, body_pose], dim=1)

        batch_size = max(betas.shape[0], global_orient.shape[0],
                         body_pose.shape[0])

        if betas.shape[0] != batch_size:
            num_repeats = int(batch_size / betas.shape[0])
            betas = betas.expand(num_repeats, -1)

        vertices, joints = lbs(betas, full_pose, self.v_template,
                               self.shapedirs, self.posedirs,
                               self.J_regressor, self.parents,
                               self.lbs_weights, pose2rot=pose2rot)

        joints = self.vertex_joint_selector(vertices, joints)
        # Map the joints to the current dataset
        if self.joint_mapper is not None:
            joints = self.joint_mapper(joints)

        if apply_trans:
            joints += transl.unsqueeze(dim=1)
            vertices += transl.unsqueeze(dim=1)

        output = SMPLOutput(vertices=vertices if return_verts else None,
                            global_orient=global_orient,
                            body_pose=body_pose,
                            joints=joints,
                            betas=betas,
                            full_pose=full_pose if return_full_pose else None)

        return output


class SMPLLayer(SMPL):
    def __init__(
        self,
        *args,
        **kwargs
    ) -> None:
        # Just create a SMPL module without any member variables
        super(SMPLLayer, self).__init__(
            create_body_pose=False,
            create_betas=False,
            create_global_orient=False,
            create_transl=False,
            *args,
            **kwargs,
        )

    def forward(
        self,
        betas: Optional[Tensor] = None,
        body_pose: Optional[Tensor] = None,
        global_orient: Optional[Tensor] = None,
        transl: Optional[Tensor] = None,
        return_verts=True,
        return_full_pose: bool = False,
        pose2rot: bool = True,
        **kwargs
    ) -> SMPLOutput:
        ''' Forward pass for the SMPL model

            Parameters
            ----------
            global_orient: torch.tensor, optional, shape Bx3x3
                Global rotation of the body.  Useful if someone wishes to
                predicts this with an external model. It is expected to be in
                rotation matrix format.  (default=None)
            betas: torch.tensor, optional, shape BxN_b
                Shape parameters. For example, it can used if shape parameters
                `betas` are predicted from some external model.
                (default=None)
            body_pose: torch.tensor, optional, shape BxJx3x3
                Body pose. For example, it can used if someone predicts the
                pose of the body joints are predicted from some external model.
                It should be a tensor that contains joint rotations in
                rotation matrix format. (default=None)
            transl: torch.tensor, optional, shape Bx3
                Translation vector of the body.
                For example, it can used if the translation
                `transl` is predicted from some external model.
                (default=None)
            return_verts: bool, optional
                Return the vertices. (default=True)
            return_full_pose: bool, optional
                Returns the full axis-angle pose vector (default=False)

            Returns
            -------
        '''
        model_vars = [betas, global_orient, body_pose, transl]
        batch_size = 1
        for var in model_vars:
            if var is None:
                continue
            batch_size = max(batch_size, len(var))
        device, dtype = self.shapedirs.device, self.shapedirs.dtype
        if global_orient is None:
            global_orient = torch.eye(3, device=device, dtype=dtype).view(
                1, 1, 3, 3).expand(batch_size, -1, -1, -1).contiguous()
        if body_pose is None:
            body_pose = torch.eye(3, device=device, dtype=dtype).view(
                1, 1, 3, 3).expand(
                    batch_size, self.NUM_BODY_JOINTS, -1, -1).contiguous()
        if betas is None:
            betas = torch.zeros([batch_size, self.num_betas],
                                dtype=dtype, device=device)
        if transl is None:
            transl = torch.zeros([batch_size, 3], dtype=dtype, device=device)
        full_pose = torch.cat(
            [global_orient.reshape(-1, 1, 3, 3),
             body_pose.reshape(-1, self.NUM_BODY_JOINTS, 3, 3)],
            dim=1)

        vertices, joints = lbs(betas, full_pose, self.v_template,
                               self.shapedirs, self.posedirs,
                               self.J_regressor, self.parents,
                               self.lbs_weights,
                               pose2rot=False)

        joints = self.vertex_joint_selector(vertices, joints)
        # Map the joints to the current dataset
        if self.joint_mapper is not None:
            joints = self.joint_mapper(joints)

        if transl is not None:
            joints += transl.unsqueeze(dim=1)
            vertices += transl.unsqueeze(dim=1)

        output = SMPLOutput(vertices=vertices if return_verts else None,
                            global_orient=global_orient,
                            body_pose=body_pose,
                            joints=joints,
                            betas=betas,
                            full_pose=full_pose if return_full_pose else None)

        return output


class SMPLH(SMPL):

    # The hand joints are replaced by MANO
    NUM_BODY_JOINTS = SMPL.NUM_JOINTS - 2
    NUM_HAND_JOINTS = 15
    NUM_JOINTS = NUM_BODY_JOINTS + 2 * NUM_HAND_JOINTS

    def __init__(
        self, model_path,
        kid_template_path: str = '',
        data_struct: Optional[Struct] = None,
        create_left_hand_pose: bool = True,
        left_hand_pose: Optional[Tensor] = None,
        create_right_hand_pose: bool = True,
        right_hand_pose: Optional[Tensor] = None,
        use_pca: bool = True,
        num_pca_comps: int = 6,
        num_betas=16,
        flat_hand_mean: bool = False,
        batch_size: int = 1,
        gender: str = 'neutral',
        age: str = 'adult',
        dtype=torch.float32,
        vertex_ids=None,
        use_compressed: bool = True,
        ext: str = 'pkl',
        **kwargs
    ) -> None:
        ''' SMPLH model constructor

            Parameters
            ----------
            model_path: str
                The path to the folder or to the file where the model
                parameters are stored
            data_struct: Strct
                A struct object. If given, then the parameters of the model are
                read from the object. Otherwise, the model tries to read the
                parameters from the given `model_path`. (default = None)
            create_left_hand_pose: bool, optional
                Flag for creating a member variable for the pose of the left
                hand. (default = True)
            left_hand_pose: torch.tensor, optional, BxP
                The default value for the left hand pose member variable.
                (default = None)
            create_right_hand_pose: bool, optional
                Flag for creating a member variable for the pose of the right
                hand. (default = True)
            right_hand_pose: torch.tensor, optional, BxP
                The default value for the right hand pose member variable.
                (default = None)
            num_pca_comps: int, optional
                The number of PCA components to use for each hand.
                (default = 6)
            flat_hand_mean: bool, optional
                If False, then the pose of the hand is initialized to False.
            batch_size: int, optional
                The batch size used for creating the member variables
            gender: str, optional
                Which gender to load
            dtype: torch.dtype, optional
                The data type for the created variables
            vertex_ids: dict, optional
                A dictionary containing the indices of the extra vertices that
                will be selected
        '''

        self.num_pca_comps = num_pca_comps
        # If no data structure is passed, then load the data from the given
        # model folder
        if data_struct is None:
            # Load the model
            if osp.isdir(model_path):
                model_fn = 'SMPLH_{}.{ext}'.format(gender.upper(), ext=ext)
                smplh_path = os.path.join(model_path, model_fn)
            else:
                smplh_path = model_path
            assert osp.exists(smplh_path), 'Path {} does not exist!'.format(
                smplh_path)

            if ext == 'pkl':
                with open(smplh_path, 'rb') as smplh_file:
                    model_data = pickle.load(smplh_file, encoding='latin1')
            elif ext == 'npz':
                model_data = np.load(smplh_path, allow_pickle=True)
            else:
                raise ValueError('Unknown extension: {}'.format(ext))
            data_struct = Struct(**model_data)

        if vertex_ids is None:
            vertex_ids = VERTEX_IDS['smplh']

        super(SMPLH, self).__init__(
            model_path=model_path,
            kid_template_path=kid_template_path,
            data_struct=data_struct,
            num_betas=num_betas,
            batch_size=batch_size, vertex_ids=vertex_ids, gender=gender, age=age,
            use_compressed=use_compressed, dtype=dtype, ext=ext, **kwargs)

        self.use_pca = use_pca
        self.num_pca_comps = num_pca_comps
        self.flat_hand_mean = flat_hand_mean

        left_hand_components = data_struct.hands_componentsl[:num_pca_comps]
        right_hand_components = data_struct.hands_componentsr[:num_pca_comps]

        self.np_left_hand_components = left_hand_components
        self.np_right_hand_components = right_hand_components
        if self.use_pca:
            self.register_buffer(
                'left_hand_components',
                torch.tensor(left_hand_components, dtype=dtype))
            self.register_buffer(
                'right_hand_components',
                torch.tensor(right_hand_components, dtype=dtype))

        if self.flat_hand_mean:
            left_hand_mean = np.zeros_like(data_struct.hands_meanl)
        else:
            left_hand_mean = data_struct.hands_meanl

        if self.flat_hand_mean:
            right_hand_mean = np.zeros_like(data_struct.hands_meanr)
        else:
            right_hand_mean = data_struct.hands_meanr

        self.register_buffer('left_hand_mean',
                             to_tensor(left_hand_mean, dtype=self.dtype))
        self.register_buffer('right_hand_mean',
                             to_tensor(right_hand_mean, dtype=self.dtype))

        # Create the buffers for the pose of the left hand
        hand_pose_dim = num_pca_comps if use_pca else 3 * self.NUM_HAND_JOINTS
        if create_left_hand_pose:
            if left_hand_pose is None:
                default_lhand_pose = torch.zeros([batch_size, hand_pose_dim],
                                                 dtype=dtype)
            else:
                default_lhand_pose = torch.tensor(left_hand_pose, dtype=dtype)

            left_hand_pose_param = nn.Parameter(default_lhand_pose,
                                                requires_grad=True)
            self.register_parameter('left_hand_pose',
                                    left_hand_pose_param)

        if create_right_hand_pose:
            if right_hand_pose is None:
                default_rhand_pose = torch.zeros([batch_size, hand_pose_dim],
                                                 dtype=dtype)
            else:
                default_rhand_pose = torch.tensor(right_hand_pose, dtype=dtype)

            right_hand_pose_param = nn.Parameter(default_rhand_pose,
                                                 requires_grad=True)
            self.register_parameter('right_hand_pose',
                                    right_hand_pose_param)

        # Create the buffer for the mean pose.
        pose_mean_tensor = self.create_mean_pose(
            data_struct, flat_hand_mean=flat_hand_mean)
        if not torch.is_tensor(pose_mean_tensor):
            pose_mean_tensor = torch.tensor(pose_mean_tensor, dtype=dtype)
        self.register_buffer('pose_mean', pose_mean_tensor)

    def create_mean_pose(self, data_struct, flat_hand_mean=False):
        # Create the array for the mean pose. If flat_hand is false, then use
        # the mean that is given by the data, rather than the flat open hand
        global_orient_mean = torch.zeros([3], dtype=self.dtype)
        body_pose_mean = torch.zeros([self.NUM_BODY_JOINTS * 3],
                                     dtype=self.dtype)

        pose_mean = torch.cat([global_orient_mean, body_pose_mean,
                               self.left_hand_mean,
                               self.right_hand_mean], dim=0)
        return pose_mean

    def name(self) -> str:
        return 'SMPL+H'

    def extra_repr(self):
        msg = super(SMPLH, self).extra_repr()
        msg = [msg]
        if self.use_pca:
            msg.append(f'Number of PCA components: {self.num_pca_comps}')
        msg.append(f'Flat hand mean: {self.flat_hand_mean}')
        return '\n'.join(msg)

    def forward(
        self,
        betas: Optional[Tensor] = None,
        global_orient: Optional[Tensor] = None,
        body_pose: Optional[Tensor] = None,
        left_hand_pose: Optional[Tensor] = None,
        right_hand_pose: Optional[Tensor] = None,
        transl: Optional[Tensor] = None,
        return_verts: bool = True,
        return_full_pose: bool = False,
        pose2rot: bool = True,
        **kwargs
    ) -> SMPLHOutput:
        '''
        '''
        print('SMPLH forward')
        # If no shape and pose parameters are passed along, then use the
        # ones from the module
        global_orient = (global_orient if global_orient is not None else
                         self.global_orient)
        body_pose = body_pose if body_pose is not None else self.body_pose
        betas = betas if betas is not None else self.betas
        left_hand_pose = (left_hand_pose if left_hand_pose is not None else
                          self.left_hand_pose)
        right_hand_pose = (right_hand_pose if right_hand_pose is not None else
                           self.right_hand_pose)

        apply_trans = transl is not None or hasattr(self, 'transl')
        if transl is None:
            if hasattr(self, 'transl'):
                transl = self.transl

        if self.use_pca:
            left_hand_pose = torch.einsum(
                'bi,ij->bj', [left_hand_pose, self.left_hand_components])
            right_hand_pose = torch.einsum(
                'bi,ij->bj', [right_hand_pose, self.right_hand_components])

        full_pose = torch.cat([global_orient, body_pose,
                               left_hand_pose,
                               right_hand_pose], dim=1)
        full_pose += self.pose_mean

        vertices, joints = lbs(betas, full_pose, self.v_template,
                               self.shapedirs, self.posedirs,
                               self.J_regressor, self.parents,
                               self.lbs_weights, pose2rot=pose2rot)

        # Add any extra joints that might be needed
        joints = self.vertex_joint_selector(vertices, joints)
        if self.joint_mapper is not None:
            joints = self.joint_mapper(joints)

        if apply_trans:
            joints += transl.unsqueeze(dim=1)
            vertices += transl.unsqueeze(dim=1)

        output = SMPLHOutput(vertices=vertices if return_verts else None,
                             joints=joints,
                             betas=betas,
                             global_orient=global_orient,
                             body_pose=body_pose,
                             left_hand_pose=left_hand_pose,
                             right_hand_pose=right_hand_pose,
                             full_pose=full_pose if return_full_pose else None)

        return output


class SMPLHLayer(SMPLH):

    def __init__(
        self, *args, **kwargs
    ) -> None:
        ''' SMPL+H as a layer model constructor
        '''
        super(SMPLHLayer, self).__init__(
            create_global_orient=False,
            create_body_pose=False,
            create_left_hand_pose=False,
            create_right_hand_pose=False,
            create_betas=False,
            create_transl=False,
            *args,
            **kwargs)

    def forward(
        self,
        betas: Optional[Tensor] = None,
        global_orient: Optional[Tensor] = None,
        body_pose: Optional[Tensor] = None,
        left_hand_pose: Optional[Tensor] = None,
        right_hand_pose: Optional[Tensor] = None,
        transl: Optional[Tensor] = None,
        return_verts: bool = True,
        return_full_pose: bool = False,
        pose2rot: bool = True,
        **kwargs
    ) -> SMPLHOutput:
        ''' Forward pass for the SMPL+H model

            Parameters
            ----------
            global_orient: torch.tensor, optional, shape Bx3x3
                Global rotation of the body. Useful if someone wishes to
                predicts this with an external model. It is expected to be in
                rotation matrix format. (default=None)
            betas: torch.tensor, optional, shape BxN_b
                Shape parameters. For example, it can used if shape parameters
                `betas` are predicted from some external model.
                (default=None)
            body_pose: torch.tensor, optional, shape BxJx3x3
                If given, ignore the member variable `body_pose` and use it
                instead. For example, it can used if someone predicts the
                pose of the body joints are predicted from some external model.
                It should be a tensor that contains joint rotations in
                rotation matrix format. (default=None)
            left_hand_pose: torch.tensor, optional, shape Bx15x3x3
                If given, contains the pose of the left hand.
                It should be a tensor that contains joint rotations in
                rotation matrix format. (default=None)
            right_hand_pose: torch.tensor, optional, shape Bx15x3x3
                If given, contains the pose of the right hand.
                It should be a tensor that contains joint rotations in
                rotation matrix format. (default=None)
            transl: torch.tensor, optional, shape Bx3
                Translation vector of the body.
                For example, it can used if the translation
                `transl` is predicted from some external model.
                (default=None)
            return_verts: bool, optional
                Return the vertices. (default=True)
            return_full_pose: bool, optional
                Returns the full axis-angle pose vector (default=False)

            Returns
            -------
        '''
        model_vars = [betas, global_orient, body_pose, transl, left_hand_pose,
                      right_hand_pose]
        batch_size = 1
        for var in model_vars:
            if var is None:
                continue
            batch_size = max(batch_size, len(var))
        device, dtype = self.shapedirs.device, self.shapedirs.dtype
        if global_orient is None:
            global_orient = torch.eye(3, device=device, dtype=dtype).view(
                1, 1, 3, 3).expand(batch_size, -1, -1, -1).contiguous()
        if body_pose is None:
            body_pose = torch.eye(3, device=device, dtype=dtype).view(
                1, 1, 3, 3).expand(batch_size, 21, -1, -1).contiguous()
        if left_hand_pose is None:
            left_hand_pose = torch.eye(3, device=device, dtype=dtype).view(
                1, 1, 3, 3).expand(batch_size, 15, -1, -1).contiguous()
        if right_hand_pose is None:
            right_hand_pose = torch.eye(3, device=device, dtype=dtype).view(
                1, 1, 3, 3).expand(batch_size, 15, -1, -1).contiguous()
        if betas is None:
            betas = torch.zeros([batch_size, self.num_betas],
                                dtype=dtype, device=device)
        if transl is None:
            transl = torch.zeros([batch_size, 3], dtype=dtype, device=device)

        # Concatenate all pose vectors
        full_pose = torch.cat(
            [global_orient.reshape(-1, 1, 3, 3),
             body_pose.reshape(-1, self.NUM_BODY_JOINTS, 3, 3),
             left_hand_pose.reshape(-1, self.NUM_HAND_JOINTS, 3, 3),
             right_hand_pose.reshape(-1, self.NUM_HAND_JOINTS, 3, 3)],
            dim=1)

        vertices, joints = lbs(betas, full_pose, self.v_template,
                               self.shapedirs, self.posedirs,
                               self.J_regressor, self.parents,
                               self.lbs_weights, pose2rot=False)

        # Add any extra joints that might be needed
        joints = self.vertex_joint_selector(vertices, joints)
        if self.joint_mapper is not None:
            joints = self.joint_mapper(joints)

        if transl is not None:
            joints += transl.unsqueeze(dim=1)
            vertices += transl.unsqueeze(dim=1)

        output = SMPLHOutput(vertices=vertices if return_verts else None,
                             joints=joints,
                             betas=betas,
                             global_orient=global_orient,
                             body_pose=body_pose,
                             left_hand_pose=left_hand_pose,
                             right_hand_pose=right_hand_pose,
                             full_pose=full_pose if return_full_pose else None)

        return output


class SMPLX(SMPLH):
    '''
    SMPL-X (SMPL eXpressive) is a unified body model, with shape parameters
    trained jointly for the face, hands and body.
    SMPL-X uses standard vertex based linear blend skinning with learned
    corrective blend shapes, has N=10475 vertices and K=54 joints,
    which includes joints for the neck, jaw, eyeballs and fingers.
    '''

    NUM_BODY_JOINTS = SMPLH.NUM_BODY_JOINTS
    NUM_HAND_JOINTS = 15
    NUM_FACE_JOINTS = 3
    NUM_JOINTS = NUM_BODY_JOINTS + 2 * NUM_HAND_JOINTS + NUM_FACE_JOINTS
    EXPRESSION_SPACE_DIM = 100
    NECK_IDX = 12

    def __init__(
        self, model_path: str,
        kid_template_path: str = '',
        num_expression_coeffs: int = 10,
        create_expression: bool = True,
        expression: Optional[Tensor] = None,
        create_jaw_pose: bool = True,
        jaw_pose: Optional[Tensor] = None,
        create_leye_pose: bool = True,
        leye_pose: Optional[Tensor] = None,
        create_reye_pose=True,
        reye_pose: Optional[Tensor] = None,
        use_face_contour: bool = False,
        batch_size: int = 1,
        gender: str = 'neutral',
        age: str = 'adult',
        dtype=torch.float32,
        ext: str = 'npz',
        **kwargs
    ) -> None:
        ''' SMPLX model constructor

            Parameters
            ----------
            model_path: str
                The path to the folder or to the file where the model
                parameters are stored
            num_expression_coeffs: int, optional
                Number of expression components to use
                (default = 10).
            create_expression: bool, optional
                Flag for creating a member variable for the expression space
                (default = True).
            expression: torch.tensor, optional, Bx10
                The default value for the expression member variable.
                (default = None)
            create_jaw_pose: bool, optional
                Flag for creating a member variable for the jaw pose.
                (default = False)
            jaw_pose: torch.tensor, optional, Bx3
                The default value for the jaw pose variable.
                (default = None)
            create_leye_pose: bool, optional
                Flag for creating a member variable for the left eye pose.
                (default = False)
            leye_pose: torch.tensor, optional, Bx10
                The default value for the left eye pose variable.
                (default = None)
            create_reye_pose: bool, optional
                Flag for creating a member variable for the right eye pose.
                (default = False)
            reye_pose: torch.tensor, optional, Bx10
                The default value for the right eye pose variable.
                (default = None)
            use_face_contour: bool, optional
                Whether to compute the keypoints that form the facial contour
            batch_size: int, optional
                The batch size used for creating the member variables
            gender: str, optional
                Which gender to load
            dtype: torch.dtype
                The data type for the created variables
        '''

        # Load the model
        if osp.isdir(model_path):
            model_fn = 'SMPLX_{}.{ext}'.format(gender.upper(), ext=ext)
            smplx_path = os.path.join(model_path, model_fn)
        else:
            smplx_path = model_path
        assert osp.exists(smplx_path), 'Path {} does not exist!'.format(
            smplx_path)

        if ext == 'pkl':
            with open(smplx_path, 'rb') as smplx_file:
                model_data = pickle.load(smplx_file, encoding='latin1')
        elif ext == 'npz':
            model_data = np.load(smplx_path, allow_pickle=True)
        else:
            raise ValueError('Unknown extension: {}'.format(ext))

        data_struct = Struct(**model_data)

        super(SMPLX, self).__init__(
            model_path=model_path,
            kid_template_path=kid_template_path,
            data_struct=data_struct,
            dtype=dtype,
            batch_size=batch_size,
            vertex_ids=VERTEX_IDS['smplx'],
            gender=gender, age=age, ext=ext,
            **kwargs)

        lmk_faces_idx = data_struct.lmk_faces_idx
        self.register_buffer('lmk_faces_idx',
                             torch.tensor(lmk_faces_idx, dtype=torch.long))
        lmk_bary_coords = data_struct.lmk_bary_coords
        self.register_buffer('lmk_bary_coords',
                             torch.tensor(lmk_bary_coords, dtype=dtype))

        self.use_face_contour = use_face_contour
        if self.use_face_contour:
            dynamic_lmk_faces_idx = data_struct.dynamic_lmk_faces_idx
            dynamic_lmk_faces_idx = torch.tensor(
                dynamic_lmk_faces_idx,
                dtype=torch.long)
            self.register_buffer('dynamic_lmk_faces_idx',
                                 dynamic_lmk_faces_idx)

            dynamic_lmk_bary_coords = data_struct.dynamic_lmk_bary_coords
            dynamic_lmk_bary_coords = torch.tensor(
                dynamic_lmk_bary_coords, dtype=dtype)
            self.register_buffer('dynamic_lmk_bary_coords',
                                 dynamic_lmk_bary_coords)

            neck_kin_chain = find_joint_kin_chain(self.NECK_IDX, self.parents)
            self.register_buffer(
                'neck_kin_chain',
                torch.tensor(neck_kin_chain, dtype=torch.long))

        if create_jaw_pose:
            if jaw_pose is None:
                default_jaw_pose = torch.zeros([batch_size, 3], dtype=dtype)
            else:
                default_jaw_pose = torch.tensor(jaw_pose, dtype=dtype)
            jaw_pose_param = nn.Parameter(default_jaw_pose,
                                          requires_grad=True)
            self.register_parameter('jaw_pose', jaw_pose_param)

        if create_leye_pose:
            if leye_pose is None:
                default_leye_pose = torch.zeros([batch_size, 3], dtype=dtype)
            else:
                default_leye_pose = torch.tensor(leye_pose, dtype=dtype)
            leye_pose_param = nn.Parameter(default_leye_pose,
                                           requires_grad=True)
            self.register_parameter('leye_pose', leye_pose_param)

        if create_reye_pose:
            if reye_pose is None:
                default_reye_pose = torch.zeros([batch_size, 3], dtype=dtype)
            else:
                default_reye_pose = torch.tensor(reye_pose, dtype=dtype)
            reye_pose_param = nn.Parameter(default_reye_pose,
                                           requires_grad=True)
            self.register_parameter('reye_pose', reye_pose_param)

        shapedirs = data_struct.shapedirs
        if len(shapedirs.shape) < 3:
            shapedirs = shapedirs[:, :, None]
        if (shapedirs.shape[-1] < self.SHAPE_SPACE_DIM +
                self.EXPRESSION_SPACE_DIM):
            print(f'WARNING: You are using a {self.name()} model, with only'
                  ' 10 shape and 10 expression coefficients.')
            expr_start_idx = 10
            expr_end_idx = 20
            num_expression_coeffs = min(num_expression_coeffs, 10)
        else:
            expr_start_idx = self.SHAPE_SPACE_DIM
            expr_end_idx = self.SHAPE_SPACE_DIM + num_expression_coeffs
            num_expression_coeffs = min(
                num_expression_coeffs, self.EXPRESSION_SPACE_DIM)

        self._num_expression_coeffs = num_expression_coeffs

        expr_dirs = shapedirs[:, :, expr_start_idx:expr_end_idx]
        self.register_buffer(
            'expr_dirs', to_tensor(to_np(expr_dirs), dtype=dtype))

        if create_expression:
            if expression is None:
                default_expression = torch.zeros(
                    [batch_size, self.num_expression_coeffs], dtype=dtype)
            else:
                default_expression = torch.tensor(expression, dtype=dtype)
            expression_param = nn.Parameter(default_expression,
                                            requires_grad=True)
            self.register_parameter('expression', expression_param)

    def name(self) -> str:
        return 'SMPL-X'

    @property
    def num_expression_coeffs(self):
        return self._num_expression_coeffs

    def create_mean_pose(self, data_struct, flat_hand_mean=False):
        # Create the array for the mean pose. If flat_hand is false, then use
        # the mean that is given by the data, rather than the flat open hand
        global_orient_mean = torch.zeros([3], dtype=self.dtype)
        body_pose_mean = torch.zeros([self.NUM_BODY_JOINTS * 3],
                                     dtype=self.dtype)
        jaw_pose_mean = torch.zeros([3], dtype=self.dtype)
        leye_pose_mean = torch.zeros([3], dtype=self.dtype)
        reye_pose_mean = torch.zeros([3], dtype=self.dtype)

        pose_mean = np.concatenate([global_orient_mean, body_pose_mean,
                                    jaw_pose_mean,
                                    leye_pose_mean, reye_pose_mean,
                                    self.left_hand_mean, self.right_hand_mean],
                                   axis=0)

        return pose_mean

    def extra_repr(self):
        msg = super(SMPLX, self).extra_repr()
        msg = [
            msg,
            f'Number of Expression Coefficients: {self.num_expression_coeffs}'
        ]
        return '\n'.join(msg)

    def forward(
        self,
        betas: Optional[Tensor] = None,
        global_orient: Optional[Tensor] = None,
        body_pose: Optional[Tensor] = None,
        left_hand_pose: Optional[Tensor] = None,
        right_hand_pose: Optional[Tensor] = None,
        transl: Optional[Tensor] = None,
        expression: Optional[Tensor] = None,
        jaw_pose: Optional[Tensor] = None,
        leye_pose: Optional[Tensor] = None,
        reye_pose: Optional[Tensor] = None,
        return_verts: bool = True,
        return_full_pose: bool = False,
        pose2rot: bool = True,
        return_shaped: bool = True,
        **kwargs
    ) -> SMPLXOutput:
        '''
        Forward pass for the SMPLX model

            Parameters
            ----------
            global_orient: torch.tensor, optional, shape Bx3
                If given, ignore the member variable and use it as the global
                rotation of the body. Useful if someone wishes to predicts this
                with an external model. (default=None)
            betas: torch.tensor, optional, shape BxN_b
                If given, ignore the member variable `betas` and use it
                instead. For example, it can used if shape parameters
                `betas` are predicted from some external model.
                (default=None)
            expression: torch.tensor, optional, shape BxN_e
                If given, ignore the member variable `expression` and use it
                instead. For example, it can used if expression parameters
                `expression` are predicted from some external model.
            body_pose: torch.tensor, optional, shape Bx(J*3)
                If given, ignore the member variable `body_pose` and use it
                instead. For example, it can used if someone predicts the
                pose of the body joints are predicted from some external model.
                It should be a tensor that contains joint rotations in
                axis-angle format. (default=None)
            left_hand_pose: torch.tensor, optional, shape BxP
                If given, ignore the member variable `left_hand_pose` and
                use this instead. It should either contain PCA coefficients or
                joint rotations in axis-angle format.
            right_hand_pose: torch.tensor, optional, shape BxP
                If given, ignore the member variable `right_hand_pose` and
                use this instead. It should either contain PCA coefficients or
                joint rotations in axis-angle format.
            jaw_pose: torch.tensor, optional, shape Bx3
                If given, ignore the member variable `jaw_pose` and
                use this instead. It should either joint rotations in
                axis-angle format.
            transl: torch.tensor, optional, shape Bx3
                If given, ignore the member variable `transl` and use it
                instead. For example, it can used if the translation
                `transl` is predicted from some external model.
                (default=None)
            return_verts: bool, optional
                Return the vertices. (default=True)
            return_full_pose: bool, optional
                Returns the full axis-angle pose vector (default=False)

            Returns
            -------
                output: ModelOutput
                A named tuple of type `ModelOutput`
        '''
        print("SMPLX forward")
        # If no shape and pose parameters are passed along, then use the
        # ones from the module
        global_orient = (global_orient if global_orient is not None else
                         self.global_orient)
        body_pose = body_pose if body_pose is not None else self.body_pose
        betas = betas if betas is not None else self.betas

        left_hand_pose = (left_hand_pose if left_hand_pose is not None else
                          self.left_hand_pose)
        right_hand_pose = (right_hand_pose if right_hand_pose is not None else
                           self.right_hand_pose)
        jaw_pose = jaw_pose if jaw_pose is not None else self.jaw_pose
        leye_pose = leye_pose if leye_pose is not None else self.leye_pose
        reye_pose = reye_pose if reye_pose is not None else self.reye_pose
        expression = expression if expression is not None else self.expression

        apply_trans = transl is not None or hasattr(self, 'transl')
        if transl is None:
            if hasattr(self, 'transl'):
                transl = self.transl

        if self.use_pca:
            left_hand_pose = torch.einsum(
                'bi,ij->bj', [left_hand_pose, self.left_hand_components])
            right_hand_pose = torch.einsum(
                'bi,ij->bj', [right_hand_pose, self.right_hand_components])

        full_pose = torch.cat([global_orient.reshape(-1, 1, 3),
                               body_pose.reshape(-1, self.NUM_BODY_JOINTS, 3),
                               jaw_pose.reshape(-1, 1, 3),
                               leye_pose.reshape(-1, 1, 3),
                               reye_pose.reshape(-1, 1, 3),
                               left_hand_pose.reshape(-1, 15, 3),
                               right_hand_pose.reshape(-1, 15, 3)],
                              dim=1).reshape(-1, 165)

        # Add the mean pose of the model. Does not affect the body, only the
        # hands when flat_hand_mean == False
        full_pose += self.pose_mean

        batch_size = max(betas.shape[0], global_orient.shape[0],
                         body_pose.shape[0])
        # Concatenate the shape and expression coefficients
        scale = int(batch_size / betas.shape[0])
        if scale > 1:
            betas = betas.expand(scale, -1)
            expression = expression.expand(scale, -1)
        shape_components = torch.cat([betas, expression], dim=-1)

        shapedirs = torch.cat([self.shapedirs, self.expr_dirs], dim=-1)

        vertices, joints = lbs(shape_components, full_pose, self.v_template,
                               shapedirs, self.posedirs,
                               self.J_regressor, self.parents,
                               self.lbs_weights, pose2rot=pose2rot,
                               )

        lmk_faces_idx = self.lmk_faces_idx.unsqueeze(
            dim=0).expand(batch_size, -1).contiguous()
        lmk_bary_coords = self.lmk_bary_coords.unsqueeze(dim=0).repeat(
            self.batch_size, 1, 1)
        if self.use_face_contour:
            lmk_idx_and_bcoords = find_dynamic_lmk_idx_and_bcoords(
                vertices, full_pose, self.dynamic_lmk_faces_idx,
                self.dynamic_lmk_bary_coords,
                self.neck_kin_chain,
                pose2rot=True,
            )
            dyn_lmk_faces_idx, dyn_lmk_bary_coords = lmk_idx_and_bcoords

            lmk_faces_idx = torch.cat([lmk_faces_idx,
                                       dyn_lmk_faces_idx], 1)
            lmk_bary_coords = torch.cat(
                [lmk_bary_coords.expand(batch_size, -1, -1),
                 dyn_lmk_bary_coords], 1)

        landmarks = vertices2landmarks(vertices, self.faces_tensor,
                                       lmk_faces_idx,
                                       lmk_bary_coords)

        # Add any extra joints that might be needed
        joints = self.vertex_joint_selector(vertices, joints)
        # Add the landmarks to the joints
        joints = torch.cat([joints, landmarks], dim=1)
        # Map the joints to the current dataset

        if self.joint_mapper is not None:
            joints = self.joint_mapper(joints=joints, vertices=vertices)

        if apply_trans:
            joints += transl.unsqueeze(dim=1)
            vertices += transl.unsqueeze(dim=1)

        v_shaped = None
        if return_shaped:
            v_shaped = self.v_template + blend_shapes(betas, self.shapedirs)
        else:
            v_shaped = Tensor(0)
        output = SMPLXOutput(vertices=vertices if return_verts else None,
                              joints=joints,
                              betas=betas,
                              expression=expression,
                              global_orient=global_orient,
                              transl=transl,
                              body_pose=body_pose,
                              left_hand_pose=left_hand_pose,
                              right_hand_pose=right_hand_pose,
                              jaw_pose=jaw_pose,
                              v_shaped=v_shaped,
                              full_pose=full_pose if return_full_pose else None)
        return output


class SMPLXLayer(SMPLX):
    def __init__(
        self,
        *args,
        **kwargs
    ) -> None:
        # Just create a SMPLX module without any member variables
        super(SMPLXLayer, self).__init__(
            create_global_orient=False,
            create_body_pose=False,
            create_left_hand_pose=False,
            create_right_hand_pose=False,
            create_jaw_pose=False,
            create_leye_pose=False,
            create_reye_pose=False,
            create_betas=False,
            create_expression=False,
            create_transl=False,
            *args, **kwargs,
        )

    def forward(
        self,
        betas: Optional[Tensor] = None,
        global_orient: Optional[Tensor] = None,
        body_pose: Optional[Tensor] = None,
        left_hand_pose: Optional[Tensor] = None,
        right_hand_pose: Optional[Tensor] = None,
        transl: Optional[Tensor] = None,
        expression: Optional[Tensor] = None,
        jaw_pose: Optional[Tensor] = None,
        leye_pose: Optional[Tensor] = None,
        reye_pose: Optional[Tensor] = None,
        return_verts: bool = True,
        return_full_pose: bool = True,
        **kwargs
    ) -> TensorOutput:
        '''
        Forward pass for the SMPLX model

            Parameters
            ----------
            global_orient: torch.tensor, optional, shape Bx3x3
                If given, ignore the member variable and use it as the global
                rotation of the body. Useful if someone wishes to predicts this
                with an external model. It is expected to be in rotation matrix
                format. (default=None)
            betas: torch.tensor, optional, shape BxN_b
                If given, ignore the member variable `betas` and use it
                instead. For example, it can used if shape parameters
                `betas` are predicted from some external model.
                (default=None)
            expression: torch.tensor, optional, shape BxN_e
                Expression coefficients.
                For example, it can used if expression parameters
                `expression` are predicted from some external model.
            body_pose: torch.tensor, optional, shape BxJx3x3
                If given, ignore the member variable `body_pose` and use it
                instead. For example, it can used if someone predicts the
                pose of the body joints are predicted from some external model.
                It should be a tensor that contains joint rotations in
                rotation matrix format. (default=None)
            left_hand_pose: torch.tensor, optional, shape Bx15x3x3
                If given, contains the pose of the left hand.
                It should be a tensor that contains joint rotations in
                rotation matrix format. (default=None)
            right_hand_pose: torch.tensor, optional, shape Bx15x3x3
                If given, contains the pose of the right hand.
                It should be a tensor that contains joint rotations in
                rotation matrix format. (default=None)
            jaw_pose: torch.tensor, optional, shape Bx3x3
                Jaw pose. It should either joint rotations in
                rotation matrix format.
            transl: torch.tensor, optional, shape Bx3
                Translation vector of the body.
                For example, it can used if the translation
                `transl` is predicted from some external model.
                (default=None)
            return_verts: bool, optional
                Return the vertices. (default=True)
            return_full_pose: bool, optional
                Returns the full pose vector (default=False)
            Returns
            -------
                output: ModelOutput
                A data class that contains the posed vertices and joints
        '''
        device, dtype = self.shapedirs.device, self.shapedirs.dtype

        model_vars = [betas, global_orient, body_pose, transl,
                      expression, left_hand_pose, right_hand_pose, jaw_pose]
        batch_size = 1
        for var in model_vars:
            if var is None:
                continue
            batch_size = max(batch_size, len(var))

        if global_orient is None:
            global_orient = torch.eye(3, device=device, dtype=dtype).view(
                1, 1, 3, 3).expand(batch_size, -1, -1, -1).contiguous()
        if body_pose is None:
            body_pose = torch.eye(3, device=device, dtype=dtype).view(
                1, 1, 3, 3).expand(
                    batch_size, self.NUM_BODY_JOINTS, -1, -1).contiguous()
        if left_hand_pose is None:
            left_hand_pose = torch.eye(3, device=device, dtype=dtype).view(
                1, 1, 3, 3).expand(batch_size, 15, -1, -1).contiguous()
        if right_hand_pose is None:
            right_hand_pose = torch.eye(3, device=device, dtype=dtype).view(
                1, 1, 3, 3).expand(batch_size, 15, -1, -1).contiguous()
        if jaw_pose is None:
            jaw_pose = torch.eye(3, device=device, dtype=dtype).view(
                1, 1, 3, 3).expand(batch_size, -1, -1, -1).contiguous()
        if leye_pose is None:
            leye_pose = torch.eye(3, device=device, dtype=dtype).view(
                1, 1, 3, 3).expand(batch_size, -1, -1, -1).contiguous()
        if reye_pose is None:
            reye_pose = torch.eye(3, device=device, dtype=dtype).view(
                1, 1, 3, 3).expand(batch_size, -1, -1, -1).contiguous()
        if expression is None:
            expression = torch.zeros([batch_size, self.num_expression_coeffs],
                                     dtype=dtype, device=device)
        if betas is None:
            betas = torch.zeros([batch_size, self.num_betas],
                                dtype=dtype, device=device)
        if transl is None:
            transl = torch.zeros([batch_size, 3], dtype=dtype, device=device)

        # Concatenate all pose vectors
        full_pose = torch.cat(
            [global_orient.reshape(-1, 1, 3, 3),
             body_pose.reshape(-1, self.NUM_BODY_JOINTS, 3, 3),
             jaw_pose.reshape(-1, 1, 3, 3),
             leye_pose.reshape(-1, 1, 3, 3),
             reye_pose.reshape(-1, 1, 3, 3),
             left_hand_pose.reshape(-1, self.NUM_HAND_JOINTS, 3, 3),
             right_hand_pose.reshape(-1, self.NUM_HAND_JOINTS, 3, 3)],
            dim=1)
        shape_components = torch.cat([betas, expression], dim=-1)

        shapedirs = torch.cat([self.shapedirs, self.expr_dirs], dim=-1)

        vertices, joints = lbs(shape_components, full_pose, self.v_template,
                               shapedirs, self.posedirs,
                               self.J_regressor, self.parents,
                               self.lbs_weights,
                               pose2rot=False,
                               )

        lmk_faces_idx = self.lmk_faces_idx.unsqueeze(
            dim=0).expand(batch_size, -1).contiguous()
        lmk_bary_coords = self.lmk_bary_coords.unsqueeze(dim=0).repeat(
            batch_size, 1, 1)
        if self.use_face_contour:
            lmk_idx_and_bcoords = find_dynamic_lmk_idx_and_bcoords(
                vertices, full_pose,
                self.dynamic_lmk_faces_idx,
                self.dynamic_lmk_bary_coords,
                self.neck_kin_chain,
                pose2rot=False,
            )
            dyn_lmk_faces_idx, dyn_lmk_bary_coords = lmk_idx_and_bcoords

            lmk_faces_idx = torch.cat([lmk_faces_idx, dyn_lmk_faces_idx], 1)
            lmk_bary_coords = torch.cat(
                [lmk_bary_coords.expand(batch_size, -1, -1),
                 dyn_lmk_bary_coords], 1)

        landmarks = vertices2landmarks(vertices, self.faces_tensor,
                                       lmk_faces_idx,
                                       lmk_bary_coords)

        # Add any extra joints that might be needed
        joints = self.vertex_joint_selector(vertices, joints)
        # Add the landmarks to the joints
        joints = torch.cat([joints, landmarks], dim=1)
        # Map the joints to the current dataset

        if self.joint_mapper is not None:
            joints = self.joint_mapper(joints=joints, vertices=vertices)

        if transl is not None:
            joints += transl.unsqueeze(dim=1)
            vertices += transl.unsqueeze(dim=1)

        output = TensorOutput(vertices=vertices if return_verts else Tensor(0),
                              joints=joints,
                              betas=betas,
                              expression=expression,
                              global_orient=global_orient,
                              body_pose=body_pose,
                              left_hand_pose=left_hand_pose,
                              right_hand_pose=right_hand_pose,
                              jaw_pose=jaw_pose,
                              transl=transl if transl != None else Tensor(0),
                              full_pose=full_pose if return_full_pose else Tensor(0))

        return output


class MANO(SMPL):
    # The hand joints are replaced by MANO
    NUM_BODY_JOINTS = 1
    NUM_HAND_JOINTS = 15
    NUM_JOINTS = NUM_BODY_JOINTS + NUM_HAND_JOINTS

    def __init__(
        self,
        model_path: str,
        is_rhand: bool = True,
        data_struct: Optional[Struct] = None,
        create_hand_pose: bool = True,
        hand_pose: Optional[Tensor] = None,
        use_pca: bool = True,
        num_pca_comps: int = 6,
        flat_hand_mean: bool = False,
        batch_size: int = 1,
        dtype=torch.float32,
        vertex_ids=None,
        use_compressed: bool = True,
        ext: str = 'pkl',
        **kwargs
    ) -> None:
        ''' MANO model constructor

            Parameters
            ----------
            model_path: str
                The path to the folder or to the file where the model
                parameters are stored
            data_struct: Strct
                A struct object. If given, then the parameters of the model are
                read from the object. Otherwise, the model tries to read the
                parameters from the given `model_path`. (default = None)
            create_hand_pose: bool, optional
                Flag for creating a member variable for the pose of the right
                hand. (default = True)
            hand_pose: torch.tensor, optional, BxP
                The default value for the right hand pose member variable.
                (default = None)
            num_pca_comps: int, optional
                The number of PCA components to use for each hand.
                (default = 6)
            flat_hand_mean: bool, optional
                If False, then the pose of the hand is initialized to False.
            batch_size: int, optional
                The batch size used for creating the member variables
            dtype: torch.dtype, optional
                The data type for the created variables
            vertex_ids: dict, optional
                A dictionary containing the indices of the extra vertices that
                will be selected
        '''

        self.num_pca_comps = num_pca_comps
        self.is_rhand = is_rhand
        # If no data structure is passed, then load the data from the given
        # model folder
        if data_struct is None:
            # Load the model
            if osp.isdir(model_path):
                model_fn = 'MANO_{}.{ext}'.format(
                    'RIGHT' if is_rhand else 'LEFT', ext=ext)
                mano_path = os.path.join(model_path, model_fn)
            else:
                mano_path = model_path
                self.is_rhand = True if 'RIGHT' in os.path.basename(
                    model_path) else False
            assert osp.exists(mano_path), 'Path {} does not exist!'.format(
                mano_path)

            if ext == 'pkl':
                with open(mano_path, 'rb') as mano_file:
                    model_data = pickle.load(mano_file, encoding='latin1')
            elif ext == 'npz':
                model_data = np.load(mano_path, allow_pickle=True)
            else:
                raise ValueError('Unknown extension: {}'.format(ext))
            data_struct = Struct(**model_data)

        if vertex_ids is None:
            vertex_ids = VERTEX_IDS['smplh']

        super(MANO, self).__init__(
            model_path=model_path, data_struct=data_struct,
            batch_size=batch_size, vertex_ids=vertex_ids,
            use_compressed=use_compressed, dtype=dtype, ext=ext, **kwargs)

        # add only MANO tips to the extra joints
        self.vertex_joint_selector.extra_joints_idxs = to_tensor(
            list(VERTEX_IDS['mano'].values()), dtype=torch.long)

        self.use_pca = use_pca
        self.num_pca_comps = num_pca_comps
        if self.num_pca_comps == 45:
            self.use_pca = False
        self.flat_hand_mean = flat_hand_mean

        hand_components = data_struct.hands_components[:num_pca_comps]

        self.np_hand_components = hand_components

        if self.use_pca:
            self.register_buffer(
                'hand_components',
                torch.tensor(hand_components, dtype=dtype))

        if self.flat_hand_mean:
            hand_mean = np.zeros_like(data_struct.hands_mean)
        else:
            hand_mean = data_struct.hands_mean

        self.register_buffer('hand_mean',
                             to_tensor(hand_mean, dtype=self.dtype))

        # Create the buffers for the pose of the left hand
        hand_pose_dim = num_pca_comps if use_pca else 3 * self.NUM_HAND_JOINTS
        if create_hand_pose:
            if hand_pose is None:
                default_hand_pose = torch.zeros([batch_size, hand_pose_dim],
                                                dtype=dtype)
            else:
                default_hand_pose = torch.tensor(hand_pose, dtype=dtype)

            hand_pose_param = nn.Parameter(default_hand_pose,
                                           requires_grad=True)
            self.register_parameter('hand_pose',
                                    hand_pose_param)

        # Create the buffer for the mean pose.
        pose_mean = self.create_mean_pose(
            data_struct, flat_hand_mean=flat_hand_mean)
        pose_mean_tensor = pose_mean.clone().to(dtype)
        # pose_mean_tensor = torch.tensor(pose_mean, dtype=dtype)
        self.register_buffer('pose_mean', pose_mean_tensor)

    def name(self) -> str:
        return 'MANO'

    def create_mean_pose(self, data_struct, flat_hand_mean=False):
        # Create the array for the mean pose. If flat_hand is false, then use
        # the mean that is given by the data, rather than the flat open hand
        global_orient_mean = torch.zeros([3], dtype=self.dtype)
        pose_mean = torch.cat([global_orient_mean, self.hand_mean], dim=0)
        return pose_mean

    def extra_repr(self):
        msg = [super(MANO, self).extra_repr()]
        if self.use_pca:
            msg.append(f'Number of PCA components: {self.num_pca_comps}')
        msg.append(f'Flat hand mean: {self.flat_hand_mean}')
        return '\n'.join(msg)

    def forward(
        self,
        betas: Optional[Tensor] = None,
        global_orient: Optional[Tensor] = None,
        hand_pose: Optional[Tensor] = None,
        transl: Optional[Tensor] = None,
        return_verts: bool = True,
        return_full_pose: bool = False,
        **kwargs
    ) -> MANOOutput:
        ''' Forward pass for the MANO model
        '''
        # If no shape and pose parameters are passed along, then use the
        # ones from the module
        global_orient = (global_orient if global_orient is not None else
                         self.global_orient)
        betas = betas if betas is not None else self.betas
        hand_pose = (hand_pose if hand_pose is not None else
                     self.hand_pose)

        apply_trans = transl is not None or hasattr(self, 'transl')
        if transl is None:
            if hasattr(self, 'transl'):
                transl = self.transl

        if self.use_pca:
            hand_pose = torch.einsum(
                'bi,ij->bj', [hand_pose, self.hand_components])

        full_pose = torch.cat([global_orient, hand_pose], dim=1)
        full_pose += self.pose_mean

        vertices, joints = lbs(betas, full_pose, self.v_template,
                               self.shapedirs, self.posedirs,
                               self.J_regressor, self.parents,
                               self.lbs_weights, pose2rot=True,
                               )

        # # Add pre-selected extra joints that might be needed
        # joints = self.vertex_joint_selector(vertices, joints)

        if self.joint_mapper is not None:
            joints = self.joint_mapper(joints)

        if apply_trans:
            joints = joints + transl.unsqueeze(dim=1)
            vertices = vertices + transl.unsqueeze(dim=1)

        output = MANOOutput(vertices=vertices if return_verts else None,
                            joints=joints if return_verts else None,
                            betas=betas,
                            global_orient=global_orient,
                            hand_pose=hand_pose,
                            full_pose=full_pose if return_full_pose else None)

        return output


class MANOLayer(MANO):
    def __init__(self, *args, **kwargs) -> None:
        ''' MANO as a layer model constructor
        '''
        super(MANOLayer, self).__init__(
            create_global_orient=False,
            create_hand_pose=False,
            create_betas=False,
            create_transl=False,
            *args, **kwargs)

    def name(self) -> str:
        return 'MANO'

    def forward(
        self,
        betas: Optional[Tensor] = None,
        global_orient: Optional[Tensor] = None,
        hand_pose: Optional[Tensor] = None,
        transl: Optional[Tensor] = None,
        return_verts: bool = True,
        return_full_pose: bool = False,
        **kwargs
    ) -> MANOOutput:
        ''' Forward pass for the MANO model
        '''
        device, dtype = self.shapedirs.device, self.shapedirs.dtype
        if global_orient is None:
            batch_size = 1
            global_orient = torch.eye(3, device=device, dtype=dtype).view(
                1, 1, 3, 3).expand(batch_size, -1, -1, -1).contiguous()
        else:
            batch_size = global_orient.shape[0]
        if hand_pose is None:
            hand_pose = torch.eye(3, device=device, dtype=dtype).view(
                1, 1, 3, 3).expand(batch_size, 15, -1, -1).contiguous()
        if betas is None:
            betas = torch.zeros(
                [batch_size, self.num_betas], dtype=dtype, device=device)
        if transl is None:
            transl = torch.zeros([batch_size, 3], dtype=dtype, device=device)

        full_pose = torch.cat([global_orient, hand_pose], dim=1)
        vertices, joints = lbs(betas, full_pose, self.v_template,
                               self.shapedirs, self.posedirs,
                               self.J_regressor, self.parents,
                               self.lbs_weights, pose2rot=False)

        if self.joint_mapper is not None:
            joints = self.joint_mapper(joints)

        if transl is not None:
            joints = joints + transl.unsqueeze(dim=1)
            vertices = vertices + transl.unsqueeze(dim=1)

        output = MANOOutput(
            vertices=vertices if return_verts else None,
            joints=joints if return_verts else None,
            betas=betas,
            global_orient=global_orient,
            hand_pose=hand_pose,
            full_pose=full_pose if return_full_pose else None)

        return output


class FLAME(SMPL):
    NUM_JOINTS = 5
    SHAPE_SPACE_DIM = 300
    EXPRESSION_SPACE_DIM = 100
    NECK_IDX = 0

    def __init__(
        self,
        model_path: str,
        data_struct=None,
        num_expression_coeffs=10,
        create_expression: bool = True,
        expression: Optional[Tensor] = None,
        create_neck_pose: bool = True,
        neck_pose: Optional[Tensor] = None,
        create_jaw_pose: bool = True,
        jaw_pose: Optional[Tensor] = None,
        create_leye_pose: bool = True,
        leye_pose: Optional[Tensor] = None,
        create_reye_pose=True,
        reye_pose: Optional[Tensor] = None,
        use_face_contour=False,
        batch_size: int = 1,
        gender: str = 'neutral',
        dtype: torch.dtype = torch.float32,
        ext='pkl',
        **kwargs
    ) -> None:
        ''' FLAME model constructor

            Parameters
            ----------
            model_path: str
                The path to the folder or to the file where the model
                parameters are stored
            num_expression_coeffs: int, optional
                Number of expression components to use
                (default = 10).
            create_expression: bool, optional
                Flag for creating a member variable for the expression space
                (default = True).
            expression: torch.tensor, optional, Bx10
                The default value for the expression member variable.
                (default = None)
            create_neck_pose: bool, optional
                Flag for creating a member variable for the neck pose.
                (default = False)
            neck_pose: torch.tensor, optional, Bx3
                The default value for the neck pose variable.
                (default = None)
            create_jaw_pose: bool, optional
                Flag for creating a member variable for the jaw pose.
                (default = False)
            jaw_pose: torch.tensor, optional, Bx3
                The default value for the jaw pose variable.
                (default = None)
            create_leye_pose: bool, optional
                Flag for creating a member variable for the left eye pose.
                (default = False)
            leye_pose: torch.tensor, optional, Bx10
                The default value for the left eye pose variable.
                (default = None)
            create_reye_pose: bool, optional
                Flag for creating a member variable for the right eye pose.
                (default = False)
            reye_pose: torch.tensor, optional, Bx10
                The default value for the right eye pose variable.
                (default = None)
            use_face_contour: bool, optional
                Whether to compute the keypoints that form the facial contour
            batch_size: int, optional
                The batch size used for creating the member variables
            gender: str, optional
                Which gender to load
            dtype: torch.dtype
                The data type for the created variables
        '''
        model_fn = f'FLAME_{gender.upper()}.{ext}'
        flame_path = os.path.join(model_path, model_fn)
        assert osp.exists(flame_path), 'Path {} does not exist!'.format(
            flame_path)
        if ext == 'npz':
            file_data = np.load(flame_path, allow_pickle=True)
        elif ext == 'pkl':
            with open(flame_path, 'rb') as smpl_file:
                file_data = pickle.load(smpl_file, encoding='latin1')
        else:
            raise ValueError('Unknown extension: {}'.format(ext))
        data_struct = Struct(**file_data)

        super(FLAME, self).__init__(
            model_path=model_path,
            data_struct=data_struct,
            dtype=dtype,
            batch_size=batch_size,
            gender=gender,
            ext=ext,
            **kwargs)

        self.use_face_contour = use_face_contour

        self.vertex_joint_selector.extra_joints_idxs = to_tensor(
            [], dtype=torch.long)

        if create_neck_pose:
            if neck_pose is None:
                default_neck_pose = torch.zeros([batch_size, 3], dtype=dtype)
            else:
                default_neck_pose = torch.tensor(neck_pose, dtype=dtype)
            neck_pose_param = nn.Parameter(
                default_neck_pose, requires_grad=True)
            self.register_parameter('neck_pose', neck_pose_param)

        if create_jaw_pose:
            if jaw_pose is None:
                default_jaw_pose = torch.zeros([batch_size, 3], dtype=dtype)
            else:
                default_jaw_pose = torch.tensor(jaw_pose, dtype=dtype)
            jaw_pose_param = nn.Parameter(default_jaw_pose,
                                          requires_grad=True)
            self.register_parameter('jaw_pose', jaw_pose_param)

        if create_leye_pose:
            if leye_pose is None:
                default_leye_pose = torch.zeros([batch_size, 3], dtype=dtype)
            else:
                default_leye_pose = torch.tensor(leye_pose, dtype=dtype)
            leye_pose_param = nn.Parameter(default_leye_pose,
                                           requires_grad=True)
            self.register_parameter('leye_pose', leye_pose_param)

        if create_reye_pose:
            if reye_pose is None:
                default_reye_pose = torch.zeros([batch_size, 3], dtype=dtype)
            else:
                default_reye_pose = torch.tensor(reye_pose, dtype=dtype)
            reye_pose_param = nn.Parameter(default_reye_pose,
                                           requires_grad=True)
            self.register_parameter('reye_pose', reye_pose_param)

        shapedirs = data_struct.shapedirs
        if len(shapedirs.shape) < 3:
            shapedirs = shapedirs[:, :, None]
        if (shapedirs.shape[-1] < self.SHAPE_SPACE_DIM +
                self.EXPRESSION_SPACE_DIM):
            print(f'WARNING: You are using a {self.name()} model, with only'
                  ' 10 shape and 10 expression coefficients.')
            expr_start_idx = 10
            expr_end_idx = 20
            num_expression_coeffs = min(num_expression_coeffs, 10)
        else:
            expr_start_idx = self.SHAPE_SPACE_DIM
            expr_end_idx = self.SHAPE_SPACE_DIM + num_expression_coeffs
            num_expression_coeffs = min(
                num_expression_coeffs, self.EXPRESSION_SPACE_DIM)

        self._num_expression_coeffs = num_expression_coeffs

        expr_dirs = shapedirs[:, :, expr_start_idx:expr_end_idx]
        self.register_buffer(
            'expr_dirs', to_tensor(to_np(expr_dirs), dtype=dtype))

        if create_expression:
            if expression is None:
                default_expression = torch.zeros(
                    [batch_size, self.num_expression_coeffs], dtype=dtype)
            else:
                default_expression = torch.tensor(expression, dtype=dtype)
            expression_param = nn.Parameter(default_expression,
                                            requires_grad=True)
            self.register_parameter('expression', expression_param)

        # The pickle file that contains the barycentric coordinates for
        # regressing the landmarks
        landmark_bcoord_filename = osp.join(
            model_path, 'flame_static_embedding.pkl')

        with open(landmark_bcoord_filename, 'rb') as fp:
            landmarks_data = pickle.load(fp, encoding='latin1')

        lmk_faces_idx = landmarks_data['lmk_face_idx'].astype(np.int64)
        self.register_buffer('lmk_faces_idx',
                             torch.tensor(lmk_faces_idx, dtype=torch.long))
        lmk_bary_coords = landmarks_data['lmk_b_coords']
        self.register_buffer('lmk_bary_coords',
                             torch.tensor(lmk_bary_coords, dtype=dtype))
        if self.use_face_contour:
            face_contour_path = os.path.join(
                model_path, 'flame_dynamic_embedding.npy')
            contour_embeddings = np.load(face_contour_path,
                                         allow_pickle=True,
                                         encoding='latin1')[()]

            dynamic_lmk_faces_idx = np.array(
                contour_embeddings['lmk_face_idx'], dtype=np.int64)
            dynamic_lmk_faces_idx = torch.tensor(
                dynamic_lmk_faces_idx,
                dtype=torch.long)
            self.register_buffer('dynamic_lmk_faces_idx',
                                 dynamic_lmk_faces_idx)

            dynamic_lmk_b_coords = torch.tensor(
                contour_embeddings['lmk_b_coords'], dtype=dtype)
            self.register_buffer(
                'dynamic_lmk_bary_coords', dynamic_lmk_b_coords)

            neck_kin_chain = find_joint_kin_chain(self.NECK_IDX, self.parents)
            self.register_buffer(
                'neck_kin_chain',
                torch.tensor(neck_kin_chain, dtype=torch.long))

    @property
    def num_expression_coeffs(self):
        return self._num_expression_coeffs

    def name(self) -> str:
        return 'FLAME'

    def extra_repr(self):
        msg = [
            super(FLAME, self).extra_repr(),
            f'Number of Expression Coefficients: {self.num_expression_coeffs}',
            f'Use face contour: {self.use_face_contour}',
        ]
        return '\n'.join(msg)

    def forward(
        self,
        betas: Optional[Tensor] = None,
        global_orient: Optional[Tensor] = None,
        neck_pose: Optional[Tensor] = None,
        transl: Optional[Tensor] = None,
        expression: Optional[Tensor] = None,
        jaw_pose: Optional[Tensor] = None,
        leye_pose: Optional[Tensor] = None,
        reye_pose: Optional[Tensor] = None,
        return_verts: bool = True,
        return_full_pose: bool = False,
        pose2rot: bool = True,
        **kwargs
    ) -> FLAMEOutput:
        '''
        Forward pass for the SMPLX model

            Parameters
            ----------
            global_orient: torch.tensor, optional, shape Bx3
                If given, ignore the member variable and use it as the global
                rotation of the body. Useful if someone wishes to predicts this
                with an external model. (default=None)
            betas: torch.tensor, optional, shape Bx10
                If given, ignore the member variable `betas` and use it
                instead. For example, it can used if shape parameters
                `betas` are predicted from some external model.
                (default=None)
            expression: torch.tensor, optional, shape Bx10
                If given, ignore the member variable `expression` and use it
                instead. For example, it can used if expression parameters
                `expression` are predicted from some external model.
            jaw_pose: torch.tensor, optional, shape Bx3
                If given, ignore the member variable `jaw_pose` and
                use this instead. It should either joint rotations in
                axis-angle format.
            jaw_pose: torch.tensor, optional, shape Bx3
                If given, ignore the member variable `jaw_pose` and
                use this instead. It should either joint rotations in
                axis-angle format.
            transl: torch.tensor, optional, shape Bx3
                If given, ignore the member variable `transl` and use it
                instead. For example, it can used if the translation
                `transl` is predicted from some external model.
                (default=None)
            return_verts: bool, optional
                Return the vertices. (default=True)
            return_full_pose: bool, optional
                Returns the full axis-angle pose vector (default=False)

            Returns
            -------
                output: ModelOutput
                A named tuple of type `ModelOutput`
        '''

        # If no shape and pose parameters are passed along, then use the
        # ones from the module
        global_orient = (global_orient if global_orient is not None else
                         self.global_orient)
        jaw_pose = jaw_pose if jaw_pose is not None else self.jaw_pose
        neck_pose = neck_pose if neck_pose is not None else self.neck_pose

        leye_pose = leye_pose if leye_pose is not None else self.leye_pose
        reye_pose = reye_pose if reye_pose is not None else self.reye_pose

        betas = betas if betas is not None else self.betas
        expression = expression if expression is not None else self.expression

        apply_trans = transl is not None or hasattr(self, 'transl')
        if transl is None:
            if hasattr(self, 'transl'):
                transl = self.transl

        full_pose = torch.cat(
            [global_orient, neck_pose, jaw_pose, leye_pose, reye_pose], dim=1)

        batch_size = max(betas.shape[0], global_orient.shape[0],
                         jaw_pose.shape[0])
        # Concatenate the shape and expression coefficients
        scale = int(batch_size / betas.shape[0])
        if scale > 1:
            betas = betas.expand(scale, -1)
            expression = expression.expand(scale, -1)
        shape_components = torch.cat([betas, expression], dim=-1)
        shapedirs = torch.cat([self.shapedirs, self.expr_dirs], dim=-1)

        vertices, joints = lbs(shape_components, full_pose, self.v_template,
                               shapedirs, self.posedirs,
                               self.J_regressor, self.parents,
                               self.lbs_weights, pose2rot=pose2rot,
                               )

        lmk_faces_idx = self.lmk_faces_idx.unsqueeze(
            dim=0).expand(batch_size, -1).contiguous()
        lmk_bary_coords = self.lmk_bary_coords.unsqueeze(dim=0).repeat(
            batch_size, 1, 1)
        if self.use_face_contour:
            lmk_idx_and_bcoords = find_dynamic_lmk_idx_and_bcoords(
                vertices, full_pose, self.dynamic_lmk_faces_idx,
                self.dynamic_lmk_bary_coords,
                self.neck_kin_chain,
                pose2rot=True,
            )
            dyn_lmk_faces_idx, dyn_lmk_bary_coords = lmk_idx_and_bcoords
            lmk_faces_idx = torch.cat([lmk_faces_idx,
                                       dyn_lmk_faces_idx], 1)
            lmk_bary_coords = torch.cat(
                [lmk_bary_coords.expand(batch_size, -1, -1),
                 dyn_lmk_bary_coords], 1)

        landmarks = vertices2landmarks(vertices, self.faces_tensor,
                                       lmk_faces_idx,
                                       lmk_bary_coords)

        # Add any extra joints that might be needed
        joints = self.vertex_joint_selector(vertices, joints)
        # Add the landmarks to the joints
        joints = torch.cat([joints, landmarks], dim=1)

        # Map the joints to the current dataset
        if self.joint_mapper is not None:
            joints = self.joint_mapper(joints=joints, vertices=vertices)

        if apply_trans:
            joints += transl.unsqueeze(dim=1)
            vertices += transl.unsqueeze(dim=1)

        output = FLAMEOutput(vertices=vertices if return_verts else None,
                             joints=joints,
                             betas=betas,
                             expression=expression,
                             global_orient=global_orient,
                             neck_pose=neck_pose,
                             jaw_pose=jaw_pose,
                             full_pose=full_pose if return_full_pose else None)
        return output


class FLAMELayer(FLAME):
    def __init__(self, *args, **kwargs) -> None:
        ''' FLAME as a layer model constructor '''
        super(FLAMELayer, self).__init__(
            create_betas=False,
            create_expression=False,
            create_global_orient=False,
            create_neck_pose=False,
            create_jaw_pose=False,
            create_leye_pose=False,
            create_reye_pose=False,
            *args,
            **kwargs)

    def forward(
        self,
        betas: Optional[Tensor] = None,
        global_orient: Optional[Tensor] = None,
        neck_pose: Optional[Tensor] = None,
        transl: Optional[Tensor] = None,
        expression: Optional[Tensor] = None,
        jaw_pose: Optional[Tensor] = None,
        leye_pose: Optional[Tensor] = None,
        reye_pose: Optional[Tensor] = None,
        return_verts: bool = True,
        return_full_pose: bool = False,
        pose2rot: bool = True,
        **kwargs
    ) -> FLAMEOutput:
        '''
        Forward pass for the SMPLX model

            Parameters
            ----------
            global_orient: torch.tensor, optional, shape Bx3x3
                Global rotation of the body. Useful if someone wishes to
                predicts this with an external model. It is expected to be in
                rotation matrix format. (default=None)
            betas: torch.tensor, optional, shape BxN_b
                Shape parameters. For example, it can used if shape parameters
                `betas` are predicted from some external model.
                (default=None)
            expression: torch.tensor, optional, shape BxN_e
                If given, ignore the member variable `expression` and use it
                instead. For example, it can used if expression parameters
                `expression` are predicted from some external model.
            jaw_pose: torch.tensor, optional, shape Bx3x3
                Jaw pose. It should either joint rotations in
                rotation matrix format.
            transl: torch.tensor, optional, shape Bx3
                Translation vector of the body.
                For example, it can used if the translation
                `transl` is predicted from some external model.
                (default=None)
            return_verts: bool, optional
                Return the vertices. (default=True)
            return_full_pose: bool, optional
                Returns the full axis-angle pose vector (default=False)

            Returns
            -------
                output: ModelOutput
                A named tuple of type `ModelOutput`
        '''
        device, dtype = self.shapedirs.device, self.shapedirs.dtype
        if global_orient is None:
            batch_size = 1
            global_orient = torch.eye(3, device=device, dtype=dtype).view(
                1, 1, 3, 3).expand(batch_size, -1, -1, -1).contiguous()
        else:
            batch_size = global_orient.shape[0]
        if neck_pose is None:
            neck_pose = torch.eye(3, device=device, dtype=dtype).view(
                1, 1, 3, 3).expand(batch_size, 1, -1, -1).contiguous()
        if jaw_pose is None:
            jaw_pose = torch.eye(3, device=device, dtype=dtype).view(
                1, 1, 3, 3).expand(batch_size, -1, -1, -1).contiguous()
        if leye_pose is None:
            leye_pose = torch.eye(3, device=device, dtype=dtype).view(
                1, 1, 3, 3).expand(batch_size, -1, -1, -1).contiguous()
        if reye_pose is None:
            reye_pose = torch.eye(3, device=device, dtype=dtype).view(
                1, 1, 3, 3).expand(batch_size, -1, -1, -1).contiguous()
        if betas is None:
            betas = torch.zeros([batch_size, self.num_betas],
                                dtype=dtype, device=device)
        if expression is None:
            expression = torch.zeros([batch_size, self.num_expression_coeffs],
                                     dtype=dtype, device=device)
        if transl is None:
            transl = torch.zeros([batch_size, 3], dtype=dtype, device=device)

        full_pose = torch.cat(
            [global_orient, neck_pose, jaw_pose, leye_pose, reye_pose], dim=1)

        shape_components = torch.cat([betas, expression], dim=-1)
        shapedirs = torch.cat([self.shapedirs, self.expr_dirs], dim=-1)

        vertices, joints = lbs(shape_components, full_pose, self.v_template,
                               shapedirs, self.posedirs,
                               self.J_regressor, self.parents,
                               self.lbs_weights, pose2rot=False,
                               )

        lmk_faces_idx = self.lmk_faces_idx.unsqueeze(
            dim=0).expand(batch_size, -1).contiguous()
        lmk_bary_coords = self.lmk_bary_coords.unsqueeze(dim=0).repeat(
            batch_size, 1, 1)
        if self.use_face_contour:
            lmk_idx_and_bcoords = find_dynamic_lmk_idx_and_bcoords(
                vertices, full_pose, self.dynamic_lmk_faces_idx,
                self.dynamic_lmk_bary_coords,
                self.neck_kin_chain,
                pose2rot=False,
            )
            dyn_lmk_faces_idx, dyn_lmk_bary_coords = lmk_idx_and_bcoords
            lmk_faces_idx = torch.cat([lmk_faces_idx,
                                       dyn_lmk_faces_idx], 1)
            lmk_bary_coords = torch.cat(
                [lmk_bary_coords.expand(batch_size, -1, -1),
                 dyn_lmk_bary_coords], 1)

        landmarks = vertices2landmarks(vertices, self.faces_tensor,
                                       lmk_faces_idx,
                                       lmk_bary_coords)

        # Add any extra joints that might be needed
        joints = self.vertex_joint_selector(vertices, joints)
        # Add the landmarks to the joints
        joints = torch.cat([joints, landmarks], dim=1)

        # Map the joints to the current dataset
        if self.joint_mapper is not None:
            joints = self.joint_mapper(joints=joints, vertices=vertices)

        joints += transl.unsqueeze(dim=1)
        vertices += transl.unsqueeze(dim=1)

        output = FLAMEOutput(vertices=vertices if return_verts else None,
                             joints=joints,
                             betas=betas,
                             expression=expression,
                             global_orient=global_orient,
                             neck_pose=neck_pose,
                             jaw_pose=jaw_pose,
                             full_pose=full_pose if return_full_pose else None)
        return output


def build_layer(
    model_path: str,
    model_type: str = 'smpl',
    **kwargs
) -> Union[SMPLLayer, SMPLHLayer, SMPLXLayer, MANOLayer, FLAMELayer]:
    ''' Method for creating a model from a path and a model type

        Parameters
        ----------
        model_path: str
            Either the path to the model you wish to load or a folder,
            where each subfolder contains the differents types, i.e.:
            model_path:
            |
            |-- smpl
                |-- SMPL_FEMALE
                |-- SMPL_NEUTRAL
                |-- SMPL_MALE
            |-- smplh
                |-- SMPLH_FEMALE
                |-- SMPLH_MALE
            |-- smplx
                |-- SMPLX_FEMALE
                |-- SMPLX_NEUTRAL
                |-- SMPLX_MALE
            |-- mano
                |-- MANO RIGHT
                |-- MANO LEFT
            |-- flame
                |-- FLAME_FEMALE
                |-- FLAME_MALE
                |-- FLAME_NEUTRAL

        model_type: str, optional
            When model_path is a folder, then this parameter specifies  the
            type of model to be loaded
        **kwargs: dict
            Keyword arguments

        Returns
        -------
            body_model: nn.Module
                The PyTorch module that implements the corresponding body model
        Raises
        ------
            ValueError: In case the model type is not one of SMPL, SMPLH,
            SMPLX, MANO or FLAME
    '''

    if osp.isdir(model_path):
        model_path = os.path.join(model_path, model_type)
    else:
        model_type = osp.basename(model_path).split('_')[0].lower()

    if model_type.lower() == 'smpl':
        return SMPLLayer(model_path, **kwargs)
    elif model_type.lower() == 'smplh':
        return SMPLHLayer(model_path, **kwargs)
    elif model_type.lower() == 'smplx':
        return SMPLXLayer(model_path, **kwargs)
    elif 'mano' in model_type.lower():
        return MANOLayer(model_path, **kwargs)
    elif 'flame' in model_type.lower():
        return FLAMELayer(model_path, **kwargs)
    else:
        raise ValueError(f'Unknown model type {model_type}, exiting!')


def create(
    model_path: str,
    model_type: str = 'smpl',
    **kwargs
) -> Union[SMPL, SMPLH, SMPLX, MANO, FLAME]:
    ''' Method for creating a model from a path and a model type

        Parameters
        ----------
        model_path: str
            Either the path to the model you wish to load or a folder,
            where each subfolder contains the differents types, i.e.:
            model_path:
            |
            |-- smpl
                |-- SMPL_FEMALE
                |-- SMPL_NEUTRAL
                |-- SMPL_MALE
            |-- smplh
                |-- SMPLH_FEMALE
                |-- SMPLH_MALE
            |-- smplx
                |-- SMPLX_FEMALE
                |-- SMPLX_NEUTRAL
                |-- SMPLX_MALE
            |-- mano
                |-- MANO RIGHT
                |-- MANO LEFT

        model_type: str, optional
            When model_path is a folder, then this parameter specifies  the
            type of model to be loaded
        **kwargs: dict
            Keyword arguments

        Returns
        -------
            body_model: nn.Module
                The PyTorch module that implements the corresponding body model
        Raises
        ------
            ValueError: In case the model type is not one of SMPL, SMPLH,
            SMPLX, MANO or FLAME
    '''

    # If it's a folder, assume
    if osp.isdir(model_path):
        model_path = os.path.join(model_path, model_type)
    else:
        model_type = osp.basename(model_path).split('_')[0].lower()

    if model_type.lower() == 'smpl':
        return SMPL(model_path, **kwargs)
    elif model_type.lower() == 'smplh':
        return SMPLH(model_path, **kwargs)
    elif model_type.lower() == 'smplx':
        return SMPLX(model_path, **kwargs)
    elif 'mano' in model_type.lower():
        return MANO(model_path, **kwargs)
    elif 'flame' in model_type.lower():
        return FLAME(model_path, **kwargs)
    else:
        raise ValueError(f'Unknown model type {model_type}, exiting!')


D:\Projects\smplify-x\smplx\joint_names.py
# -*- coding: utf-8 -*-

# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is
# holder of all proprietary rights on this computer program.
# You can only use this computer program if you have closed
# a license agreement with MPG or you get the right to use the computer
# program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and
# liable to prosecution.
#
# Copyright©2019 Max-Planck-Gesellschaft zur Förderung
# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute
# for Intelligent Systems. All rights reserved.
#
# Contact: ps-license@tuebingen.mpg.de

import numpy as np

JOINT_NAMES = [
    "pelvis",
    "left_hip",
    "right_hip",
    "spine1",
    "left_knee",
    "right_knee",
    "spine2",
    "left_ankle",
    "right_ankle",
    "spine3",
    "left_foot",
    "right_foot",
    "neck",
    "left_collar",
    "right_collar",
    "head",
    "left_shoulder",
    "right_shoulder",
    "left_elbow",
    "right_elbow",
    "left_wrist",
    "right_wrist",
    "jaw",
    "left_eye_smplhf",
    "right_eye_smplhf",
    "left_index1",
    "left_index2",
    "left_index3",
    "left_middle1",
    "left_middle2",
    "left_middle3",
    "left_pinky1",
    "left_pinky2",
    "left_pinky3",
    "left_ring1",
    "left_ring2",
    "left_ring3",
    "left_thumb1",
    "left_thumb2",
    "left_thumb3",
    "right_index1",
    "right_index2",
    "right_index3",
    "right_middle1",
    "right_middle2",
    "right_middle3",
    "right_pinky1",
    "right_pinky2",
    "right_pinky3",
    "right_ring1",
    "right_ring2",
    "right_ring3",
    "right_thumb1",
    "right_thumb2",
    "right_thumb3",
    "nose",
    "right_eye",
    "left_eye",
    "right_ear",
    "left_ear",
    "left_big_toe",
    "left_small_toe",
    "left_heel",
    "right_big_toe",
    "right_small_toe",
    "right_heel",
    "left_thumb",
    "left_index",
    "left_middle",
    "left_ring",
    "left_pinky",
    "right_thumb",
    "right_index",
    "right_middle",
    "right_ring",
    "right_pinky",
    "right_eye_brow1",
    "right_eye_brow2",
    "right_eye_brow3",
    "right_eye_brow4",
    "right_eye_brow5",
    "left_eye_brow5",
    "left_eye_brow4",
    "left_eye_brow3",
    "left_eye_brow2",
    "left_eye_brow1",
    "nose1",
    "nose2",
    "nose3",
    "nose4",
    "right_nose_2",
    "right_nose_1",
    "nose_middle",
    "left_nose_1",
    "left_nose_2",
    "right_eye1",
    "right_eye2",
    "right_eye3",
    "right_eye4",
    "right_eye5",
    "right_eye6",
    "left_eye4",
    "left_eye3",
    "left_eye2",
    "left_eye1",
    "left_eye6",
    "left_eye5",
    "right_mouth_1",
    "right_mouth_2",
    "right_mouth_3",
    "mouth_top",
    "left_mouth_3",
    "left_mouth_2",
    "left_mouth_1",
    "left_mouth_5",  # 59 in OpenPose output
    "left_mouth_4",  # 58 in OpenPose output
    "mouth_bottom",
    "right_mouth_4",
    "right_mouth_5",
    "right_lip_1",
    "right_lip_2",
    "lip_top",
    "left_lip_2",
    "left_lip_1",
    "left_lip_3",
    "lip_bottom",
    "right_lip_3",
    # Face contour
    "right_contour_1",
    "right_contour_2",
    "right_contour_3",
    "right_contour_4",
    "right_contour_5",
    "right_contour_6",
    "right_contour_7",
    "right_contour_8",
    "contour_middle",
    "left_contour_8",
    "left_contour_7",
    "left_contour_6",
    "left_contour_5",
    "left_contour_4",
    "left_contour_3",
    "left_contour_2",
    "left_contour_1",
]


SMPLH_JOINT_NAMES = [
    "pelvis",
    "left_hip",
    "right_hip",
    "spine1",
    "left_knee",
    "right_knee",
    "spine2",
    "left_ankle",
    "right_ankle",
    "spine3",
    "left_foot",
    "right_foot",
    "neck",
    "left_collar",
    "right_collar",
    "head",
    "left_shoulder",
    "right_shoulder",
    "left_elbow",
    "right_elbow",
    "left_wrist",
    "right_wrist",
    "left_index1",
    "left_index2",
    "left_index3",
    "left_middle1",
    "left_middle2",
    "left_middle3",
    "left_pinky1",
    "left_pinky2",
    "left_pinky3",
    "left_ring1",
    "left_ring2",
    "left_ring3",
    "left_thumb1",
    "left_thumb2",
    "left_thumb3",
    "right_index1",
    "right_index2",
    "right_index3",
    "right_middle1",
    "right_middle2",
    "right_middle3",
    "right_pinky1",
    "right_pinky2",
    "right_pinky3",
    "right_ring1",
    "right_ring2",
    "right_ring3",
    "right_thumb1",
    "right_thumb2",
    "right_thumb3",
    "nose",
    "right_eye",
    "left_eye",
    "right_ear",
    "left_ear",
    "left_big_toe",
    "left_small_toe",
    "left_heel",
    "right_big_toe",
    "right_small_toe",
    "right_heel",
    "left_thumb",
    "left_index",
    "left_middle",
    "left_ring",
    "left_pinky",
    "right_thumb",
    "right_index",
    "right_middle",
    "right_ring",
    "right_pinky",
]

SMPL_JOINT_NAMES = [
    "pelvis",
    "left_hip",
    "right_hip",
    "spine1",
    "left_knee",
    "right_knee",
    "spine2",
    "left_ankle",
    "right_ankle",
    "spine3",
    "left_foot",
    "right_foot",
    "neck",
    "left_collar",
    "right_collar",
    "head",
    "left_shoulder",
    "right_shoulder",
    "left_elbow",
    "right_elbow",
    "left_wrist",
    "right_wrist",
    "left_hand",
    "right_hand",
]


class Body:
    """
    Class for storing a single body pose.
    """

    def __init__(self, joints, joint_names):
        assert joints.ndim > 1
        assert joints.shape[0] == len(joint_names)
        self.joints = {}
        for i, j in enumerate(joint_names):
            self.joints[j] = joints[i]

    @staticmethod
    def from_smpl(joints):
        """
        Create a Body object from SMPL joints.
        """
        return Body(joints, SMPL_JOINT_NAMES)

    @staticmethod
    def from_smplh(joints):
        """
        Create a Body object from SMPLH joints.
        """
        return Body(joints, SMPLH_JOINT_NAMES)

    def _as(self, joint_names):
        """
        Return a Body object with the specified joint names.
        """
        joint_list = []
        for j in joint_names:
            if j not in self.joints:
                joint_list.append(np.zeros_like(self.joints["spine1"]))
            else:
                joint_list.append(self.joints[j])
        return np.stack(joint_list, axis=0)

    def as_smpl(self):
        """
        Convert the body to SMPL joints.
        """
        return self._as(SMPL_JOINT_NAMES)

    def as_smplh(self):
        """
        Convert the body to SMPLH joints.
        """
        return self._as(SMPLH_JOINT_NAMES)


D:\Projects\smplify-x\smplx\lbs.py
# -*- coding: utf-8 -*-

# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is
# holder of all proprietary rights on this computer program.
# You can only use this computer program if you have closed
# a license agreement with MPG or you get the right to use the computer
# program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and
# liable to prosecution.
#
# Copyright©2019 Max-Planck-Gesellschaft zur Förderung
# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute
# for Intelligent Systems. All rights reserved.
#
# Contact: ps-license@tuebingen.mpg.de

from __future__ import absolute_import
from __future__ import print_function
from __future__ import division

from typing import Tuple, List
import numpy as np

import torch
import torch.nn.functional as F

from .utils import rot_mat_to_euler, Tensor


def find_dynamic_lmk_idx_and_bcoords(
    vertices: Tensor,
    pose: Tensor,
    dynamic_lmk_faces_idx: Tensor,
    dynamic_lmk_b_coords: Tensor,
    neck_kin_chain: List[int],
    pose2rot: bool = True,
) -> Tuple[Tensor, Tensor]:
    ''' Compute the faces, barycentric coordinates for the dynamic landmarks


        To do so, we first compute the rotation of the neck around the y-axis
        and then use a pre-computed look-up table to find the faces and the
        barycentric coordinates that will be used.

        Special thanks to Soubhik Sanyal (soubhik.sanyal@tuebingen.mpg.de)
        for providing the original TensorFlow implementation and for the LUT.

        Parameters
        ----------
        vertices: torch.tensor BxVx3, dtype = torch.float32
            The tensor of input vertices
        pose: torch.tensor Bx(Jx3), dtype = torch.float32
            The current pose of the body model
        dynamic_lmk_faces_idx: torch.tensor L, dtype = torch.long
            The look-up table from neck rotation to faces
        dynamic_lmk_b_coords: torch.tensor Lx3, dtype = torch.float32
            The look-up table from neck rotation to barycentric coordinates
        neck_kin_chain: list
            A python list that contains the indices of the joints that form the
            kinematic chain of the neck.
        dtype: torch.dtype, optional

        Returns
        -------
        dyn_lmk_faces_idx: torch.tensor, dtype = torch.long
            A tensor of size BxL that contains the indices of the faces that
            will be used to compute the current dynamic landmarks.
        dyn_lmk_b_coords: torch.tensor, dtype = torch.float32
            A tensor of size BxL that contains the indices of the faces that
            will be used to compute the current dynamic landmarks.
    '''

    dtype = vertices.dtype
    batch_size = vertices.shape[0]

    if pose2rot:
        aa_pose = torch.index_select(pose.view(batch_size, -1, 3), 1,
                                     neck_kin_chain)
        rot_mats = batch_rodrigues(
            aa_pose.view(-1, 3)).view(batch_size, -1, 3, 3)
    else:
        rot_mats = torch.index_select(
            pose.view(batch_size, -1, 3, 3), 1, neck_kin_chain)

    rel_rot_mat = torch.eye(
        3, device=vertices.device, dtype=dtype).unsqueeze_(dim=0).repeat(
            batch_size, 1, 1)
    for idx in range(len(neck_kin_chain)):
        rel_rot_mat = torch.bmm(rot_mats[:, idx], rel_rot_mat)

    y_rot_angle = torch.round(
        torch.clamp(-rot_mat_to_euler(rel_rot_mat) * 180.0 / np.pi,
                    max=39)).to(dtype=torch.long)
    neg_mask = y_rot_angle.lt(0).to(dtype=torch.long)
    mask = y_rot_angle.lt(-39).to(dtype=torch.long)
    neg_vals = mask * 78 + (1 - mask) * (39 - y_rot_angle)
    y_rot_angle = (neg_mask * neg_vals +
                   (1 - neg_mask) * y_rot_angle)

    dyn_lmk_faces_idx = torch.index_select(dynamic_lmk_faces_idx,
                                           0, y_rot_angle)
    dyn_lmk_b_coords = torch.index_select(dynamic_lmk_b_coords,
                                          0, y_rot_angle)

    return dyn_lmk_faces_idx, dyn_lmk_b_coords


def vertices2landmarks(
    vertices: Tensor,
    faces: Tensor,
    lmk_faces_idx: Tensor,
    lmk_bary_coords: Tensor
) -> Tensor:
    ''' Calculates landmarks by barycentric interpolation

        Parameters
        ----------
        vertices: torch.tensor BxVx3, dtype = torch.float32
            The tensor of input vertices
        faces: torch.tensor Fx3, dtype = torch.long
            The faces of the mesh
        lmk_faces_idx: torch.tensor L, dtype = torch.long
            The tensor with the indices of the faces used to calculate the
            landmarks.
        lmk_bary_coords: torch.tensor Lx3, dtype = torch.float32
            The tensor of barycentric coordinates that are used to interpolate
            the landmarks

        Returns
        -------
        landmarks: torch.tensor BxLx3, dtype = torch.float32
            The coordinates of the landmarks for each mesh in the batch
    '''
    # Extract the indices of the vertices for each face
    # BxLx3
    batch_size, num_verts = vertices.shape[:2]
    device = vertices.device

    lmk_faces = torch.index_select(faces, 0, lmk_faces_idx.view(-1).to(torch.long)).view(
        batch_size, -1, 3)
                        #The '.to(torch.long)'.
                        # added to make the trace work in c++,
                        # otherwise you get a runtime error in c++:
                        # 'index_select(): Expected dtype int32 or int64 for index'

    lmk_faces += torch.arange(
        batch_size, dtype=torch.long, device=device).view(-1, 1, 1) * num_verts

    lmk_vertices = vertices.view(-1, 3)[lmk_faces].view(
        batch_size, -1, 3, 3)

    landmarks = torch.einsum('blfi,blf->bli', [lmk_vertices, lmk_bary_coords])
    return landmarks


def lbs(
    betas: Tensor,
    pose: Tensor,
    v_template: Tensor,
    shapedirs: Tensor,
    posedirs: Tensor,
    J_regressor: Tensor,
    parents: Tensor,
    lbs_weights: Tensor,
    pose2rot: bool = True,
) -> Tuple[Tensor, Tensor]:
    ''' Performs Linear Blend Skinning with the given shape and pose parameters

        Parameters
        ----------
        betas : torch.tensor BxNB
            The tensor of shape parameters
        pose : torch.tensor Bx(J + 1) * 3
            The pose parameters in axis-angle format
        v_template torch.tensor BxVx3
            The template mesh that will be deformed
        shapedirs : torch.tensor 1xNB
            The tensor of PCA shape displacements
        posedirs : torch.tensor Px(V * 3)
            The pose PCA coefficients
        J_regressor : torch.tensor JxV
            The regressor array that is used to calculate the joints from
            the position of the vertices
        parents: torch.tensor J
            The array that describes the kinematic tree for the model
        lbs_weights: torch.tensor N x V x (J + 1)
            The linear blend skinning weights that represent how much the
            rotation matrix of each part affects each vertex
        pose2rot: bool, optional
            Flag on whether to convert the input pose tensor to rotation
            matrices. The default value is True. If False, then the pose tensor
            should already contain rotation matrices and have a size of
            Bx(J + 1)x9
        dtype: torch.dtype, optional

        Returns
        -------
        verts: torch.tensor BxVx3
            The vertices of the mesh after applying the shape and pose
            displacements.
        joints: torch.tensor BxJx3
            The joints of the model
    '''

    batch_size = max(betas.shape[0], pose.shape[0])
    device, dtype = betas.device, betas.dtype

    # Add shape contribution
    v_shaped = v_template + blend_shapes(betas, shapedirs)

    # Get the joints
    # NxJx3 array
    J = vertices2joints(J_regressor, v_shaped)

    # 3. Add pose blend shapes
    # N x J x 3 x 3
    ident = torch.eye(3, dtype=dtype, device=device)
    if pose2rot:
        rot_mats = batch_rodrigues(pose.view(-1, 3)).view(
            [batch_size, -1, 3, 3])

        pose_feature = (rot_mats[:, 1:, :, :] - ident).view([batch_size, -1])
        # (N x P) x (P, V * 3) -> N x V x 3
        pose_offsets = torch.matmul(
            pose_feature, posedirs).view(batch_size, -1, 3)
    else:
        pose_feature = pose[:, 1:].view(batch_size, -1, 3, 3) - ident
        rot_mats = pose.view(batch_size, -1, 3, 3)

        pose_offsets = torch.matmul(pose_feature.view(batch_size, -1),
                                    posedirs).view(batch_size, -1, 3)

    v_posed = pose_offsets + v_shaped
    # 4. Get the global joint location
    J_transformed, A = batch_rigid_transform(rot_mats, J, parents, dtype=dtype)

    # 5. Do skinning:
    # W is N x V x (J + 1)
    W = lbs_weights.unsqueeze(dim=0).expand([batch_size, -1, -1])
    # (N x V x (J + 1)) x (N x (J + 1) x 16)
    num_joints = J_regressor.shape[0]
    T = torch.matmul(W, A.view(batch_size, num_joints, 16)) \
        .view(batch_size, -1, 4, 4)

    homogen_coord = torch.ones([batch_size, v_posed.shape[1], 1],
                               dtype=dtype, device=device)
    v_posed_homo = torch.cat([v_posed, homogen_coord], dim=2)
    v_homo = torch.matmul(T, torch.unsqueeze(v_posed_homo, dim=-1))

    verts = v_homo[:, :, :3, 0]

    return verts, J_transformed


def vertices2joints(J_regressor: Tensor, vertices: Tensor) -> Tensor:
    ''' Calculates the 3D joint locations from the vertices

    Parameters
    ----------
    J_regressor : torch.tensor JxV
        The regressor array that is used to calculate the joints from the
        position of the vertices
    vertices : torch.tensor BxVx3
        The tensor of mesh vertices

    Returns
    -------
    torch.tensor BxJx3
        The location of the joints
    '''

    return torch.einsum('bik,ji->bjk', [vertices, J_regressor])


def blend_shapes(betas: Tensor, shape_disps: Tensor) -> Tensor:
    ''' Calculates the per vertex displacement due to the blend shapes


    Parameters
    ----------
    betas : torch.tensor Bx(num_betas)
        Blend shape coefficients
    shape_disps: torch.tensor Vx3x(num_betas)
        Blend shapes

    Returns
    -------
    torch.tensor BxVx3
        The per-vertex displacement due to shape deformation
    '''

    # Displacement[b, m, k] = sum_{l} betas[b, l] * shape_disps[m, k, l]
    # i.e. Multiply each shape displacement by its corresponding beta and
    # then sum them.
    blend_shape = torch.einsum('bl,mkl->bmk', [betas, shape_disps])
    return blend_shape


def batch_rodrigues(
    rot_vecs: Tensor,
    epsilon: float = 1e-8,
) -> Tensor:
    ''' Calculates the rotation matrices for a batch of rotation vectors
        Parameters
        ----------
        rot_vecs: torch.tensor Nx3
            array of N axis-angle vectors
        Returns
        -------
        R: torch.tensor Nx3x3
            The rotation matrices for the given axis-angle parameters
    '''

    batch_size = rot_vecs.shape[0]
    device, dtype = rot_vecs.device, rot_vecs.dtype

    angle = torch.norm(rot_vecs + 1e-8, dim=1, keepdim=True)
    rot_dir = rot_vecs / angle

    cos = torch.unsqueeze(torch.cos(angle), dim=1)
    sin = torch.unsqueeze(torch.sin(angle), dim=1)

    # Bx1 arrays
    rx, ry, rz = torch.split(rot_dir, 1, dim=1)
    K = torch.zeros((batch_size, 3, 3), dtype=dtype, device=device)

    zeros = torch.zeros((batch_size, 1), dtype=dtype, device=device)
    K = torch.cat([zeros, -rz, ry, rz, zeros, -rx, -ry, rx, zeros], dim=1) \
        .view((batch_size, 3, 3))

    ident = torch.eye(3, dtype=dtype, device=device).unsqueeze(dim=0)
    rot_mat = ident + sin * K + (1 - cos) * torch.bmm(K, K)
    return rot_mat


def transform_mat(R: Tensor, t: Tensor) -> Tensor:
    ''' Creates a batch of transformation matrices
        Args:
            - R: Bx3x3 array of a batch of rotation matrices
            - t: Bx3x1 array of a batch of translation vectors
        Returns:
            - T: Bx4x4 Transformation matrix
    '''
    # No padding left or right, only add an extra row
    return torch.cat([F.pad(R, [0, 0, 0, 1]),
                      F.pad(t, [0, 0, 0, 1], value=1)], dim=2)


def batch_rigid_transform(
    rot_mats: Tensor,
    joints: Tensor,
    parents: Tensor,
    dtype=torch.float32
) -> Tensor:
    """
    Applies a batch of rigid transformations to the joints

    Parameters
    ----------
    rot_mats : torch.tensor BxNx3x3
        Tensor of rotation matrices
    joints : torch.tensor BxNx3
        Locations of joints
    parents : torch.tensor BxN
        The kinematic tree of each object
    dtype : torch.dtype, optional:
        The data type of the created tensors, the default is torch.float32

    Returns
    -------
    posed_joints : torch.tensor BxNx3
        The locations of the joints after applying the pose rotations
    rel_transforms : torch.tensor BxNx4x4
        The relative (with respect to the root joint) rigid transformations
        for all the joints
    """

    joints = torch.unsqueeze(joints, dim=-1)

    rel_joints = joints.clone()
    rel_joints[:, 1:] -= joints[:, parents[1:]]

    transforms_mat = transform_mat(
        rot_mats.reshape(-1, 3, 3),
        rel_joints.reshape(-1, 3, 1)).reshape(-1, joints.shape[1], 4, 4)

    transform_chain = [transforms_mat[:, 0]]
    for i in range(1, parents.shape[0]):
        # Subtract the joint location at the rest pose
        # No need for rotation, since it's identity when at rest
        curr_res = torch.matmul(transform_chain[parents[i]],
                                transforms_mat[:, i])
        transform_chain.append(curr_res)

    transforms = torch.stack(transform_chain, dim=1)

    # The last column of the transformations contains the posed joints
    posed_joints = transforms[:, :, :3, 3]

    joints_homogen = F.pad(joints, [0, 0, 0, 1])

    rel_transforms = transforms - F.pad(
        torch.matmul(transforms, joints_homogen), [3, 0, 0, 0, 0, 0, 0, 0])

    return posed_joints, rel_transforms


D:\Projects\smplify-x\smplx\utils.py
# -*- coding: utf-8 -*-

# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is
# holder of all proprietary rights on this computer program.
# You can only use this computer program if you have closed
# a license agreement with MPG or you get the right to use the computer
# program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and
# liable to prosecution.
#
# Copyright©2019 Max-Planck-Gesellschaft zur Förderung
# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute
# for Intelligent Systems. All rights reserved.
#
# Contact: ps-license@tuebingen.mpg.de

from typing import NewType, Union, Optional
from dataclasses import dataclass, asdict, fields
import numpy as np
import torch

Tensor = NewType('Tensor', torch.Tensor)
Array = NewType('Array', np.ndarray)


@dataclass
class ModelOutput:
    vertices: Optional[Tensor] = None
    joints: Optional[Tensor] = None
    full_pose: Optional[Tensor] = None
    global_orient: Optional[Tensor] = None
    transl: Optional[Tensor] = None
    v_shaped: Optional[Tensor] = None

    def __getitem__(self, key):
        return getattr(self, key)

    def get(self, key, default=None):
        return getattr(self, key, default)

    def __iter__(self):
        return self.keys()

    def keys(self):
        keys = [t.name for t in fields(self)]
        return iter(keys)

    def values(self):
        values = [getattr(self, t.name) for t in fields(self)]
        return iter(values)

    def items(self):
        data = [(t.name, getattr(self, t.name)) for t in fields(self)]
        return iter(data)


@dataclass
class SMPLOutput(ModelOutput):
    betas: Optional[Tensor] = None
    body_pose: Optional[Tensor] = None


@dataclass
class SMPLHOutput(SMPLOutput):
    left_hand_pose: Optional[Tensor] = None
    right_hand_pose: Optional[Tensor] = None
    transl: Optional[Tensor] = None


@dataclass
class SMPLXOutput(SMPLHOutput):
    expression: Optional[Tensor] = None
    jaw_pose: Optional[Tensor] = None


@dataclass
class MANOOutput(ModelOutput):
    betas: Optional[Tensor] = None
    hand_pose: Optional[Tensor] = None


@dataclass
class FLAMEOutput(ModelOutput):
    betas: Optional[Tensor] = None
    expression: Optional[Tensor] = None
    jaw_pose: Optional[Tensor] = None
    neck_pose: Optional[Tensor] = None


def find_joint_kin_chain(joint_id, kinematic_tree):
    kin_chain = []
    curr_idx = joint_id
    while curr_idx != -1:
        kin_chain.append(curr_idx)
        curr_idx = kinematic_tree[curr_idx]
    return kin_chain


def to_tensor(
        array: Union[Array, Tensor], dtype=torch.float32
) -> Tensor:
    if torch.is_tensor(array):
        return array
    else:
        return torch.tensor(array, dtype=dtype)


class Struct(object):
    def __init__(self, **kwargs):
        for key, val in kwargs.items():
            setattr(self, key, val)


def to_np(array, dtype=np.float32):
    if 'scipy.sparse' in str(type(array)):
        array = array.todense()
    return np.array(array, dtype=dtype)


def rot_mat_to_euler(rot_mats):
    # Calculates rotation matrix to euler angles
    # Careful for extreme cases of eular angles like [0.0, pi, 0.0]

    sy = torch.sqrt(rot_mats[:, 0, 0] * rot_mats[:, 0, 0] +
                    rot_mats[:, 1, 0] * rot_mats[:, 1, 0])
    return torch.atan2(-rot_mats[:, 2, 0], sy)


D:\Projects\smplify-x\smplx\vertex_ids.py
# -*- coding: utf-8 -*-

# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is
# holder of all proprietary rights on this computer program.
# You can only use this computer program if you have closed
# a license agreement with MPG or you get the right to use the computer
# program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and
# liable to prosecution.
#
# Copyright©2019 Max-Planck-Gesellschaft zur Förderung
# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute
# for Intelligent Systems. All rights reserved.
#
# Contact: ps-license@tuebingen.mpg.de

from __future__ import print_function
from __future__ import absolute_import
from __future__ import division

# Joint name to vertex mapping. SMPL/SMPL-H/SMPL-X vertices that correspond to
# MSCOCO and OpenPose joints
vertex_ids = {
    'smplh': {
        'nose':		    332,
        'reye':		    6260,
        'leye':		    2800,
        'rear':		    4071,
        'lear':		    583,
        'rthumb':		6191,
        'rindex':		5782,
        'rmiddle':		5905,
        'rring':		6016,
        'rpinky':		6133,
        'lthumb':		2746,
        'lindex':		2319,
        'lmiddle':		2445,
        'lring':		2556,
        'lpinky':		2673,
        'LBigToe':		3216,
        'LSmallToe':	3226,
        'LHeel':		3387,
        'RBigToe':		6617,
        'RSmallToe':    6624,
        'RHeel':		6787
    },
    'smplx': {
        'nose':		    9120,
        'reye':		    9929,
        'leye':		    9448,
        'rear':		    616,
        'lear':		    6,
        'rthumb':		8079,
        'rindex':		7669,
        'rmiddle':		7794,
        'rring':		7905,
        'rpinky':		8022,
        'lthumb':		5361,
        'lindex':		4933,
        'lmiddle':		5058,
        'lring':		5169,
        'lpinky':		5286,
        'LBigToe':		5770,
        'LSmallToe':    5780,
        'LHeel':		8846,
        'RBigToe':		8463,
        'RSmallToe': 	8474,
        'RHeel':  		8635
    },
    'mano': {
            'thumb':		744,
            'index':		320,
            'middle':		443,
            'ring':		    554,
            'pinky':		671,
        }
}


D:\Projects\smplify-x\smplx\vertex_joint_selector.py
# -*- coding: utf-8 -*-

# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is
# holder of all proprietary rights on this computer program.
# You can only use this computer program if you have closed
# a license agreement with MPG or you get the right to use the computer
# program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and
# liable to prosecution.
#
# Copyright©2019 Max-Planck-Gesellschaft zur Förderung
# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute
# for Intelligent Systems. All rights reserved.
#
# Contact: ps-license@tuebingen.mpg.de

from __future__ import absolute_import
from __future__ import print_function
from __future__ import division

import numpy as np

import torch
import torch.nn as nn

from .utils import to_tensor


class VertexJointSelector(nn.Module):

    def __init__(self, vertex_ids=None,
                 use_hands=True,
                 use_feet_keypoints=True, **kwargs):
        super(VertexJointSelector, self).__init__()

        extra_joints_idxs = []

        face_keyp_idxs = np.array([
            vertex_ids['nose'],
            vertex_ids['reye'],
            vertex_ids['leye'],
            vertex_ids['rear'],
            vertex_ids['lear']], dtype=np.int64)

        extra_joints_idxs = np.concatenate([extra_joints_idxs,
                                            face_keyp_idxs])

        if use_feet_keypoints:
            feet_keyp_idxs = np.array([vertex_ids['LBigToe'],
                                       vertex_ids['LSmallToe'],
                                       vertex_ids['LHeel'],
                                       vertex_ids['RBigToe'],
                                       vertex_ids['RSmallToe'],
                                       vertex_ids['RHeel']], dtype=np.int32)

            extra_joints_idxs = np.concatenate(
                [extra_joints_idxs, feet_keyp_idxs])

        if use_hands:
            self.tip_names = ['thumb', 'index', 'middle', 'ring', 'pinky']

            tips_idxs = []
            for hand_id in ['l', 'r']:
                for tip_name in self.tip_names:
                    tips_idxs.append(vertex_ids[hand_id + tip_name])

            extra_joints_idxs = np.concatenate(
                [extra_joints_idxs, tips_idxs])

        self.register_buffer('extra_joints_idxs',
                             to_tensor(extra_joints_idxs, dtype=torch.long))

    def forward(self, vertices, joints):
        extra_joints = torch.index_select(vertices, 1, self.extra_joints_idxs.to(torch.long)) #The '.to(torch.long)'.
                                                                                            # added to make the trace work in c++,
                                                                                            # otherwise you get a runtime error in c++:
                                                                                            # 'index_select(): Expected dtype int32 or int64 for index'
        joints = torch.cat([joints, extra_joints], dim=1)

        return joints


D:\Projects\smplify-x\smplx\__init__.py
# -*- coding: utf-8 -*-

# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is
# holder of all proprietary rights on this computer program.
# You can only use this computer program if you have closed
# a license agreement with MPG or you get the right to use the computer
# program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and
# liable to prosecution.
#
# Copyright©2019 Max-Planck-Gesellschaft zur Förderung
# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute
# for Intelligent Systems. All rights reserved.
#
# Contact: ps-license@tuebingen.mpg.de

from .body_models import (
    create,
    SMPL,
    SMPLH,
    SMPLX,
    MANO,
    FLAME,
    build_layer,
    SMPLLayer,
    SMPLHLayer,
    SMPLXLayer,
    MANOLayer,
    FLAMELayer,
)


D:\Projects\smplify-x\src\human-body-prior\README.md
# VPoser: Variational Human Pose Prior for Body Inverse Kinematics

![alt text](support_data/vposer_samples.png "Novel Human Poses Sampled From the VPoser.")
## Description
The articulated 3D pose of the human body is high-dimensional and complex. 
Many applications make use of a prior distribution over valid human poses, but modeling this distribution is difficult.
Here we present VPoser, a learning based variational human pose prior trained from a large dataset of human poses represented as SMPL bodies.
This body prior can be used as an Inverse Kinematics (IK) solver for many tasks such as fitting a body model to images 
as the main contribution of this repository for [SMPLify-X](https://smpl-x.is.tue.mpg.de/). 
VPoser has the following features: 
 - defines a prior of SMPL pose parameters
 - is end-to-end differentiable
 - provides a way to penalize impossible poses while admitting valid ones
 - effectively models correlations among the joints of the body
 - introduces an efficient, low-dimensional, representation for human pose
 - can be used to generate valid 3D human poses for data-dependent tasks

## Table of Contents
  * [Description](#description)
  * [Installation](#installation)
  * [Tutorials](#tutorials)
  * [Advanced IK Capabilities](#advanced-ik-capabilities)
  * [Train VPoser](#train-vposer)
  * [Citation](#citation)
  * [License](#license)
  * [Acknowledgments](#acknowledgments)
  * [Contact](#contact)
  * [FAQ](https://github.com/nghorbani/human_body_prior/wiki/FAQ)

## Installation
**Requirements**
- Python 3.7
- [PyTorch 1.7.1](https://pytorch.org/get-started)

[comment]: <> (- [Torchgeometry 0.1.2]&#40;https://pypi.org/project/torchgeometry/0.1.2/&#41;)

[comment]: <> (- [Body Visualizer]&#40;https://github.com/nghorbani/body_visualizer&#41; for visualizations)
  

Clone this repo and run the following from the root folder:
```bash
pip install -r requirements.txt
python setup.py develop
```

## Tutorials
![alt text](support_data/latent_interpolation_1.gif "Interpolation of novel poses on the smoother VPoser latent space.")
![alt text](support_data/latent_interpolation_2.gif "Interpolation of novel poses on the smoother VPoser latent space.")

* [VPoser Body poZ Space for SMPL Body Model Family](tutorials/vposer.ipynb)
* [Sampling Novel Body Poses with VPoser](tutorials/vposer_sampling.ipynb)

## Advanced IK Capabilities
![alt text](support_data/SMPL_inverse_kinematics.gif "Batched SMPL Inverse Kinematics With Learned Body Prior")

Given position of some key points one can find the necessary body joints' rotation configurations via inverse kinematics (IK). 
The keypoints could either be 3D (joint locations, 3D mocap markers on body surface) or 2D (as in [SMPLify-X](https://smpl-x.is.tue.mpg.de/)).
We provide a comprehensive IK engine with flexible key point definition interface demonstrated in tutorials: 
- [IK for 3D joints](tutorials/ik_example_joints.py) 
- [IK for mocap markers](tutorials/ik_example_mocap.py) 

One can define keypoints on the SMPL body, e.g. joints, or any locations relative to the body surface 
and fit body model parameters to them while utilizing the efficient learned pose parameterization of 
[VPoser](https://github.com/nghorbani/human_body_prior). The supported features are:
- Batch enabled
- Flexible key point definition
- LBFGS with wolfe line-search and ADAM optimizer already enabled
- No need for initializing the body (always starts from zero)
- Optimizes body pose, translation and body global orientation jointly and iteratively


## Train VPoser
We train VPoser, as a [variational autoencoder](https://arxiv.org/abs/1312.6114)
that learns a latent representation of human pose and regularizes the distribution of the latent code 
to be a normal distribution.
We train our prior on data from the [AMASS](https://amass.is.tue.mpg.de/) dataset, 
that holds the SMPL pose parameters of various publicly available human motion capture datasets.


## Citation
Please cite the following paper if you use this code directly or indirectly in your research/projects:
```
@inproceedings{SMPL-X:2019,
  title = {Expressive Body Capture: 3D Hands, Face, and Body from a Single Image},
  author = {Pavlakos, Georgios and Choutas, Vasileios and Ghorbani, Nima and Bolkart, Timo and Osman, Ahmed A. A. and Tzionas, Dimitrios and Black, Michael J.},
  booktitle = {Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)},
  year = {2019}
}
```
Also note that if you consider training your own VPoser for your research using the AMASS dataset, 
then please follow its respective citation guideline.
 

## Contact
The code in this repository is developed by [Nima Ghorbani](https://nghorbani.github.io/) 
while at [Perceiving Systems](https://ps.is.mpg.de/), Max-Planck Institute for Intelligent Systems, Tübingen, Germany.

If you have any questions you can contact us at [smplx@tuebingen.mpg.de](mailto:smplx@tuebingen.mpg.de).

For commercial licensing, contact [ps-licensing@tue.mpg.de](mailto:ps-licensing@tue.mpg.de)

## License

Software Copyright License for **non-commercial scientific research purposes**.
Please read carefully the [terms and conditions](./LICENSE) and any accompanying documentation before you download and/or use the SMPL-X/SMPLify-X model, data and software, (the "Model & Software"), including 3D meshes, blend weights, blend shapes, textures, software, scripts, and animations. By downloading and/or using the Model & Software (including downloading, cloning, installing, and any other use of this github repository), you acknowledge that you have read these terms and conditions, understand them, and agree to be bound by them. If you do not agree with these terms and conditions, you must not download and/or use the Model & Software. Any infringement of the terms of this agreement will automatically terminate your rights under this [License](./LICENSE).


D:\Projects\smplify-x\src\human-body-prior\setup.py
# -*- coding: utf-8 -*-
#
# Copyright (C) 2019 Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG),
# acting on behalf of its Max Planck Institute for Intelligent Systems and the
# Max Planck Institute for Biological Cybernetics. All rights reserved.
#
# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights
# on this computer program. You can only use this computer program if you have closed a license agreement
# with MPG or you get the right to use the computer program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and liable to prosecution.
# Contact: ps-license@tuebingen.mpg.de
#
#
# If you use this code in a research publication please consider citing the following:
#
# Expressive Body Capture: 3D Hands, Face, and Body from a Single Image <https://arxiv.org/abs/1904.05866>
#
#
# Code Developed by:
# Nima Ghorbani <https://nghorbani.github.io/>
#
# 2019.05.10


from setuptools import setup, find_packages
from glob import glob
setup(name='human_body_prior',
      version='2.2.2.0',
      packages=find_packages('src'),
      package_dir={'': 'src'},
      include_package_data=True,
      data_files=[('human_body_prior/support_data', glob('support_data/*.*'))],


      author='Nima Ghorbani',
      author_email='nghorbani@tue.mpg.de',
      maintainer='Nima Ghorbani',
      maintainer_email='nghorbani@tue.mpg.de',
      url='https://github.com/nghorbani/human_body_prior',
      description='Variational human pose prior for human pose synthesis and estimation.',
      long_description=open("README.md").read(),
      long_description_content_type="text/markdown",
      install_requires=[],
      dependency_links=[],
      classifiers=[
          "Intended Audience :: Research",
          "Natural Language :: English",
          "Operating System :: POSIX",
          "Operating System :: POSIX :: BSD",
          "Operating System :: POSIX :: Linux",
          "Programming Language :: Python",
          "Programming Language :: Python :: 3",
          "Programming Language :: Python :: 3.7", ],
      )


D:\Projects\smplify-x\src\human-body-prior\src\human_body_prior\body_model\body_model.py
# -*- coding: utf-8 -*-
#
# Copyright (C) 2019 Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG),
# acting on behalf of its Max Planck Institute for Intelligent Systems and the
# Max Planck Institute for Biological Cybernetics. All rights reserved.
#
# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights
# on this computer program. You can only use this computer program if you have closed a license agreement
# with MPG or you get the right to use the computer program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and liable to prosecution.
# Contact: ps-license@tuebingen.mpg.de
#
#
# If you use this code in a research publication please consider citing the following:
#
# Expressive Body Capture: 3D Hands, Face, and Body from a Single Image <https://arxiv.org/abs/1904.05866>
#
#
# Code Developed by:
# Nima Ghorbani <https://nghorbani.github.io/>
#
# 2018.12.13

import numpy as np
import torch
import torch.nn as nn

# from smplx.lbs import lbs
from human_body_prior.body_model.lbs import lbs
import sys

class BodyModel(nn.Module):

    def __init__(self,
                 bm_fname,
                 num_betas=10,
                 num_dmpls=None, dmpl_fname=None,
                 num_expressions=None,
                 use_posedirs=True,
                 model_type=None,
                 dtype=torch.float32,
                 persistant_buffer=False):

        super(BodyModel, self).__init__()

        '''
        :param bm_fname: path to a SMPL model as pkl file
        :param num_betas: number of shape parameters to include.
        :param device: default on gpu
        :param dtype: float precision of the computations
        :return: verts, trans, pose, betas 
        '''

        self.dtype = dtype


        # -- Load SMPL params --
        if bm_fname.endswith('.npz'):
            smpl_dict = np.load(bm_fname, encoding='latin1')
        else:
            raise ValueError(f'bm_fname must be a .npz file: {bm_fname}')

        # these are supposed for later convenient look up
        self.num_betas = num_betas
        self.num_dmpls = num_dmpls
        self.num_expressions = num_expressions

        npose_params = smpl_dict['posedirs'].shape[2] // 3
        if model_type:
            self.model_type = model_type
        else:
            self.model_type = {12:'flame', 69: 'smpl', 153: 'smplh', 162: 'smplx', 45: 'mano',
                           105: 'animal_horse', 102: 'animal_dog'}[npose_params]

        assert self.model_type in ['smpl', 'smplh', 'smplx', 'mano', 'mano', 'animal_horse', 'animal_dog', 'flame', 'animal_rat'], ValueError(
            'model_type should be in smpl/smplh/smplx/mano.')

        self.use_dmpl = False
        if num_dmpls is not None:
            if dmpl_fname is not None:
                self.use_dmpl = True
            else:
                raise (ValueError('dmpl_fname should be provided when using dmpls!'))

        if self.use_dmpl and self.model_type in ['smplx', 'mano', 'animal_horse', 'animal_dog']: raise (
            NotImplementedError('DMPLs only work with SMPL/SMPLH models for now.'))

        self.use_expression = self.model_type in ['smplx','flame'] and num_expressions is not None

        # Mean template vertices
        self.comp_register('init_v_template', torch.tensor(smpl_dict['v_template'][None], dtype=dtype), persistent=persistant_buffer)

        self.comp_register('f', torch.tensor(smpl_dict['f'].astype(np.int32), dtype=torch.int32), persistent=persistant_buffer)

        num_total_betas = smpl_dict['shapedirs'].shape[-1]
        if num_betas < 1:
            num_betas = num_total_betas

        shapedirs = smpl_dict['shapedirs'][:, :, :num_betas]
        self.comp_register('shapedirs', torch.tensor(shapedirs, dtype=dtype), persistent=persistant_buffer)

        if self.use_expression:
            if smpl_dict['shapedirs'].shape[-1] > 300:
                begin_shape_id = 300
            else:
                begin_shape_id = 10
                num_expressions = smpl_dict['shapedirs'].shape[-1] - 10

            exprdirs = smpl_dict['shapedirs'][:, :, begin_shape_id:(begin_shape_id + num_expressions)]
            self.comp_register('exprdirs', torch.tensor(exprdirs, dtype=dtype), persistent=persistant_buffer)

            expression = torch.tensor(np.zeros((1, num_expressions)), dtype=dtype)
            self.comp_register('init_expression', expression, persistent=persistant_buffer)

        if self.use_dmpl:
            dmpldirs = np.load(dmpl_fname)['eigvec']

            dmpldirs = dmpldirs[:, :, :num_dmpls]
            self.comp_register('dmpldirs', torch.tensor(dmpldirs, dtype=dtype), persistent=persistant_buffer)

        # Regressor for joint locations given shape - 6890 x 24
        self.comp_register('J_regressor', torch.tensor(smpl_dict['J_regressor'], dtype=dtype), persistent=persistant_buffer)

        # Pose blend shape basis: 6890 x 3 x 207, reshaped to 6890*30 x 207
        if use_posedirs:
            posedirs = smpl_dict['posedirs']
            # print(self.model_type, posedirs.shape)
            posedirs = posedirs.reshape([posedirs.shape[0] * 3, -1]).T
            self.comp_register('posedirs', torch.tensor(posedirs, dtype=dtype), persistent=persistant_buffer)
        else:
            self.posedirs = None

        # indices of parents for each joints
        kintree_table = smpl_dict['kintree_table'].astype(np.int32)
        self.comp_register('kintree_table', torch.tensor(kintree_table, dtype=torch.int32), persistent=persistant_buffer)

        # LBS weights
        # weights = np.repeat(smpl_dict['weights'][np.newaxis], batch_size, axis=0)
        weights = smpl_dict['weights']
        self.comp_register('weights', torch.tensor(weights, dtype=dtype), persistent=persistant_buffer)

        self.comp_register('init_trans', torch.zeros((1,3), dtype=dtype), persistent=persistant_buffer)
        # self.register_parameter('trans', nn.Parameter(trans, requires_grad=True))

        # root_orient
        # if self.model_type in ['smpl', 'smplh']:
        self.comp_register('init_root_orient', torch.zeros((1,3), dtype=dtype), persistent=persistant_buffer)

        # pose_body
        if self.model_type in ['smpl', 'smplh', 'smplx']:
            self.comp_register('init_pose_body', torch.zeros((1,63), dtype=dtype), persistent=persistant_buffer)
        elif self.model_type == 'animal_horse':
            self.comp_register('init_pose_body', torch.zeros((1,105), dtype=dtype), persistent=persistant_buffer)
        elif self.model_type == 'flame':
            self.comp_register('init_pose_body', torch.zeros((1,3), dtype=dtype), persistent=persistant_buffer)
        elif self.model_type in ['animal_dog','animal_rat']:
            self.comp_register('init_pose_body', torch.zeros((1,102), dtype=dtype), persistent=persistant_buffer)

        # pose_hand
        if self.model_type in ['smpl']:
            self.comp_register('init_pose_hand', torch.zeros((1,1*3*2), dtype=dtype), persistent=persistant_buffer)
        elif self.model_type in ['smplh', 'smplx']:
            self.comp_register('init_pose_hand', torch.zeros((1,15*3*2), dtype=dtype), persistent=persistant_buffer)
        elif self.model_type in ['mano']:
            self.comp_register('init_pose_hand', torch.zeros((1,15*3), dtype=dtype), persistent=persistant_buffer)

        # face poses
        if self.model_type in ['smplx','flame']:
            self.comp_register('init_pose_jaw', torch.zeros((1,1*3), dtype=dtype), persistent=persistant_buffer)
            self.comp_register('init_pose_eye', torch.zeros((1,2*3), dtype=dtype), persistent=persistant_buffer)

        self.comp_register('init_betas', torch.zeros((1,num_betas), dtype=dtype), persistent=persistant_buffer)

        if self.use_dmpl:
            self.comp_register('init_dmpls', torch.zeros((1,num_dmpls), dtype=dtype), persistent=persistant_buffer)

    def comp_register(self, name, value, persistent=False):
        if sys.version_info[0] > 2:
            self.register_buffer(name, value, persistent)
        else:
            self.register_buffer(name, value)

    def r(self):
        from human_body_prior.tools.omni_tools import copy2cpu as c2c
        return c2c(self.forward().v)

    def forward(self, root_orient=None, pose_body=None, pose_hand=None, pose_jaw=None, pose_eye=None, betas=None,
                trans=None, dmpls=None, expression=None, v_template =None, joints=None, v_shaped=None, return_dict=False,  **kwargs):
        '''

        :param root_orient: Nx3
        :param pose_body:
        :param pose_hand:
        :param pose_jaw:
        :param pose_eye:
        :param kwargs:
        :return:
        '''
        batch_size = 1
        # compute batchsize by any of the provided variables
        for arg in [root_orient,pose_body,pose_hand,pose_jaw,pose_eye,betas,trans, dmpls,expression, v_template,joints]:
            if arg is not None:
                batch_size = arg.shape[0]
                break

        # assert not (v_template is not None and betas is not None), ValueError('vtemplate and betas could not be used jointly.')
        assert self.model_type in ['smpl', 'smplh', 'smplx', 'mano', 'animal_horse', 'animal_dog', 'flame', 'animal_rat'], ValueError(
            'model_type should be in smpl/smplh/smplx/mano')
        if root_orient is None:  root_orient = self.init_root_orient.expand(batch_size, -1)
        if self.model_type in ['smplh', 'smpl']:
            if pose_body is None:  pose_body = self.init_pose_body.expand(batch_size, -1)
            if pose_hand is None:  pose_hand = self.init_pose_hand.expand(batch_size, -1)
        elif self.model_type == 'smplx':
            if pose_body is None:  pose_body = self.init_pose_body.expand(batch_size, -1)
            if pose_hand is None:  pose_hand = self.init_pose_hand.expand(batch_size, -1)
            if pose_jaw is None:  pose_jaw = self.init_pose_jaw.expand(batch_size, -1)
            if pose_eye is None:  pose_eye = self.init_pose_eye.expand(batch_size, -1)
        elif self.model_type == 'flame':
            if pose_body is None:  pose_body = self.init_pose_body.expand(batch_size, -1)
            if pose_jaw is None:  pose_jaw = self.init_pose_jaw.expand(batch_size, -1)
            if pose_eye is None:  pose_eye = self.init_pose_eye.expand(batch_size, -1)
        elif self.model_type in ['mano',]:
            if pose_hand is None:  pose_hand = self.init_pose_hand.expand(batch_size, -1)
        elif self.model_type in ['animal_horse','animal_dog', 'animal_rat']:
            if pose_body is None:  pose_body = self.init_pose_body.expand(batch_size, -1)

        if pose_hand is None and self.model_type not in ['animal_horse', 'animal_dog', 'animal_rat','flame']:
            pose_hand = self.init_pose_hand.expand(batch_size, -1)

        if trans is None: trans = self.init_trans.expand(batch_size, -1)
        if v_template is None: v_template = self.init_v_template.expand(batch_size, -1,-1)
        if betas is None: betas = self.init_betas.expand(batch_size, -1)

        if self.model_type in ['smplh', 'smpl']:
            full_pose = torch.cat([root_orient, pose_body, pose_hand], dim=-1)
        elif self.model_type == 'smplx':
            full_pose = torch.cat([root_orient, pose_body, pose_jaw, pose_eye, pose_hand], dim=-1)  # orient:3, body:63, jaw:3, eyel:3, eyer:3, handl, handr
        elif self.model_type == 'flame':
            full_pose = torch.cat([root_orient, pose_body, pose_jaw, pose_eye], dim=-1)  # orient:3, body:63, jaw:3, eyel:3, eyer:3, handl, handr
        elif self.model_type in ['mano', ]:
            full_pose = torch.cat([root_orient, pose_hand], dim=-1)
        elif self.model_type in ['animal_horse', 'animal_dog', 'animal_rat']:
            full_pose = torch.cat([root_orient, pose_body], dim=-1)

        if self.use_dmpl:
            if dmpls is None: dmpls = self.init_dmpls.expand(batch_size, -1)
            shape_components = torch.cat([betas, dmpls], dim=-1)
            shapedirs = torch.cat([self.shapedirs, self.dmpldirs], dim=-1)
        elif self.use_expression:
            if expression is None: expression = self.init_expression.expand(batch_size, -1)
            shape_components = torch.cat([betas, expression], dim=-1)
            shapedirs = torch.cat([self.shapedirs, self.exprdirs], dim=-1)
        else:
            shape_components = betas
            shapedirs = self.shapedirs

        verts, Jtr = lbs(betas=shape_components, pose=full_pose, v_template=v_template,
                            shapedirs=shapedirs, posedirs=self.posedirs,
                            J_regressor=self.J_regressor, parents=self.kintree_table[0].long(),
                            lbs_weights=self.weights, joints=joints, v_shaped=v_shaped,
                            dtype=self.dtype)

        Jtr = Jtr + trans.unsqueeze(dim=1)
        verts = verts + trans.unsqueeze(dim=1)

        res = {}
        res['v'] = verts
        res['f'] = self.f
        res['Jtr'] = Jtr  # Todo: ik can be made with vposer
        # res['bStree_table'] = self.kintree_table

        # if self.model_type == 'smpl':
        #     res['pose_body'] = pose_body
        # elif self.model_type == 'smplh':
        #     res['pose_body'] = pose_body
        #     res['pose_hand'] = pose_hand
        # elif self.model_type == 'smplx':
        #     res['pose_body'] = pose_body
        #     res['pose_hand'] = pose_hand
        #     res['pose_jaw'] = pose_jaw
        #     res['pose_eye'] = pose_eye
        # elif self.model_type in ['mano', 'mano']:
        #     res['pose_hand'] = pose_hand
        res['full_pose'] = full_pose

        if not return_dict:
            class result_meta(object):
                pass

            res_class = result_meta()
            for k, v in res.items():
                res_class.__setattr__(k, v)
            res = res_class

        return res




D:\Projects\smplify-x\src\human-body-prior\src\human_body_prior\body_model\lbs.py
# -*- coding: utf-8 -*-
#
# Copyright (C) 2019 Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG),
# acting on behalf of its Max Planck Institute for Intelligent Systems and the
# Max Planck Institute for Biological Cybernetics. All rights reserved.
#
# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights
# on this computer program. You can only use this computer program if you have closed a license agreement
# with MPG or you get the right to use the computer program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and liable to prosecution.
# Contact: ps-license@tuebingen.mpg.de
#
#
# If you use this code in a research publication please consider citing the following:
#
# Expressive Body Capture: 3D Hands, Face, and Body from a Single Image <https://arxiv.org/abs/1904.05866>
#
#
# Code Developed by:
# Vassilis Choutas <https://vchoutas.github.io/>
#

from __future__ import absolute_import
from __future__ import print_function
from __future__ import division

import numpy as np

import torch
import torch.nn.functional as F

def to_tensor(array, dtype=torch.float32):
    if 'torch.tensor' not in str(type(array)):
        return torch.tensor(array, dtype=dtype)


class Struct(object):
    def __init__(self, **kwargs):
        for key, val in kwargs.items():
            setattr(self, key, val)


def to_np(array, dtype=np.float32):
    if 'scipy.sparse' in str(type(array)):
        array = array.todense()
    return np.array(array, dtype=dtype)


def rot_mat_to_euler(rot_mats):
    # Calculates rotation matrix to euler angles
    # Careful for extreme cases of eular angles like [0.0, pi, 0.0]

    sy = torch.sqrt(rot_mats[:, 0, 0] * rot_mats[:, 0, 0] +
                    rot_mats[:, 1, 0] * rot_mats[:, 1, 0])
    return torch.atan2(-rot_mats[:, 2, 0], sy)


def find_dynamic_lmk_idx_and_bcoords(vertices, pose, dynamic_lmk_faces_idx,
                                     dynamic_lmk_b_coords,
                                     neck_kin_chain, dtype=torch.float32):
    ''' Compute the faces, barycentric coordinates for the dynamic landmarks


        To do so, we first compute the rotation of the neck around the y-axis
        and then use a pre-computed look-up table to find the faces and the
        barycentric coordinates that will be used.

        Special thanks to Soubhik Sanyal (soubhik.sanyal@tuebingen.mpg.de)
        for providing the original TensorFlow implementation and for the LUT.

        Parameters
        ----------
        vertices: torch.tensor BxVx3, dtype = torch.float32
            The tensor of input vertices
        pose: torch.tensor Bx(Jx3), dtype = torch.float32
            The current pose of the body model
        dynamic_lmk_faces_idx: torch.tensor L, dtype = torch.long
            The look-up table from neck rotation to faces
        dynamic_lmk_b_coords: torch.tensor Lx3, dtype = torch.float32
            The look-up table from neck rotation to barycentric coordinates
        neck_kin_chain: list
            A python list that contains the indices of the joints that form the
            kinematic chain of the neck.
        dtype: torch.dtype, optional

        Returns
        -------
        dyn_lmk_faces_idx: torch.tensor, dtype = torch.long
            A tensor of size BxL that contains the indices of the faces that
            will be used to compute the current dynamic landmarks.
        dyn_lmk_b_coords: torch.tensor, dtype = torch.float32
            A tensor of size BxL that contains the indices of the faces that
            will be used to compute the current dynamic landmarks.
    '''

    batch_size = vertices.shape[0]

    aa_pose = torch.index_select(pose.view(batch_size, -1, 3), 1,
                                 neck_kin_chain)
    rot_mats = batch_rodrigues(
        aa_pose.view(-1, 3), dtype=dtype).view(batch_size, -1, 3, 3)

    rel_rot_mat = torch.eye(3, device=vertices.device,
                            dtype=dtype).unsqueeze_(dim=0)
    for idx in range(len(neck_kin_chain)):
        rel_rot_mat = torch.bmm(rot_mats[:, idx], rel_rot_mat)

    y_rot_angle = torch.round(
        torch.clamp(-rot_mat_to_euler(rel_rot_mat) * 180.0 / np.pi,
                    max=39)).to(dtype=torch.long)
    neg_mask = y_rot_angle.lt(0).to(dtype=torch.long)
    mask = y_rot_angle.lt(-39).to(dtype=torch.long)
    neg_vals = mask * 78 + (1 - mask) * (39 - y_rot_angle)
    y_rot_angle = (neg_mask * neg_vals +
                   (1 - neg_mask) * y_rot_angle)

    dyn_lmk_faces_idx = torch.index_select(dynamic_lmk_faces_idx,
                                           0, y_rot_angle)
    dyn_lmk_b_coords = torch.index_select(dynamic_lmk_b_coords,
                                          0, y_rot_angle)

    return dyn_lmk_faces_idx, dyn_lmk_b_coords


def vertices2landmarks(vertices, faces, lmk_faces_idx, lmk_bary_coords):
    ''' Calculates landmarks by barycentric interpolation

        Parameters
        ----------
        vertices: torch.tensor BxVx3, dtype = torch.float32
            The tensor of input vertices
        faces: torch.tensor Fx3, dtype = torch.long
            The faces of the mesh
        lmk_faces_idx: torch.tensor L, dtype = torch.long
            The tensor with the indices of the faces used to calculate the
            landmarks.
        lmk_bary_coords: torch.tensor Lx3, dtype = torch.float32
            The tensor of barycentric coordinates that are used to interpolate
            the landmarks

        Returns
        -------
        landmarks: torch.tensor BxLx3, dtype = torch.float32
            The coordinates of the landmarks for each mesh in the batch
    '''
    # Extract the indices of the vertices for each face
    # BxLx3
    batch_size, num_verts = vertices.shape[:2]
    device = vertices.device

    lmk_faces = torch.index_select(faces, 0, lmk_faces_idx.view(-1)).view(
        batch_size, -1, 3)

    lmk_faces += torch.arange(
        batch_size, dtype=torch.long, device=device).view(-1, 1, 1) * num_verts

    lmk_vertices = vertices.view(-1, 3)[lmk_faces].view(
        batch_size, -1, 3, 3)

    landmarks = torch.einsum('blfi,blf->bli', [lmk_vertices, lmk_bary_coords])
    return landmarks


def lbs(betas, pose, v_template, shapedirs, posedirs, J_regressor, parents,
        lbs_weights, joints = None, pose2rot=True, v_shaped=None, dtype=torch.float32):
    ''' Performs Linear Blend Skinning with the given shape and pose parameters

        Parameters
        ----------
        betas : torch.tensor BxNB
            The tensor of shape parameters
        pose : torch.tensor Bx(J + 1) * 3
            The pose parameters in axis-angle format
        v_template torch.tensor BxVx3
            The template mesh that will be deformed
        shapedirs : torch.tensor 1xNB
            The tensor of PCA shape displacements
        posedirs : torch.tensor Px(V * 3)
            The pose PCA coefficients
        J_regressor : torch.tensor JxV
            The regressor array that is used to calculate the joints from
            the position of the vertices
        parents: torch.tensor J
            The array that describes the kinematic tree for the model
        lbs_weights: torch.tensor N x V x (J + 1)
            The linear blend skinning weights that represent how much the
            rotation matrix of each part affects each vertex
        pose2rot: bool, optional
            Flag on whether to convert the input pose tensor to rotation
            matrices. The default value is True. If False, then the pose tensor
            should already contain rotation matrices and have a size of
            Bx(J + 1)x9
        dtype: torch.dtype, optional

        Returns
        -------
        verts: torch.tensor BxVx3
            The vertices of the mesh after applying the shape and pose
            displacements.
        joints: torch.tensor BxJx3
            The joints of the model
    '''

    batch_size = max(betas.shape[0], pose.shape[0])
    device = betas.device

    # Add shape contribution
    if v_shaped is None:
        v_shaped = v_template + blend_shapes(betas, shapedirs)

    # Get the joints
    # NxJx3 array
    if joints is not None:
        J = joints
    else:
        J = vertices2joints(J_regressor, v_shaped)

    # 3. Add pose blend shapes
    # N x J x 3 x 3
    ident = torch.eye(3, dtype=dtype, device=device)
    if pose2rot:
        rot_mats = batch_rodrigues(
            pose.view(-1, 3), dtype=dtype).view([batch_size, -1, 3, 3])

        pose_feature = (rot_mats[:, 1:, :, :] - ident).view([batch_size, -1])
        # (N x P) x (P, V * 3) -> N x V x 3
        pose_offsets = torch.matmul(pose_feature, posedirs).view(batch_size, -1, 3)
    else:
        pose_feature = pose[:, 1:].view(batch_size, -1, 3, 3) - ident
        rot_mats = pose.view(batch_size, -1, 3, 3)

        pose_offsets = torch.matmul(pose_feature.view(batch_size, -1),
                                    posedirs).view(batch_size, -1, 3)

    v_posed = pose_offsets + v_shaped
    # 4. Get the global joint location
    J_transformed, A = batch_rigid_transform(rot_mats, J, parents, dtype=dtype)

    # 5. Do skinning:
    # W is N x V x (J + 1)
    W = lbs_weights.unsqueeze(dim=0).expand([batch_size, -1, -1])
    # (N x V x (J + 1)) x (N x (J + 1) x 16)
    num_joints = J_regressor.shape[0]
    T = torch.matmul(W, A.view(batch_size, num_joints, 16)) \
        .view(batch_size, -1, 4, 4)

    homogen_coord = torch.ones([batch_size, v_posed.shape[1], 1],
                               dtype=dtype, device=device)
    v_posed_homo = torch.cat([v_posed, homogen_coord], dim=2)
    v_homo = torch.matmul(T, torch.unsqueeze(v_posed_homo, dim=-1))

    verts = v_homo[:, :, :3, 0]

    return verts, J_transformed


def vertices2joints(J_regressor, vertices):
    ''' Calculates the 3D joint locations from the vertices

    Parameters
    ----------
    J_regressor : torch.tensor JxV
        The regressor array that is used to calculate the joints from the
        position of the vertices
    vertices : torch.tensor BxVx3
        The tensor of mesh vertices

    Returns
    -------
    torch.tensor BxJx3
        The location of the joints
    '''

    return torch.einsum('bik,ji->bjk', [vertices, J_regressor])


def blend_shapes(betas, shape_disps):
    ''' Calculates the per vertex displacement due to the blend shapes


    Parameters
    ----------
    betas : torch.tensor Bx(num_betas)
        Blend shape coefficients
    shape_disps: torch.tensor Vx3x(num_betas)
        Blend shapes

    Returns
    -------
    torch.tensor BxVx3
        The per-vertex displacement due to shape deformation
    '''

    # Displacement[b, m, k] = sum_{l} betas[b, l] * shape_disps[m, k, l]
    # i.e. Multiply each shape displacement by its corresponding beta and
    # then sum them.

    #print(betas.device,shape_disps.device)
    blend_shape = torch.einsum('bl,mkl->bmk', [betas, shape_disps])
    return blend_shape


def batch_rodrigues(rot_vecs, epsilon=1e-8, dtype=torch.float32):
    ''' Calculates the rotation matrices for a batch of rotation vectors
        Parameters
        ----------
        rot_vecs: torch.tensor Nx3
            array of N axis-angle vectors
        Returns
        -------
        R: torch.tensor Nx3x3
            The rotation matrices for the given axis-angle parameters
    '''

    batch_size = rot_vecs.shape[0]
    device = rot_vecs.device

    angle = torch.norm(rot_vecs + 1e-8, dim=1, keepdim=True)
    rot_dir = rot_vecs / angle

    cos = torch.unsqueeze(torch.cos(angle), dim=1)
    sin = torch.unsqueeze(torch.sin(angle), dim=1)

    # Bx1 arrays
    rx, ry, rz = torch.split(rot_dir, 1, dim=1)
    K = torch.zeros((batch_size, 3, 3), dtype=dtype, device=device)

    zeros = torch.zeros((batch_size, 1), dtype=dtype, device=device)
    K = torch.cat([zeros, -rz, ry, rz, zeros, -rx, -ry, rx, zeros], dim=1) \
        .view((batch_size, 3, 3))

    ident = torch.eye(3, dtype=dtype, device=device).unsqueeze(dim=0)
    rot_mat = ident + sin * K + (1 - cos) * torch.bmm(K, K)
    return rot_mat


def transform_mat(R, t):
    ''' Creates a batch of transformation matrices
        Args:
            - R: Bx3x3 array of a batch of rotation matrices
            - t: Bx3x1 array of a batch of translation vectors
        Returns:
            - T: Bx4x4 Transformation matrix
    '''
    # No padding left or right, only add an extra row
    return torch.cat([F.pad(R, [0, 0, 0, 1]),
                      F.pad(t, [0, 0, 0, 1], value=1)], dim=2)


def batch_rigid_transform(rot_mats, joints, parents, dtype=torch.float32):
    """
    Applies a batch of rigid transformations to the joints

    Parameters
    ----------
    rot_mats : torch.tensor BxNx3x3
        Tensor of rotation matrices
    joints : torch.tensor BxNx3
        Locations of joints
    parents : torch.tensor BxN
        The kinematic tree of each object
    dtype : torch.dtype, optional:
        The data type of the created tensors, the default is torch.float32

    Returns
    -------
    posed_joints : torch.tensor BxNx3
        The locations of the joints after applying the pose rotations
    rel_transforms : torch.tensor BxNx4x4
        The relative (with respect to the root joint) rigid transformations
        for all the joints
    """

    joints = torch.unsqueeze(joints, dim=-1)

    rel_joints = joints.clone()
    rel_joints[:, 1:] -= joints[:, parents[1:]]

    transforms_mat = transform_mat(
        rot_mats.reshape(-1, 3, 3),
        rel_joints.reshape(-1, 3, 1)).reshape(-1, joints.shape[1], 4, 4)

    transform_chain = [transforms_mat[:, 0]]
    for i in range(1, parents.shape[0]):
        # Subtract the joint location at the rest pose
        # No need for rotation, since it's identity when at rest
        curr_res = torch.matmul(transform_chain[parents[i]],
                                transforms_mat[:, i])
        transform_chain.append(curr_res)

    transforms = torch.stack(transform_chain, dim=1)

    # The last column of the transformations contains the posed joints
    posed_joints = transforms[:, :, :3, 3]

    # The last column of the transformations contains the posed joints
    posed_joints = transforms[:, :, :3, 3]

    joints_homogen = F.pad(joints, [0, 0, 0, 1])

    rel_transforms = transforms - F.pad(
        torch.matmul(transforms, joints_homogen), [3, 0, 0, 0, 0, 0, 0, 0])

    return posed_joints, rel_transforms


D:\Projects\smplify-x\src\human-body-prior\src\human_body_prior\body_model\rigid_object_model.py
# -*- coding: utf-8 -*-
#
# Copyright (C) 2019 Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG),
# acting on behalf of its Max Planck Institute for Intelligent Systems and the
# Max Planck Institute for Biological Cybernetics. All rights reserved.
#
# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights
# on this computer program. You can only use this computer program if you have closed a license agreement
# with MPG or you get the right to use the computer program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and liable to prosecution.
# Contact: ps-license@tuebingen.mpg.de
#
#
# If you use this code in a research publication please consider citing the following:
#
# Expressive Body Capture: 3D Hands, Face, and Body from a Single Image <https://arxiv.org/abs/1904.05866>
#
#
# Code Developed by:
# Nima Ghorbani <https://nghorbani.github.io/>
#
# 2018.12.13

import numpy as np

import torch
import torch.nn as nn

# from smplx.lbs import lbs
from human_body_prior.body_model.lbs import lbs
# import trimesh # dont use this package for loading meshes since it messes up the order of vertices
from psbody.mesh import Mesh
from human_body_prior.body_model.lbs import batch_rodrigues

class RigidObjectModel(nn.Module):

    def __init__(self, plpath, batch_size=1, dtype=torch.float32):
        super(RigidObjectModel, self).__init__()

        trans = torch.tensor(np.zeros((batch_size, 3)), dtype=dtype, requires_grad=True)
        self.register_parameter('trans', nn.Parameter(trans, requires_grad=True))

        root_orient = torch.tensor(np.zeros((batch_size, 3)), dtype=dtype, requires_grad=True)
        self.register_parameter('root_orient', nn.Parameter(root_orient, requires_grad=True))

        mesh = Mesh(filename=plpath)

        self.rigid_v = torch.from_numpy(np.repeat(mesh.v[np.newaxis], batch_size, axis=0)).type(dtype)
        self.f = torch.from_numpy(mesh.f.astype(np.int32))

    def forward(self, root_orient, trans):
        if root_orient is None: root_orient = self.root_orient
        if trans is None: trans = self.trans
        verts = torch.bmm(self.rigid_v, batch_rodrigues(root_orient)) + trans.view(-1,1,3)

        res = {}
        res['v'] = verts
        res['f'] = self.f

        class result_meta(object): pass

        res_class = result_meta()
        for k, v in res.items():
            res_class.__setattr__(k, v)
        res = res_class

        return res


D:\Projects\smplify-x\src\human-body-prior\src\human_body_prior\body_model\__init__.py
# -*- coding: utf-8 -*-
#
# Copyright (C) 2019 Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG),
# acting on behalf of its Max Planck Institute for Intelligent Systems and the
# Max Planck Institute for Biological Cybernetics. All rights reserved.
#
# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights
# on this computer program. You can only use this computer program if you have closed a license agreement
# with MPG or you get the right to use the computer program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and liable to prosecution.
# Contact: ps-license@tuebingen.mpg.de
#
#
# If you use this code in a research publication please consider citing the following:
#
# Expressive Body Capture: 3D Hands, Face, and Body from a Single Image <https://arxiv.org/abs/1904.05866>
#
#
# Code Developed by:
# Nima Ghorbani <https://nghorbani.github.io/>
#
# 2018.01.02


D:\Projects\smplify-x\src\human-body-prior\src\human_body_prior\data\dataloader.py
# -*- coding: utf-8 -*-
#
# Copyright (C) 2019 Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG),
# acting on behalf of its Max Planck Institute for Intelligent Systems and the
# Max Planck Institute for Biological Cybernetics. All rights reserved.
#
# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights
# on this computer program. You can only use this computer program if you have closed a license agreement
# with MPG or you get the right to use the computer program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and liable to prosecution.
# Contact: ps-license@tuebingen.mpg.de
#
#
# If you use this code in a research publication please consider citing the following:
#
# Expressive Body Capture: 3D Hands, Face, and Body from a Single Image <https://arxiv.org/abs/1904.05866>
#
#
# Code Developed by:
# Nima Ghorbani <https://nghorbani.github.io/>
#
# 2018.01.02

import glob, os

import torch
from torch.utils.data import Dataset
from configer import Configer

class VPoserDS(Dataset):
    """AMASS: a pytorch loader for unified human motion capture dataset. http://amass.is.tue.mpg.de/"""

    def __init__(self, dataset_dir, data_fields=[]):
        assert os.path.exists(dataset_dir),dataset_dir
        self.ds = {}
        for data_fname in glob.glob(os.path.join(dataset_dir, '*.pt')):
            k = os.path.basename(data_fname).replace('.pt','')
            if len(data_fields) != 0 and k not in data_fields: continue
            self.ds[k] = torch.load(data_fname).type(torch.float32)

        dataset_ps_fname = glob.glob(os.path.join(dataset_dir, '..', '*.ini'))
        if len(dataset_ps_fname):
            self.ps = Configer(default_ps_fname=dataset_ps_fname[0], dataset_dir=dataset_dir)

    def __len__(self):
        k = list(self.ds.keys())[0]
        return len(self.ds[k])

    def __getitem__(self, idx):
        return self.fetch_data(idx)

    def fetch_data(self, idx):
        data = {k: self.ds[k][idx] for k in self.ds.keys()}
        return data



D:\Projects\smplify-x\src\human-body-prior\src\human_body_prior\data\prepare_data.py
# -*- coding: utf-8 -*-
#
# Copyright (C) 2019 Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG),
# acting on behalf of its Max Planck Institute for Intelligent Systems and the
# Max Planck Institute for Biological Cybernetics. All rights reserved.
#
# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights
# on this computer program. You can only use this computer program if you have closed a license agreement
# with MPG or you get the right to use the computer program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and liable to prosecution.
# Contact: ps-license@tuebingen.mpg.de
#
#
# If you use this code in a research publication please consider citing the following:
#
# Expressive Body Capture: 3D Hands, Face, and Body from a Single Image <https://arxiv.org/abs/1904.05866>
#
#
# Code Developed by:
# Nima Ghorbani <https://nghorbani.github.io/>
#
# 2018.01.02

import glob
import os.path as osp
import shutil

import numpy as np
import torch
from configer import Configer
from human_body_prior.tools.omni_tools import logger_sequencer
from human_body_prior.tools.omni_tools import makepath, log2file


def dataset_exists(dataset_dir, split_names=None):
    '''
    This function checks whether a valid SuperCap dataset directory exists at a location
    Parameters
    ----------
    dataset_dir

    Returns
    -------

    '''
    if dataset_dir is None: return False
    if split_names is None:
        split_names = ['train', 'vald', 'test']
    import os

    import numpy as np

    done = []
    for split_name in split_names:
        for k in ['root_orient', 'pose_body']:  # , 'betas', 'trans', 'joints']:
            outfname = os.path.join(dataset_dir, split_name, f'{k}.pt')
            done.append(os.path.exists(outfname))
    return np.all(done)


def prepare_vposer_datasets(vposer_dataset_dir, amass_splits, amass_dir, logger=None):
    if dataset_exists(vposer_dataset_dir):
        if logger is not None: logger(f'VPoser dataset already exists at {vposer_dataset_dir}')
        return

    ds_logger = log2file(makepath(vposer_dataset_dir, 'dataset.log', isfile=True), write2file_only=True)
    logger = ds_logger if logger is None else logger_sequencer([ds_logger, logger])

    logger(f'Creating pytorch dataset at {vposer_dataset_dir}')
    logger(f'Using AMASS body parameters from {amass_dir}')

    shutil.copy2(__file__, vposer_dataset_dir)

    # class AMASS_ROW(pytables.IsDescription):
    #
    #     # gender = pytables.Int16Col(1)  # 1-character String
    #     root_orient = pytables.Float32Col(3)  # float  (single-precision)
    #     pose_body = pytables.Float32Col(21 * 3)  # float  (single-precision)
    #     # pose_hand = pytables.Float32Col(2 * 15 * 3)  # float  (single-precision)
    #
    #     # betas = pytables.Float32Col(16)  # float  (single-precision)
    #     # trans = pytables.Float32Col(3)  # float  (single-precision)

    def fetch_from_amass(ds_names):
        keep_rate = 0.3

        npz_fnames = []
        for ds_name in ds_names:
            mosh_stageII_fnames = glob.glob(osp.join(amass_dir, ds_name, '*/*_poses.npz'))
            npz_fnames.extend(mosh_stageII_fnames)
            logger('Found {} sequences from {}.'.format(len(mosh_stageII_fnames), ds_name))

            for npz_fname in npz_fnames:
                cdata = np.load(npz_fname)
                N = len(cdata['poses'])

                # skip first and last frames to avoid initial standard poses, e.g. T pose
                cdata_ids = np.random.choice(list(range(int(0.1 * N), int(0.9 * N), 1)), int(keep_rate * 0.8 * N),
                                             replace=False)
                if len(cdata_ids) < 1: continue
                fullpose = cdata['poses'][cdata_ids].astype(np.float32)
                yield {'pose_body': fullpose[:, 3:66], 'root_orient': fullpose[:, :3]}

    for split_name, ds_names in amass_splits.items():
        if dataset_exists(vposer_dataset_dir, split_names=[split_name]): continue
        logger(f'Preparing VPoser data for split {split_name}')

        data_fields = {}
        for data in fetch_from_amass(ds_names):
            for k in data.keys():
                if k not in data_fields: data_fields[k] = []
                data_fields[k].append(data[k])

        for k, v in data_fields.items():
            outpath = makepath(vposer_dataset_dir, split_name, '{}.pt'.format(k), isfile=True)
            v = np.concatenate(v)
            torch.save(torch.tensor(v), outpath)

        logger(
            f'{len(v)} datapoints dumped for split {split_name}. ds_meta_pklpath: {osp.join(vposer_dataset_dir, split_name)}')

    Configer(**{
        'amass_splits': amass_splits.toDict(),
        'amass_dir': amass_dir,
    }).dump_settings(makepath(vposer_dataset_dir, 'settings.ini', isfile=True))

    logger(f'Dumped final pytorch dataset at {vposer_dataset_dir}')


D:\Projects\smplify-x\src\human-body-prior\src\human_body_prior\data\README.md
# Preparing VPoser Training Dataset
The Human Body Prior, VPoser, presented here is trained on  [AMASS](https://amass.is.tue.mpg.de/) dataset. 
AMASS is a large collection of human marker based optical mocap data as [SMPL](http://smpl.is.tue.mpg.de/) body model parameters.
VPoser code here is implemented in [PyTorch](https://pytorch.org/), therefore, the data preparation code, 
turns AMASS data into pytorch readable *.pt* files in three stages:

***Stage I*** turns the AMASS numpy *.npz* files into PyTorch *.pt* files. 
For this, first you would need to download body parameters from the AMASS webpage: https://amass.is.tue.mpg.de/dataset.
Then you have to select subsets of AMASS to be used for each data splits, e.g. train/validation/test. 
Here we follow the recommended data splits of AMASS, that is:

```python
amass_splits = {
    'vald': ['HumanEva', 'MPI_HDM05', 'SFU', 'MPI_mosh'],
    'test': ['Transitions_mocap', 'SSM_synced'],
    'train': ['CMU', 'MPI_Limits', 'TotalCapture', 'Eyes_Japan_Dataset', 'KIT', 'BML', 'EKUT', 'TCD_handMocap', 'ACCAD']
}
amass_splits['train'] = list(set(amass_splits['train']).difference(set(amass_splits['test'] + amass_splits['vald'])))
```

During this stage, we also subsample the original data, so that we only take every some frames of the original mocap
to be included in the final data files. 
 
***Stage II*** turns the AMASS pytorch files into HDF5, *h5* files and along the process augments the data with extra fields or noise. 
Using pytorch in the middle stage helps to parallelize augmentation tasks. 
Furthermore, we use HDF5 files for the middle stage so that they can be used in other deep learning frameworks as well.

***Stage III*** again converts the augmented HDF5 files into final pytorch files that should be provided to the current VPoser training script.

During the process, the data preparation code can dump a log file to make it possible to track how data for different
experiments has been produced.

Below is a full python script example to prepare a VPoser training data:

```python
import os
from human_body_prior.tools.omni_tools import makepath, log2file
from human_body_prior.data.prepare_data import prepare_vposer_datasets

expr_code = 'SOME_UNIQUE_ID'

amass_dir = 'THE_PATH_TO_AMASS_NPZ_FILES'

vposer_datadir = makepath('OUTPUT_DATA_PATH/%s' % (expr_code))

logger = log2file(os.path.join(vposer_datadir, '%s.log' % (expr_code)))
logger('[%s] Preparing data for training VPoser.'%expr_code)

amass_splits = {
    'vald': ['HumanEva', 'MPI_HDM05', 'SFU', 'MPI_mosh'],
    'test': ['Transitions_mocap', 'SSM_synced'],
    'train': ['CMU', 'MPI_Limits', 'TotalCapture', 'Eyes_Japan_Dataset', 'KIT', 'BML', 'EKUT', 'TCD_handMocap', 'ACCAD']
}
amass_splits['train'] = list(set(amass_splits['train']).difference(set(amass_splits['test'] + amass_splits['vald'])))

prepare_vposer_datasets(vposer_datadir,amass_splits,amass_dir,logger=logger)
```

## Note
If you consider training your own VPoser for your research using AMASS dataset, then please follow its respective citation guideline. 

D:\Projects\smplify-x\src\human-body-prior\src\human_body_prior\data\__init__.py
# -*- coding: utf-8 -*-
#
# Copyright (C) 2019 Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG),
# acting on behalf of its Max Planck Institute for Intelligent Systems and the
# Max Planck Institute for Biological Cybernetics. All rights reserved.
#
# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights
# on this computer program. You can only use this computer program if you have closed a license agreement
# with MPG or you get the right to use the computer program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and liable to prosecution.
# Contact: ps-license@tuebingen.mpg.de
#
#
# If you use this code in a research publication please consider citing the following:
#
# Expressive Body Capture: 3D Hands, Face, and Body from a Single Image <https://arxiv.org/abs/1904.05866>
#
#
# Code Developed by:
# Nima Ghorbani <https://nghorbani.github.io/>
#
# 2018.01.02


D:\Projects\smplify-x\src\human-body-prior\src\human_body_prior\evaluations\run_on_amass.py
# -*- coding: utf-8 -*-
#
# Copyright (C) 2019 Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG),
# acting on behalf of its Max Planck Institute for Intelligent Systems and the
# Max Planck Institute for Biological Cybernetics. All rights reserved.
#
# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights
# on this computer program. You can only use this computer program if you have closed a license agreement
# with MPG or you get the right to use the computer program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and liable to prosecution.
# Contact: ps-license@tuebingen.mpg.de
#
#
# If you use this code in a research publication please consider citing the following:
#
# Expressive Body Capture: 3D Hands, Face, and Body from a Single Image <https://arxiv.org/abs/1904.05866>
# AMASS: Archive of Motion Capture as Surface Shapes <https://arxiv.org/abs/1904.03278>
#
#
# Code Developed by:
# Nima Ghorbani <https://www.linkedin.com/in/nghorbani/>
#
# 2019.05.28

import json
import os

import torch
from human_body_prior.body_model.body_model import BodyModel
from human_body_prior.data.dataloader import VPoserDS
from human_body_prior.tools.model_loader import load_vposer
from human_body_prior.tools.omni_tools import copy2cpu as c2c
from human_body_prior.tools.omni_tools import makepath
from human_body_prior.train.vposer_smpl import VPoserTrainer
from torch.utils.data import DataLoader
from tqdm import tqdm


def evaluate_model(dataset_dir, vp_model, vp_ps, batch_size=5, save_upto_bnum=10, splitname='test'):
    assert splitname in ['test', 'train', 'vald']
    comp_device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    ds_name = dataset_dir.split('/')[-2]

    vp_model.eval()
    vp_model = vp_model.to(comp_device)

    with torch.no_grad():
        bm = BodyModel(vp_ps.bm_fname, batch_size=1, num_betas=16).to(comp_device)

    ds = VPoserDS(dataset_dir=os.path.join(dataset_dir, splitname))
    ds = DataLoader(ds, batch_size=batch_size, shuffle=False, drop_last=False)

    outpath = os.path.join(vp_ps.work_dir, 'evaluations', 'ds_%s'%ds_name, os.path.basename(vp_ps.best_model_fname).replace('.pt',''), '%s_samples'%splitname)
    print('dumping to %s'%outpath)

    for bId, dorig in enumerate(ds):
        dorig = {k: dorig[k].to(comp_device) for k in dorig.keys()}

        imgpath = makepath(os.path.join(outpath, '%s-%03d.png' % (vp_ps.expr_code, bId)), isfile=True)
        VPoserTrainer.vis_results(dorig, bm, vp_model, imgpath, view_angles=[0, 180])#, view_angles = [0, 180, 90])

        if bId> save_upto_bnum: break


def evaluate_error(dataset_dir, vp_model, vp_ps, batch_size=512):
    vp_model.eval()

    ds_name = dataset_dir.split('/')[-2]

    comp_device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    bm = BodyModel(vp_ps.bm_fname, batch_size=batch_size, num_betas=16).to(comp_device)
    vp_model = vp_model.to(comp_device)

    # from psbody.mesh import Mesh, MeshViewer
    # from human_body_prior.tools.omni_tools import colors
    # import time
    # mv = MeshViewer()

    final_errors = {}
    # for splitname in ['test']:
    for splitname in ['test', 'train', 'vald']:

        ds = VPoserDS(dataset_dir=os.path.join(dataset_dir, splitname))
        print('%s dataset size: %s'%(splitname,len(ds)))
        ds = DataLoader(ds, batch_size=batch_size, shuffle=False, drop_last=True)#batchsize for bm is fixed so drop the last one

        loss_mean = []
        with torch.no_grad():
            for dorig in tqdm(ds):
                dorig = {k: dorig[k].to(comp_device) for k in dorig.keys()}

                MESH_SCALER = 1000

                drec = vp_model(**dorig)
                for k in dorig: # whatever field is missing from drec copy from dorig
                    if k not in drec:
                        drec[k] = dorig[k]
                        if len(final_errors)==0 and len(loss_mean) == 0: 
                            print('Field %s is not predicted by the model and is copied from the original data.'%k)

                with torch.no_grad():
                    body_orig = bm(**dorig).v
                    body_rec = bm(**drec).v

                # body_orig_mesh = Mesh(c2c(body_orig[0]), c2c(bm.f), vc= colors['blue'])
                # body_rec_mesh = Mesh(c2c(body_rec[0]), c2c(bm.f), vc=colors['red'])
                # mv.set_dynamic_meshes([body_rec_mesh, body_orig_mesh])
                # time.sleep(0.2)

                # loss_mean.append(torch.mean(torch.sqrt(torch.pow((mesh_orig - mesh_rec)* MESH_SCALER, 2))))
                loss_mean.append(torch.mean(torch.abs(body_orig - body_rec)* MESH_SCALER))

        final_errors[splitname] = {'v2v_mae': float(c2c(torch.stack(loss_mean).mean()))}
        print(splitname, final_errors[splitname])

    outpath = makepath(os.path.join(vp_ps.work_dir, 'evaluations', 'ds_%s'%ds_name, os.path.basename(vp_ps.best_model_fname).replace('.pt','.json')), isfile=True)
    with open(outpath, 'w') as f:
        json.dump(final_errors,f)

    return final_errors

if __name__ == '__main__':
    expr_code = '008_SV01_T00'
    # data_code = '007_00_00'

    expr_dir = '/ps/project/human_body_prior/VPoser/smpl/pytorch/%s'%expr_code

    vp_model, vp_ps = load_vposer(expr_dir)
    dataset_dir = vp_ps.dataset_dir
    # dataset_dir = '/ps/project/human_body_prior/VPoser/data/%s/smpl/pytorch/stage_III'%data_code

    print('dataset_dir: %s'%dataset_dir)
    # # # for splitname in ['test']:
    # for splitname in ['train', 'test', 'vald']:
    #    evaluate_model(dataset_dir, vp_model, vp_ps, batch_size=3, save_upto_bnum=5, splitname=splitname)

    final_errors = evaluate_error(dataset_dir, vp_model, vp_ps, batch_size=512)
    print('[%s] [DS: %s] -- %s' % (vp_ps.best_model_fname, dataset_dir,  ', '.join(['%s: %.2e'%(k, v['v2v_mae']) for k,v in final_errors.items()])))



D:\Projects\smplify-x\src\human-body-prior\src\human_body_prior\evaluations\__init__.py


D:\Projects\smplify-x\src\human-body-prior\src\human_body_prior\models\ik_engine.py
# -*- coding: utf-8 -*-
#
# Copyright (C) 2019 Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG),
# acting on behalf of its Max Planck Institute for Intelligent Systems and the
# Max Planck Institute for Biological Cybernetics. All rights reserved.
#
# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights
# on this computer program. You can only use this computer program if you have closed a license agreement
# with MPG or you get the right to use the computer program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and liable to prosecution.
# Contact: ps-license@tuebingen.mpg.de
#
#
# If you use this code in a research publication please consider citing the following:
#
# Expressive Body Capture: 3D Hands, Face, and Body from a Single Image <https://arxiv.org/abs/1904.05866>
#
#
# Code Developed by:
# Nima Ghorbani <https://nghorbani.github.io/>
#
# 2021.02.12

from typing import List, Dict

can_display = True

try:
    from psbody.mesh import Mesh

    from body_visualizer.mesh.psbody_mesh_cube import points_to_cubes
    from body_visualizer.mesh.psbody_mesh_sphere import points_to_spheres
    from body_visualizer.tools.mesh_tools import rotateXYZ
    from body_visualizer.tools.vis_tools import colors
    from psbody.mesh import MeshViewers

except Exception as e:
    print(e)
    print('psbody.mesh based visualization could not be started. skipping ...')
    can_display = False

from torch import nn
import torch

from human_body_prior.tools.model_loader import load_model

import numpy as np

from human_body_prior.tools.omni_tools import copy2cpu as c2c

from human_body_prior.tools.omni_tools import log2file

from human_body_prior.models.vposer_model import VPoser
from human_body_prior.tools.omni_tools import flatten_list


def visualize(points, bm_f, mvs, kpts_colors, verbosity=2, logger=None):
    from human_body_prior.tools.omni_tools import log2file

    if logger is None: logger = log2file()

    def view(opt_objs, body_v, virtual_markers, opt_it):
        if verbosity <= 0: return
        opt_objs_cpu = {k: c2c(v) for k, v in opt_objs.items()}

        total_loss = np.sum([np.sum(v) for k, v in opt_objs_cpu.items()])
        message = 'it {} -- [total loss = {:.2e}] - {}'.format(opt_it, total_loss, ' | '.join(
            ['%s = %2.2e' % (k, np.sum(v)) for k, v in opt_objs_cpu.items()]))
        logger(message)
        if verbosity > 1 and can_display:
            bs = body_v.shape[0]
            np.random.seed(100)
            frame_ids = list(range(bs)) if bs <= len(mvs) else np.random.choice(bs, size=len(mvs),
                                                                                replace=False).tolist()
            if bs > len(mvs): message += ' -- [frame_ids: {}]'.format(frame_ids)
            for dispId, fId in enumerate(
                    frame_ids):  # check for the number of frames in mvs and show a randomly picked number of frames in body if there is more to show than row*cols available
                new_body_v = rotateXYZ(body_v[fId], [-90, 0, 0])

                orig_mrk_mesh = points_to_spheres(rotateXYZ(c2c(points[fId]), [-90, 0, 0]), radius=0.01,
                                                  point_color=kpts_colors)
                virtual_markers_mesh = points_to_cubes(rotateXYZ(virtual_markers[fId], [-90, 0, 0]), radius=0.01,
                                                       point_color=kpts_colors)
                new_body_mesh = Mesh(new_body_v, bm_f, vc=colors['grey'])

                # linev = rotateXYZ(np.hstack((c2c(points[fId]), virtual_markers[fId])).reshape((-1, 3)), [-90,0,0])
                # linee = np.arange(len(linev)).reshape((-1, 2))
                # ll = Lines(v=linev, e=linee)
                # ll.vc = (ll.v * 0. + 1) * np.array([0.00, 0.00, 1.00])
                # mvs[dispId].set_dynamic_lines([ll])

                # orig_mrk_mesh = points_to_spheres(data_pc, radius=0.01, vc=colors['blue'])
                mvs[dispId].set_dynamic_meshes([orig_mrk_mesh, virtual_markers_mesh])
                mvs[dispId].set_static_meshes([new_body_mesh])

            mvs[0].set_titlebar(message)
            # if out_dir is not None: mv.save_snapshot(os.path.join(out_dir, '%05d_it_%.5d.png' %(frame_id, opt_it)))

    return view


class AdamInClosure():
    def __init__(self, var_list, lr, max_iter=100, tolerance_change=1e-5):
        self.optimizer = torch.optim.Adam(var_list, lr)
        self.max_iter = max_iter
        self.tolerance_change = tolerance_change

    def step(self, closure):
        prev_loss = None
        for it in range(self.max_iter):
            loss = closure()
            self.optimizer.step()
            if prev_loss is None:
                prev_loss = loss
                continue
            if torch.isnan(loss):
                # breakpoint()
                break
            if abs(loss - prev_loss) < self.tolerance_change:
                print('abs(loss - prev_loss) <  self.tolerance_change')
                break

    def zero_grad(self):
        self.optimizer.zero_grad()


def ik_fit(optimizer, source_kpts_model, static_vars, vp_model, extra_params={}, on_step=None, gstep=0):
    data_loss = extra_params.get('data_loss', torch.nn.SmoothL1Loss(reduction='mean'))

    # data_loss =
    # data_loss = torch.nn.L1Loss(reduction='mean')#change with SmoothL1

    def fit(weights, free_vars):
        fit.gstep += 1
        optimizer.zero_grad()

        free_vars['pose_body'] = vp_model.decode(free_vars['poZ_body'])['pose_body'].contiguous().view(-1, 63)
        nonan_mask = torch.isnan(free_vars['poZ_body']).sum(-1) == 0

        opt_objs = {}

        res = source_kpts_model(free_vars)

        opt_objs['data'] = data_loss(res['source_kpts'], static_vars['target_kpts'])

        opt_objs['betas'] = torch.pow(free_vars['betas'][nonan_mask], 2).sum()
        opt_objs['poZ_body'] = torch.pow(free_vars['poZ_body'][nonan_mask], 2).sum()

        opt_objs = {k: opt_objs[k] * v for k, v in weights.items() if k in opt_objs.keys()}
        loss_total = torch.sum(torch.stack(list(opt_objs.values())))
        # breakpoint()

        loss_total.backward()

        if on_step is not None:
            on_step(opt_objs, c2c(res['body'].v), c2c(res['source_kpts']), fit.gstep)

        fit.free_vars = {k: v for k, v in free_vars.items()}  # if k in IK_Engine.fields_to_optimize}
        # fit.nonan_mask = nonan_mask
        fit.final_loss = loss_total

        return loss_total

    fit.gstep = gstep
    fit.final_loss = None
    fit.free_vars = {}
    # fit.nonan_mask = None
    return fit


class IK_Engine(nn.Module):

    def __init__(self,
                 vposer_expr_dir: str,
                 data_loss,
                 optimizer_args: dict = {'type': 'ADAM'},
                 stepwise_weights: List[Dict] = [{'data': 10., 'poZ_body': .01, 'betas': .5}],
                 display_rc: tuple = (2, 1),
                 verbosity: int = 1,
                 num_betas: int = 16,
                 logger=None,
                 ):
        '''

        :param vposer_expr_dir: The vposer directory that holds the settings and model snapshot
        :param data_loss: should be a pytorch callable (source, target) that returns the accumulated loss
        :param optimizer_args: arguments for optimizers
        :param stepwise_weights: list of dictionaries. each list element defines weights for one full step of optimization
                                 if a weight value is left out, its respective object item will be removed as well. imagine optimizing without data term!
        :param display_rc: number of row and columns in case verbosity > 1
        :param verbosity: 0: silent, 1: text, 2: text/visual. running 2 over ssh would need extra work
        :param logger: an instance of human_body_prior.tools.omni_tools.log2file
        '''

        super(IK_Engine, self).__init__()

        assert isinstance(stepwise_weights, list), ValueError('stepwise_weights should be a list of dictionaries.')
        assert np.all(['data' in l for l in stepwise_weights]), ValueError(
            'The term data should be available in every weight of anealed optimization step: {}'.format(
                stepwise_weights))

        self.data_loss = torch.nn.SmoothL1Loss(reduction='mean') if data_loss is None else data_loss
        self.num_betas = num_betas
        self.stepwise_weights = stepwise_weights
        self.verbosity = verbosity
        self.optimizer_args = optimizer_args

        self.logger = log2file() if logger is None else logger

        if verbosity > 1 and can_display:
            mvs = MeshViewers(display_rc, keepalive=True)
            self.mvs = flatten_list(mvs)
            self.mvs[0].set_background_color(colors['white'])
        else:
            self.mvs = None

        self.vp_model, _ = load_model(vposer_expr_dir,
                                      model_code=VPoser,
                                      remove_words_in_model_weights='vp_model.',
                                      disable_grad=True)

    def forward(self, source_kpts, target_kpts, initial_body_params={}):
        '''
        source_kpts is a function that given body parameters computes source key points that should match target key points
        Try to reconstruct the bps signature by optimizing the body_poZ
        '''
        # if self.rt_ps.verbosity > 0: self.logger('Processing {} frames'.format(points.shape[0]))
        bs = target_kpts.shape[0]

        on_step = visualize(target_kpts,
                            kpts_colors=source_kpts.kpts_colors,
                            bm_f=source_kpts.bm_f,
                            mvs=self.mvs,
                            verbosity=self.verbosity,
                            logger=self.logger)

        comp_device = target_kpts.device
        # comp_device = self.vp_model.named_parameters().__next__()[1].device
        if 'pose_body' not in initial_body_params:
            initial_body_params['pose_body'] = torch.zeros([bs, 63], device=comp_device, dtype=torch.float,
                                                           requires_grad=False)
        if 'trans' not in initial_body_params:
            initial_body_params['trans'] = torch.zeros([bs, 3], device=comp_device, dtype=torch.float,
                                                       requires_grad=False)
        if 'betas' not in initial_body_params:
            initial_body_params['betas'] = torch.zeros([bs, self.num_betas], device=comp_device, dtype=torch.float,
                                                       requires_grad=False)
        if 'root_orient' not in initial_body_params:
            initial_body_params['root_orient'] = torch.zeros([bs, 3], device=comp_device, dtype=torch.float,
                                                             requires_grad=False)

        initial_body_params['poZ_body'] = self.vp_model.encode(initial_body_params['pose_body']).mean

        free_vars = {k: torch.nn.Parameter(v.detach(), requires_grad=True) for k, v in initial_body_params.items() if
                     k in ['betas', 'trans', 'poZ_body', 'root_orient']}
        static_vars = {
            'target_kpts': target_kpts,
            # 'trans': initial_body_params['trans'].detach(),
            # 'betas': initial_body_params['betas'].detach(),
            # 'poZ_body': initial_body_params['poZ_body'].detach()
        }

        if self.optimizer_args['type'].upper() == 'LBFGS':
            optimizer = torch.optim.LBFGS(list(free_vars.values()),
                                          lr=self.optimizer_args.get('lr', 1),
                                          max_iter=self.optimizer_args.get('max_iter', 100),
                                          tolerance_change=self.optimizer_args.get('tolerance_change', 1e-5),
                                          max_eval=self.optimizer_args.get('max_eval', None),
                                          history_size=self.optimizer_args.get('history_size', 100),
                                          line_search_fn='strong_wolfe')

        elif self.optimizer_args['type'].upper() == 'ADAM':
            optimizer = AdamInClosure(list(free_vars.values()),
                                      lr=self.optimizer_args.get('lr', 1e-3),
                                      max_iter=self.optimizer_args.get('max_iter', 100),
                                      tolerance_change=self.optimizer_args.get('tolerance_change', 1e-5),
                                      )
        else:
            raise ValueError('optimizer_type not recognized.')

        gstep = 0
        closure = ik_fit(optimizer,
                         source_kpts_model=source_kpts,
                         static_vars=static_vars,
                         vp_model=self.vp_model,
                         extra_params={'data_loss': self.data_loss},
                         on_step=on_step,
                         gstep=gstep)
        # try:

        for wts in self.stepwise_weights:
            optimizer.step(lambda: closure(wts, free_vars))
            free_vars = closure.free_vars
        # except:
        #
        #     pass

        # if closure.final_loss is None or torch.isnan(closure.final_loss) or torch.any(torch.isnan(free_vars['trans'])):
        #     if self.verbosity > 0:
        #         self.logger('NaN observed in the optimization results. you might want to restart the refinment procedure.')
        #     breakpoint()
        #     return None

        return closure.free_vars  # , closure.nonan_mask


D:\Projects\smplify-x\src\human-body-prior\src\human_body_prior\models\model_components.py
# -*- coding: utf-8 -*-
#
# Copyright (C) 2019 Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG),
# acting on behalf of its Max Planck Institute for Intelligent Systems and the
# Max Planck Institute for Biological Cybernetics. All rights reserved.
#
# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights
# on this computer program. You can only use this computer program if you have closed a license agreement
# with MPG or you get the right to use the computer program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and liable to prosecution.
# Contact: ps-license@tuebingen.mpg.de
#
#
# If you use this code in a research publication please consider citing the following:
#
# Expressive Body Capture: 3D Hands, Face, and Body from a Single Image <https://arxiv.org/abs/1904.05866>
#
#
# Code Developed by:
# Nima Ghorbani <https://nghorbani.github.io/>
#
# 2020.12.12

from torch import nn

class View(nn.Module):
    def __init__(self, *args):
        super(View, self).__init__()
        self.shape = args
        self._name = 'reshape'

    def forward(self, x):
        return x.view(self.shape)

class BatchFlatten(nn.Module):
    def __init__(self):
        super(BatchFlatten, self).__init__()
        self._name = 'batch_flatten'

    def forward(self, x):
        return x.view(x.shape[0], -1)

D:\Projects\smplify-x\src\human-body-prior\src\human_body_prior\models\vposer_model.py
# -*- coding: utf-8 -*-
#
# Copyright (C) 2019 Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG),
# acting on behalf of its Max Planck Institute for Intelligent Systems and the
# Max Planck Institute for Biological Cybernetics. All rights reserved.
#
# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights
# on this computer program. You can only use this computer program if you have closed a license agreement
# with MPG or you get the right to use the computer program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and liable to prosecution.
# Contact: ps-license@tuebingen.mpg.de
#
#
# If you use this code in a research publication please consider citing the following:
#
# Expressive Body Capture: 3D Hands, Face, and Body from a Single Image <https://arxiv.org/abs/1904.05866>
#
#
# Code Developed by:
# Nima Ghorbani <https://nghorbani.github.io/>
#
# 2020.12.12

import numpy as np
import torch
from human_body_prior.models.model_components import BatchFlatten
from human_body_prior.tools.rotation_tools import matrot2aa
from torch import nn
from torch.nn import functional as F


class ContinousRotReprDecoder(nn.Module):
    def __init__(self):
        super(ContinousRotReprDecoder, self).__init__()

    def forward(self, module_input):
        reshaped_input = module_input.view(-1, 3, 2)

        b1 = F.normalize(reshaped_input[:, :, 0], dim=1)

        dot_prod = torch.sum(b1 * reshaped_input[:, :, 1], dim=1, keepdim=True)
        b2 = F.normalize(reshaped_input[:, :, 1] - dot_prod * b1, dim=-1)
        b3 = torch.cross(b1, b2, dim=1)

        return torch.stack([b1, b2, b3], dim=-1)


class NormalDistDecoder(nn.Module):
    def __init__(self, num_feat_in, latentD):
        super(NormalDistDecoder, self).__init__()

        self.mu = nn.Linear(num_feat_in, latentD)
        self.logvar = nn.Linear(num_feat_in, latentD)

    def forward(self, Xout):
        return torch.distributions.normal.Normal(self.mu(Xout), F.softplus(self.logvar(Xout)))


class VPoser(nn.Module):
    def __init__(self, model_ps):
        super(VPoser, self).__init__()

        num_neurons, self.latentD = model_ps.model_params.num_neurons, model_ps.model_params.latentD

        self.num_joints = 21
        n_features = self.num_joints * 3

        self.encoder_net = nn.Sequential(
            BatchFlatten(),
            nn.BatchNorm1d(n_features),
            nn.Linear(n_features, num_neurons),
            nn.LeakyReLU(),
            nn.BatchNorm1d(num_neurons),
            nn.Dropout(0.1),
            nn.Linear(num_neurons, num_neurons),
            nn.Linear(num_neurons, num_neurons),
            NormalDistDecoder(num_neurons, self.latentD)
        )

        self.decoder_net = nn.Sequential(
            nn.Linear(self.latentD, num_neurons),
            nn.LeakyReLU(),
            nn.Dropout(0.1),
            nn.Linear(num_neurons, num_neurons),
            nn.LeakyReLU(),
            nn.Linear(num_neurons, self.num_joints * 6),
            ContinousRotReprDecoder(),
        )

    def encode(self, pose_body):
        '''
        :param Pin: Nx(numjoints*3)
        :param rep_type: 'matrot'/'aa' for matrix rotations or axis-angle
        :return:
        '''
        return self.encoder_net(pose_body)

    def decode(self, Zin):
        bs = Zin.shape[0]

        prec = self.decoder_net(Zin)

        return {
            'pose_body': matrot2aa(prec.view(-1, 3, 3)).view(bs, -1, 3),
            'pose_body_matrot': prec.view(bs, -1, 9)
        }


    def forward(self, pose_body):
        '''
        :param Pin: aa: Nx1xnum_jointsx3 / matrot: Nx1xnum_jointsx9
        :param input_type: matrot / aa for matrix rotations or axis angles
        :param output_type: matrot / aa
        :return:
        '''

        q_z = self.encode(pose_body)
        q_z_sample = q_z.rsample()
        decode_results = self.decode(q_z_sample)
        decode_results.update({'poZ_body_mean': q_z.mean, 'poZ_body_std': q_z.scale, 'q_z': q_z})
        return decode_results

    def sample_poses(self, num_poses, seed=None):
        np.random.seed(seed)

        some_weight = [a for a in self.parameters()][0]
        dtype = some_weight.dtype
        device = some_weight.device
        self.eval()
        with torch.no_grad():
            Zgen = torch.tensor(np.random.normal(0., 1., size=(num_poses, self.latentD)), dtype=dtype, device=device)

        return self.decode(Zgen)


D:\Projects\smplify-x\src\human-body-prior\src\human_body_prior\models\__init__.py
# -*- coding: utf-8 -*-
#
# Copyright (C) 2019 Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG),
# acting on behalf of its Max Planck Institute for Intelligent Systems and the
# Max Planck Institute for Biological Cybernetics. All rights reserved.
#
# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights
# on this computer program. You can only use this computer program if you have closed a license agreement
# with MPG or you get the right to use the computer program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and liable to prosecution.
# Contact: ps-license@tuebingen.mpg.de
#
#
# If you use this code in a research publication please consider citing the following:
#
# Expressive Body Capture: 3D Hands, Face, and Body from a Single Image <https://arxiv.org/abs/1904.05866>
#
#
# Code Developed by:
# Nima Ghorbani <https://nghorbani.github.io/>
#
# 2020.12.12

D:\Projects\smplify-x\src\human-body-prior\src\human_body_prior\tools\angle_continuous_repres.py
# -*- coding: utf-8 -*-
#
# Copyright (C) 2019 Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG),
# acting on behalf of its Max Planck Institute for Intelligent Systems and the
# Max Planck Institute for Biological Cybernetics. All rights reserved.
#
# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights
# on this computer program. You can only use this computer program if you have closed a license agreement
# with MPG or you get the right to use the computer program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and liable to prosecution.
# Contact: ps-license@tuebingen.mpg.de
#
#
# If you use this code in a research publication please consider citing the following:
#
# Expressive Body Capture: 3D Hands, Face, and Body from a Single Image <https://arxiv.org/abs/1904.05866>
#
#
# Code Developed by:
# Nima Ghorbani <https://nghorbani.github.io/>
#
# 2020.12.12
import torch.nn.functional as F
import torch
from torch import nn

import numpy as np

# numpy implementation of yi zhou's method
def norm(v):
    return v/np.linalg.norm(v)

def gs(M):
    a1 = M[:,0]
    a2 = M[:,1]
    b1 = norm(a1)
    b2 = norm((a2-np.dot(b1,a2)*b1))
    b3 = np.cross(b1,b2)
    return np.vstack([b1,b2,b3]).T

# input sz bszx3x2
def bgs(d6s):

    bsz = d6s.shape[0]
    b1 = F.normalize(d6s[:,:,0], p=2, dim=1)
    a2 = d6s[:,:,1]
    c = torch.bmm(b1.view(bsz,1,-1),a2.view(bsz,-1,1)).view(bsz,1)*b1
    b2 = F.normalize(a2-c,p=2,dim=1)
    b3=torch.cross(b1,b2,dim=1)
    return torch.stack([b1,b2,b3],dim=1).permute(0,2,1)


class geodesic_loss_R(nn.Module):
    def __init__(self, reduction='batchmean'):
        super(geodesic_loss_R, self).__init__()

        self.reduction = reduction
        self.eps = 1e-6

    # batch geodesic loss for rotation matrices
    def bgdR(self,m1,m2):
        batch = m1.shape[0]
        m = torch.bmm(m1, m2.transpose(1, 2))  # batch*3*3

        cos = (m[:, 0, 0] + m[:, 1, 1] + m[:, 2, 2] - 1) / 2
        cos = torch.min(cos, m1.new(np.ones(batch)))
        cos = torch.max(cos, m1.new(np.ones(batch)) * -1)

        return torch.acos(cos)

    def forward(self, ypred, ytrue):
        theta = self.bgdR(ypred,ytrue)
        if self.reduction == 'mean':
            return torch.mean(theta)
        if self.reduction == 'batchmean':
            breakpoint()
            return torch.mean(torch.sum(theta, dim=theta.shape[1:]))

        else:
            return theta

D:\Projects\smplify-x\src\human-body-prior\src\human_body_prior\tools\bodypart2vertexid.py
from psbody.mesh.meshviewer import MeshViewer
from psbody.mesh import Mesh
import numpy as np





def find_handVertexIDs(blend_weights, all_partIds, interested_partIds):

    segm = np.argmax(blend_weights, axis=1)               # n_vertex
    num2part = {v: k for k, v in all_partIds.items()}    # n_joints
    print(num2part)
    vert2part = [num2part[i] for i in segm]                 # 6890

    # handPartIDs = np.arange(20, len(num2part))      # SMPL+HH --> B
   #handPartIDs = [20] + range(22, 36 + 1)          # SMPL+HH --> L
   #handPartIDs = [21] + range(37, len(num2part))   # SMPL+HH --> R
   #handPartIDs = [15]  # head things
    #
   #handPartIDs = [20, 21] + np.arange(25, len(num2part)).tolist()   # SMPL+HF --> B
   #handPartIDs = [20] + np.arange(25, 40).tolist()                  # SMPL+HF --> L
   #handPartIDs = [21] + np.arange(40, len(num2part)).tolist()       # SMPL+HF --> R
   #handPartIDs = [15, 22, 23, 24]  # head things

    PartLABELs = [num2part[ii] for ii in interested_partIds]
    VertexIDs = [ii for ii in range(len(vert2part)) if vert2part[ii] in PartLABELs]

    # test
    allOK = all(vert2part[VertexID] in PartLABELs for VertexID in VertexIDs)
    print('allOK =', allOK)
    print('isSorted =', all(VertexIDs[i] <= VertexIDs[i+1] for i in range(len(VertexIDs)-1)))

    return VertexIDs

def smplx_part_ids():
    from human_body_prior.tools.omni_tools import copy2cpu as c2c
    from body_visualizer.tools.vis_tools import colors
    import torch

    from human_body_prior.body_model.body_model import BodyModel
    # all_partids = np.load('/ps/project/common/moshpp/smplx/part2num.npy', allow_pickle=True).tolist()
    bm = BodyModel(bm_fname='/ps/project/common/moshpp/smplx/locked_head/model_6_merged_exp_hands_fixed_eyes/neutral/model.npz')
    # bm = BodyModel(bm_fname='/ps/scratch/soma/support_files/smplx_downsampled/328/female/model.npz')

    # joints = np.load('/ps/project/common/moshpp/smplx/locked_head/model_6_merged_exp_hands_fixed_eyes/neutral/model.npz')['joints']
    joints = np.load('/ps/project/supercap/support_files/smplx/smplx_downsampled/328/female/model.npz')['joints']
    joints = torch.from_numpy(joints)

    # bm = BodyModel(bm_fname='/ps/scratch/common/moshpp/smplx/locked_head/model_6_merged_exp_hands_fixed_eyes/female/model.npz')
    # all_partids = np.load('/ps/project/common/moshpp/smplx/part2num.npy', allow_pickle=True).tolist()
    # print(all_partids)
    #
    # from psbody.smpl.serialization import load_model
    # model = load_model(fname_or_dict='/ps/body/projects/faces/fullbody_hand_head_models/SMPL+HF/trained_models/init_low_res_fixed_neck/male/model_0.pkl')  # not 6 !!!
    # model.part2num = part2num_body_hand_face

    # smplx_partids = {'body': [0,1,2,3,4,5,6,7,8,9,10,11,13,14,16,17,18,19],
    #                 'face': [15, 12, 22],
    #                 'eyeballs': [23, 24],
    #                 'hand': list(range(20,22)),
    #                 'finger': list(range(25, 55)),
    #                 'handR': [21] + list(range(40, 55))
    #                 }
    smplx_partids = {
                    'body': [0,1,2,3,4,5,6,9,13,14,16,17,18,19],
                    'face': [12, 15, 22],
                    'eyeball': [23, 24],
                     'leg': [4, 5, 7, 8, 10, 11],
                     'arm': [18, 19, 20, 21],
                    'handl': [20] + list(range(25, 40)),
                    'handr': [21] + list(range(40, 55)),
                    'footl': [7,10],
                    'footr': [8,11],
                     'ftip': [27, 30, 33, 36, 39, 42, 45, 48, 51, 54]

                     }

    all_partids = {}
    for bk, jids in smplx_partids.items():
        for jid in jids:
            all_partids['%s_%02d'%(bk, jid)] = jid
            print('%s_%02d'%(bk, jid))

    body_part_vc = {'body': colors['yellow'], 'arm': colors['orange'],'face': colors['green'],'leg': colors['green'],
                    'ftip':colors['white'],
                    'footl': colors['blue'],  'footr': colors['blue'],
                    'handl': colors['red'],  'handr': colors['orange'],

                    }

    part2vids = {}
    for partname, partids in smplx_partids.items():
        vertex_ids = find_handVertexIDs(c2c(bm.weights), all_partids, partids)
        vertex_ids = np.array(sorted(vertex_ids))
        part2vids[partname] = vertex_ids

    # body_v = c2c(bm().v[0])
    body_v = c2c(bm(joints=joints).v[0])

    part2vids['all'] = np.arange(0,body_v.shape[0])
    # part2vids.pop('eyeballs')

    np.savez('/ps/project/common/moshpp/smplx/part2vids.npz', **part2vids)
    # np.savez('/ps/scratch/soma/support_files/smplx_downsampled/328/part2vids_v2v_errs.npz', **part2vids)


    from psbody.mesh.meshviewer import MeshViewer
    mv = MeshViewer(keepalive=True)
    meshes = [Mesh(v=body_v[part2vids[partname]], f=[], vc=part_vc) for partname, part_vc in body_part_vc.items()]
    mv.set_static_meshes(meshes)

#
#
def smplh_part_ids():
    from human_body_prior.tools.omni_tools import copy2cpu as c2c
    from body_visualizer.tools.vis_tools import colors

    from human_body_prior.body_model.body_model import BodyModel
    bm = BodyModel(bm_fname='/ps/scratch/common/moshpp/smplh/locked_head/female/model.npz')

    smplx_partids = {'body': [0, 1, 2, 3, 4, 5, 6, 9, 13, 14, 16, 17, 18, 19,22,23,24,],
                     'face': [15, 12],
                     'handl': [20] + list(range(22, 37)),
                     'handr': [21] + list(range(37, 52)),
                     # 'leg': [4,5,7,8],
                     # 'arm': [18,19,20,21],
                     'footl': [7, 10],
                     'footr': [8, 11],
                     # 'finger': list(range(22, 52)),
                     }
    all_partids = {}
    for bk, jids in smplx_partids.items():
        for jid in jids:
            all_partids['%s_%02d' % (bk, jid)] = jid
            print('%s_%02d' % (bk, jid))

    body_part_vc = {k:v for k,v in {'body': colors['yellow'],
                                    # 'arm': colors['orange'],
                                    'face': colors['green'],
                                    # 'leg': colors['green'],
                    # 'ftipl':colors['white'],
                    # 'ftipr':colors['pink'],
                    'footl': colors['brown'],  'footr': colors['blue'],
                    'handl': colors['pink'],  'handr': colors['orange'],

                    }.items() if k in smplx_partids}
    part2vids = {}
    for partname, partids in smplx_partids.items():
        vertex_ids = find_handVertexIDs(c2c(bm.weights), all_partids, partids)
        vertex_ids = np.array(sorted(vertex_ids))
        part2vids[partname] = vertex_ids

    body_v = c2c(bm().v[0])

    part2vids['all'] = np.arange(0, body_v.shape[0])
    np.savez('/ps/scratch/common/moshpp/smplh/part2vids.npz', **part2vids)

    from psbody.mesh.meshviewer import MeshViewer
    mv = MeshViewer(keepalive=True)
    meshes = [Mesh(v=body_v[part2vids[partname]], f=[], vc=part_vc) for partname, part_vc in body_part_vc.items()]
    mv.set_static_meshes(meshes)
    mv.save_snapshot('/ps/scratch/common/moshpp/smplh/part2vids.jpeg')
# #
# def mano_part_ids():
#     from human_body_prior.tools.omni_tools import copy2cpu as c2c
#     from human_body_prior.tools.omni_tools import colors
#
#     from human_body_prior.body_model.body_model import BodyModel
#     bm = BodyModel(bm_fname='/ps/scratch/common/moshpp/mano/MANO_LEFT.npz')
#
#     smplx_partids = {'hand': [0, 1],
#                      'finger': [15,3,6,12,9,14,2,5,11,8],
#                      }
#     smplx_partids['all_others'] = list(set(range(16)).difference(set([i for v in list(smplx_partids.values()) for i in v])))
#     all_partids = {}
#     for bk, jids in smplx_partids.items():
#         for jid in jids:
#             all_partids['%s_%02d' % (bk, jid)] = jid
#             print('%s_%02d' % (bk, jid))
#
#     body_part_vc = {'all_others': colors['orange'], 'finger': colors['blue'], 'hand': colors['red']}
#
#     part2vids = {}
#     for partname, partids in smplx_partids.items():
#         vertex_ids = find_handVertexIDs(c2c(bm.weights), all_partids, partids)
#         vertex_ids = np.array(sorted(vertex_ids))
#         part2vids[partname] = vertex_ids
#
#     body_v = c2c(bm().v[0])
#
#     part2vids['all'] = np.arange(0, body_v.shape[0])
#     np.savez('/ps/scratch/common/moshpp/mano/part2vids.npz', **part2vids)
#
#     from psbody.mesh.meshviewer import MeshViewer
#     mv = MeshViewer(keepalive=True)
#     meshes = [Mesh(v=body_v[part2vids[partname]], f=[], vc=part_vc) for partname, part_vc in body_part_vc.items()]
#     mv.set_static_meshes(meshes)

if __name__ == '__main__':
    # smplx_part_ids()
    smplh_part_ids()
    # mano_part_ids()

    

D:\Projects\smplify-x\src\human-body-prior\src\human_body_prior\tools\configurations.py
# -*- coding: utf-8 -*-
#
# Copyright (C) 2019 Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG),
# acting on behalf of its Max Planck Institute for Intelligent Systems and the
# Max Planck Institute for Biological Cybernetics. All rights reserved.
#
# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights
# on this computer program. You can only use this computer program if you have closed a license agreement
# with MPG or you get the right to use the computer program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and liable to prosecution.
# Contact: ps-license@tuebingen.mpg.de
#
#
# If you use this code in a research publication please consider citing the following:
#
# Expressive Body Capture: 3D Hands, Face, and Body from a Single Image <https://arxiv.org/abs/1904.05866>
#
#
# Code Developed by:
# Nima Ghorbani <https://nghorbani.github.io/>
#
# 2020.12.12
from dotmap import DotMap
import os
import yaml

def load_config(default_ps_fname=None, **kwargs):
    if isinstance(default_ps_fname, str):
        assert os.path.exists(default_ps_fname), FileNotFoundError(default_ps_fname)
        assert default_ps_fname.lower().endswith('.yaml'), NotImplementedError('Only .yaml files are accepted.')
        default_ps = yaml.safe_load(open(default_ps_fname, 'r'))
    else:
        default_ps = {}

    default_ps.update(kwargs)

    return DotMap(default_ps, _dynamic=False)

def dump_config(data, fname):
    '''
    dump current configuration to an ini file
    :param fname:
    :return:
    '''
    with open(fname, 'w') as file:
        yaml.dump(data.toDict(), file)
    return fname


D:\Projects\smplify-x\src\human-body-prior\src\human_body_prior\tools\model_loader.py
# -*- coding: utf-8 -*-
#
# Copyright (C) 2019 Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG),
# acting on behalf of its Max Planck Institute for Intelligent Systems and the
# Max Planck Institute for Biological Cybernetics. All rights reserved.
#
# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights
# on this computer program. You can only use this computer program if you have closed a license agreement
# with MPG or you get the right to use the computer program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and liable to prosecution.
# Contact: ps-license@tuebingen.mpg.de
#
#
# If you use this code in a research publication please consider citing the following:
#
# Expressive Body Capture: 3D Hands, Face, and Body from a Single Image <https://arxiv.org/abs/1904.05866>
#
#
# Code Developed by: Nima Ghorbani <https://www.linkedin.com/in/nghorbani/>
# 2018.01.02

import glob
import os
import os.path as osp

from omegaconf import OmegaConf
from loguru import logger

def exprdir2model(expr_dir, model_cfg_override: dict = None):
    if not os.path.exists(expr_dir): raise ValueError(f'Could not find the experiment directory: {expr_dir}')

    model_snapshots_dir = osp.join(expr_dir, 'snapshots')
    available_ckpts = sorted(glob.glob(osp.join(model_snapshots_dir, '*.ckpt')), key=osp.getmtime)
    assert len(available_ckpts) > 0, ValueError('No checkpoint found at {}'.format(model_snapshots_dir))
    trained_weights_fname = available_ckpts[-1]

    model_cfg_fname = glob.glob(osp.join('/', '/'.join(trained_weights_fname.split('/')[:-2]), '*.yaml'))
    if len(model_cfg_fname) == 0:
        model_cfg_fname = glob.glob(osp.join('/'.join(trained_weights_fname.split('/')[:-2]), '*.yaml'))

    model_cfg_fname = model_cfg_fname[0]
    model_cfg = OmegaConf.load(model_cfg_fname)
    if model_cfg_override:
        override_cfg_dotlist = [f'{k}={v}' for k, v in model_cfg_override.items()]
        override_cfg = OmegaConf.from_dotlist(override_cfg_dotlist)
        model_cfg = OmegaConf.merge(model_cfg, override_cfg)

    return model_cfg, trained_weights_fname

def load_model(expr_dir, model_code=None,
               remove_words_in_model_weights: str = None,
               load_only_cfg: bool = False,
               disable_grad: bool = True,
               model_cfg_override: dict = None,
               comp_device='gpu'):
    """

    :param expr_dir:
    :param model_code: an imported module
    from supercap.train.supercap_smpl import SuperCap, then pass SuperCap to this function
    :param if True will load the model definition used for training, and not the one in current repository
    :return:
    """
    import torch

    model_cfg, trained_weights_fname = exprdir2model(expr_dir, model_cfg_override=model_cfg_override)

    if load_only_cfg: return model_cfg

    assert model_code is not None, ValueError('model_code should be provided')
    model_instance = model_code(model_cfg)
    if disable_grad:  # i had to do this. torch.no_grad() couldnt achieve what i was looking for
        for param in model_instance.parameters():
            param.requires_grad = False

    if comp_device=='cpu' or not torch.cuda.is_available():
        logger.info('No GPU detected. Loading on CPU!')
        state_dict = torch.load(trained_weights_fname, map_location=torch.device('cpu'))['state_dict']
    else:
        state_dict = torch.load(trained_weights_fname)['state_dict']
    if remove_words_in_model_weights is not None:
        words = '{}'.format(remove_words_in_model_weights)
        state_dict = {k.replace(words, '') if k.startswith(words) else k: v for k, v in state_dict.items()}

    ## keys that were in the model trained file and not in the current model
    instance_model_keys = list(model_instance.state_dict().keys())
    # trained_model_keys = list(state_dict.keys())
    # wts_in_model_not_in_file = set(instance_model_keys).difference(set(trained_model_keys))
    ## keys that are in the current model not in the training weights
    # wts_in_file_not_in_model = set(trained_model_keys).difference(set(instance_model_keys))
    # assert len(wts_in_model_not_in_file) == 0, ValueError('Some model weights are not present in the pretrained file. {}'.format(wts_in_model_not_in_file))

    state_dict = {k: v for k, v in state_dict.items() if k in instance_model_keys}
    model_instance.load_state_dict(state_dict,strict=False)
    # Todo fix the issues so that we can set the strict to true. The body model uses unnecessary registered buffers
    model_instance.eval()
    logger.info(f'Loaded model in eval mode with trained weights: {trained_weights_fname}')
    return model_instance,  model_cfg


D:\Projects\smplify-x\src\human-body-prior\src\human_body_prior\tools\omni_tools.py
# -*- coding: utf-8 -*-
#
# Copyright (C) 2019 Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG),
# acting on behalf of its Max Planck Institute for Intelligent Systems and the
# Max Planck Institute for Biological Cybernetics. All rights reserved.
#
# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights
# on this computer program. You can only use this computer program if you have closed a license agreement
# with MPG or you get the right to use the computer program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and liable to prosecution.
# Contact: ps-license@tuebingen.mpg.de
#
#
# If you use this code in a research publication please consider citing the following:
#
# Expressive Body Capture: 3D Hands, Face, and Body from a Single Image <https://arxiv.org/abs/1904.05866>
#
#
# Code Developed by:
# Nima Ghorbani <https://nghorbani.github.io/>
#
# 2018.01.02
import os
import os.path as osp
import random
import sys

import numpy as np
import torch


def copy2cpu(tensor):
    if isinstance(tensor, np.ndarray): return tensor
    return tensor.detach().cpu().numpy()


def create_list_chunks(list_, group_size, overlap_size, cut_smaller_batches=True):
    if cut_smaller_batches:
        return [list_[i:i + group_size] for i in range(0, len(list_), group_size - overlap_size) if
                len(list_[i:i + group_size]) == group_size]
    else:
        return [list_[i:i + group_size] for i in range(0, len(list_), group_size - overlap_size)]


def trainable_params_count(params):
    return sum([p.numel() for p in params if p.requires_grad])


def flatten_list(l):
    return [item for sublist in l for item in sublist]


def get_support_data_dir(current_fname=__file__):
    # print(current_fname)
    support_data_dir = osp.abspath(current_fname)
    support_data_dir_split = support_data_dir.split('/')
    # print(support_data_dir_split)
    try:
        support_data_dir = '/'.join(support_data_dir_split[:support_data_dir_split.index('src')])
    except:
        for i in range(len(support_data_dir_split)-1, 0, -1):
            support_data_dir = '/'.join(support_data_dir_split[:i])
            # print(i, support_data_dir)
            list_dir = os.listdir(support_data_dir)
            # print('-- ',list_dir)
            if 'support_data' in list_dir: break

    support_data_dir = osp.join(support_data_dir, 'support_data')
    assert osp.exists(support_data_dir)
    return support_data_dir


def make_deterministic(seed):
    random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def id_generator(size=13):
    import string
    import random
    chars = string.ascii_uppercase + string.digits
    return ''.join(random.choice(chars) for _ in range(size))


def logger_sequencer(logger_list, prefix=None):
    def post_text(text):
        if prefix is not None: text = '{} -- '.format(prefix) + text
        for logger_call in logger_list: logger_call(text)

    return post_text


class log2file():
    def __init__(self, logpath=None, prefix='', auto_newline=True, write2file_only=False):
        if logpath is not None:
            makepath(logpath, isfile=True)
            self.fhandle = open(logpath, 'a+')
        else:
            self.fhandle = None

        self.prefix = prefix
        self.auto_newline = auto_newline
        self.write2file_only = write2file_only

    def __call__(self, text):
        if text is None: return
        if self.prefix != '': text = '{} -- '.format(self.prefix) + text
        # breakpoint()
        if self.auto_newline:
            if not text.endswith('\n'):
                text = text + '\n'
        if not self.write2file_only: sys.stderr.write(text)
        if self.fhandle is not None:
            self.fhandle.write(text)
            self.fhandle.flush()


def makepath(*args, **kwargs):
    '''
    if the path does not exist make it
    :param desired_path: can be path to a file or a folder name
    :return:
    '''
    isfile = kwargs.get('isfile', False)
    import os
    desired_path = os.path.join(*args)
    if isfile:
        if not os.path.exists(os.path.dirname(desired_path)): os.makedirs(os.path.dirname(desired_path))
    else:
        if not os.path.exists(desired_path): os.makedirs(desired_path)
    return desired_path


def matrot2axisangle(matrots):
    '''
    :param matrots: N*T*num_joints*9
    :return: N*T*num_joints*3
    '''
    import cv2
    N = matrots.shape[0]
    T = matrots.shape[1]
    n_joints = matrots.shape[2]
    out_axisangle = []
    for tIdx in range(T):
        T_axisangle = []
        for mIdx in range(N):
            cur_axisangle = []
            for jIdx in range(n_joints):
                cur_axisangle.append(cv2.Rodrigues(matrots[mIdx, tIdx, jIdx:jIdx + 1, :].reshape(3, 3))[0].T)
            T_axisangle.append(np.vstack(cur_axisangle)[np.newaxis])
        out_axisangle.append(np.vstack(T_axisangle).reshape([N, 1, -1, 3]))
    return np.concatenate(out_axisangle, axis=1)


def axisangle2matrots(axisangle):
    '''
    :param matrots: N*1*num_joints*3
    :return: N*num_joints*9
    '''
    import cv2
    batch_size = axisangle.shape[0]
    axisangle = axisangle.reshape([batch_size, 1, -1, 3])
    out_matrot = []
    for mIdx in range(axisangle.shape[0]):
        cur_axisangle = []
        for jIdx in range(axisangle.shape[2]):
            a = cv2.Rodrigues(axisangle[mIdx, 0, jIdx:jIdx + 1, :].reshape(1, 3))[0].T
            cur_axisangle.append(a)

        out_matrot.append(np.array(cur_axisangle).reshape([batch_size, 1, -1, 9]))
    return np.vstack(out_matrot)


def apply_mesh_tranfsormations_(meshes, transf):
    '''
    apply inplace translations to meshes
    :param meshes: list of trimesh meshes
    :param transf:
    :return:
    '''
    for i in range(len(meshes)):
        meshes[i] = meshes[i].apply_transform(transf)


def rm_spaces(in_text): return in_text.replace(' ', '_')

D:\Projects\smplify-x\src\human-body-prior\src\human_body_prior\tools\rotation_tools.py
# -*- coding: utf-8 -*-
#
# Copyright (C) 2019 Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG),
# acting on behalf of its Max Planck Institute for Intelligent Systems and the
# Max Planck Institute for Biological Cybernetics. All rights reserved.
#
# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights
# on this computer program. You can only use this computer program if you have closed a license agreement
# with MPG or you get the right to use the computer program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and liable to prosecution.
# Contact: ps-license@tuebingen.mpg.de
#
#
# If you use this code in a research publication please consider citing the following:
#
# Expressive Body Capture: 3D Hands, Face, and Body from a Single Image <https://arxiv.org/abs/1904.05866>
#
#
# Code Developed by:
# Nima Ghorbani <https://nghorbani.github.io/>
#
# 2020.12.12


import numpy as np
import torch
from human_body_prior.tools import tgm_conversion as tgm
from torch.nn import functional as F


def local2global_pose(local_pose, kintree):
    bs = local_pose.shape[0]

    local_pose = local_pose.view(bs, -1, 3, 3)

    global_pose = local_pose.clone()

    for jId in range(len(kintree)):
        parent_id = kintree[jId]
        if parent_id >= 0:
            global_pose[:, jId] = torch.matmul(global_pose[:, parent_id], global_pose[:, jId])

    return global_pose


def em2euler(em):
    '''

    :param em: rotation in expo-map (3,)
    :return: rotation in euler angles (3,)
    '''
    from transforms3d.euler import axangle2euler

    theta = np.sqrt((em ** 2).sum())
    axis = em / theta
    return np.array(axangle2euler(axis, theta))


def euler2em(ea):
    '''

    :param ea: rotation in euler angles (3,)
    :return: rotation in expo-map (3,)
    '''
    from transforms3d.euler import euler2axangle
    axis, theta = euler2axangle(*ea)
    return np.array(axis * theta)


def remove_zrot(pose):
    noZ = em2euler(pose[:3].copy())
    noZ[2] = 0
    pose[:3] = euler2em(noZ).copy()
    return pose


def matrot2aa(pose_matrot):
    '''
    :param pose_matrot: Nx3x3
    :return: Nx3
    '''
    bs = pose_matrot.size(0)
    homogen_matrot = F.pad(pose_matrot, [0, 1])
    pose = tgm.rotation_matrix_to_angle_axis(homogen_matrot)
    return pose


def aa2matrot(pose):
    '''
    :param Nx3
    :return: pose_matrot: Nx3x3
    '''
    bs = pose.size(0)
    num_joints = pose.size(1) // 3
    pose_body_matrot = tgm.angle_axis_to_rotation_matrix(pose)[:, :3, :3].contiguous()  # .view(bs, num_joints*9)
    return pose_body_matrot


def noisy_zrot(rot_in):
    '''
    :param rot_in: np.array Nx3 rotations in axis-angle representation
    :return:
        randomize the zrotations and reutn in the same shape as input.
        the firt element t of T will be added a random angle and this addition will happen to all frames
    '''
    is_batched = False
    if rot_in.ndim == 2: is_batched = True
    if not is_batched:
        rot_in = rot_in[np.newaxis]

    rnd_zrot = np.random.uniform(-np.pi, np.pi)
    rot_out = []
    for bId in range(len(rot_in)):
        pose_cpu = rot_in[bId]
        pose_euler = em2euler(pose_cpu)

        pose_euler[2] += rnd_zrot

        pose_aa = euler2em(pose_euler)
        if np.any(np.isnan(pose_aa)):
            rot_out.append(pose_cpu)
        else:
            rot_out.append(pose_aa.copy())

    return np.array(rot_out)

from typing import Union, List


def rotate_points_xyz(mesh_v: np.ndarray, Rxyz: Union[List[int], np.ndarray]):
    '''

    :param mesh_v: Nxnum_vx3
    :param Rxyz: Nx3 or 3 in degrees
    :return:
    '''
    if Rxyz is not None:
        Rxyz = list(Rxyz)
        Rxyz = np.repeat(np.array(Rxyz).reshape(1, 3), repeats=len(mesh_v), axis=0)

    mesh_v_rotated = []

    for fId in range(mesh_v.shape[0]):
        angle = np.radians(Rxyz[fId, 0])
        rx = np.array([
            [1., 0., 0.],
            [0., np.cos(angle), -np.sin(angle)],
            [0., np.sin(angle), np.cos(angle)]
        ])

        angle = np.radians(Rxyz[fId, 1])
        ry = np.array([
            [np.cos(angle), 0., np.sin(angle)],
            [0., 1., 0.],
            [-np.sin(angle), 0., np.cos(angle)]
        ])

        angle = np.radians(Rxyz[fId, 2])
        rz = np.array([
            [np.cos(angle), -np.sin(angle), 0.],
            [np.sin(angle), np.cos(angle), 0.],
            [0., 0., 1.]
        ])
        mesh_v_rotated.append(rz.dot(ry.dot(rx.dot(mesh_v[fId].T))).T)

    return np.array(mesh_v_rotated)


def tmat(R, t):
    ''' Creates a batch of transformation matrices
        Args:
            - R: NxBx3x3 array of a batch of rotation matrices
            - t: NxBx3x1 array of a batch of translation vectors
        Returns:
            - T: Bx4x4 Transformation matrix
    '''
    # No padding left or right, only add an extra row

    bs = R.shape[0]

    return torch.cat([F.pad(R.view(-1, 3, 3), [0, 0, 0, 1]),
                      F.pad(t.view(-1, 3, 1), [0, 0, 0, 1], value=1)], dim=2).view(bs, -1, 4, 4)


def batch_rigid_transform(rot_mats, joints, parents):
    """
    Applies a batch of rigid transformations to the joints

    Parameters
    ----------
    rot_mats : torch.tensor BxNx3x3
        Tensor of rotation matrices
    joints : torch.tensor BxNx3
        Locations of joints
    parents : torch.tensor BxN
        The kinematic tree of each object
    dtype : torch.dtype, optional:
        The data type of the created tensors, the default is torch.float32

    Returns
    -------
    posed_joints : torch.tensor BxNx3
        The locations of the joints after applying the pose rotations
    rel_transforms : torch.tensor BxNx4x4
        The relative (with respect to the root joint) rigid transformations
        for all the joints
    """

    joints = torch.unsqueeze(joints, dim=-1)  # BxNx3X1

    rel_joints = joints.clone()
    rel_joints[:, 1:] -= joints[:, parents[1:]]

    transform_chain = [tmat(rot_mats[:, 0], rel_joints[:, 0])[:, 0]]
    for i in range(1, parents.shape[0]):
        # Subtract the joint location at the rest pose
        # No need for rotation, since it's identity when at rest
        curr_res = torch.matmul(transform_chain[parents[i]], tmat(rot_mats[:, i], rel_joints[:, i])[:, 0])
        transform_chain.append(curr_res)

    transforms = torch.stack(transform_chain, dim=1)

    # The last column of the transformations contains the posed joints
    posed_joints = transforms[:, :, :3, 3]

    return posed_joints


D:\Projects\smplify-x\src\human-body-prior\src\human_body_prior\tools\tgm_conversion.py
'''
This is a ripped code from an version of torchgeometry now called Kornia. Since Kornia has a
known bug: https://github.com/kornia/kornia/issues/317#issuecomment-751305910
in converting rotation representations we use this code until the original bug in Kornia is addressed
'''

import torch
import torch.nn as nn

__all__ = [
    # functional api
    "pi",
    "rad2deg",
    "deg2rad",
    "convert_points_from_homogeneous",
    "convert_points_to_homogeneous",
    "angle_axis_to_rotation_matrix",
    "rotation_matrix_to_angle_axis",
    "rotation_matrix_to_quaternion",
    "quaternion_to_angle_axis",
    "angle_axis_to_quaternion",
    "rtvec_to_pose",
    # layer api
    "RadToDeg",
    "DegToRad",
    "ConvertPointsFromHomogeneous",
    "ConvertPointsToHomogeneous",
]


"""Constant with number pi
"""
pi = torch.Tensor([3.14159265358979323846])


def rad2deg(tensor):
    r"""Function that converts angles from radians to degrees.

    See :class:`~torchgeometry.RadToDeg` for details.

    Args:
        tensor (Tensor): Tensor of arbitrary shape.

    Returns:
        Tensor: Tensor with same shape as input.

    Example:
        >>> input = tgm.pi * torch.rand(1, 3, 3)
        >>> output = tgm.rad2deg(input)
    """
    if not torch.is_tensor(tensor):
        raise TypeError("Input type is not a torch.Tensor. Got {}"
                        .format(type(tensor)))

    return 180. * tensor / pi.to(tensor.device).type(tensor.dtype)


def deg2rad(tensor):
    r"""Function that converts angles from degrees to radians.

    See :class:`~torchgeometry.DegToRad` for details.

    Args:
        tensor (Tensor): Tensor of arbitrary shape.

    Returns:
        Tensor: Tensor with same shape as input.

    Examples::

        >>> input = 360. * torch.rand(1, 3, 3)
        >>> output = tgm.deg2rad(input)
    """
    if not torch.is_tensor(tensor):
        raise TypeError("Input type is not a torch.Tensor. Got {}"
                        .format(type(tensor)))

    return tensor * pi.to(tensor.device).type(tensor.dtype) / 180.


def convert_points_from_homogeneous(points):
    r"""Function that converts points from homogeneous to Euclidean space.

    See :class:`~torchgeometry.ConvertPointsFromHomogeneous` for details.

    Examples::

        >>> input = torch.rand(2, 4, 3)  # BxNx3
        >>> output = tgm.convert_points_from_homogeneous(input)  # BxNx2
    """
    if not torch.is_tensor(points):
        raise TypeError("Input type is not a torch.Tensor. Got {}".format(
            type(points)))
    if len(points.shape) < 2:
        raise ValueError("Input must be at least a 2D tensor. Got {}".format(
            points.shape))

    return points[..., :-1] / points[..., -1:]


def convert_points_to_homogeneous(points):
    r"""Function that converts points from Euclidean to homogeneous space.

    See :class:`~torchgeometry.ConvertPointsToHomogeneous` for details.

    Examples::

        >>> input = torch.rand(2, 4, 3)  # BxNx3
        >>> output = tgm.convert_points_to_homogeneous(input)  # BxNx4
    """
    if not torch.is_tensor(points):
        raise TypeError("Input type is not a torch.Tensor. Got {}".format(
            type(points)))
    if len(points.shape) < 2:
        raise ValueError("Input must be at least a 2D tensor. Got {}".format(
            points.shape))

    return nn.functional.pad(points, (0, 1), "constant", 1.0)


def angle_axis_to_rotation_matrix(angle_axis):
    """Convert 3d vector of axis-angle rotation to 4x4 rotation matrix

    Args:
        angle_axis (Tensor): tensor of 3d vector of axis-angle rotations.

    Returns:
        Tensor: tensor of 4x4 rotation matrices.

    Shape:
        - Input: :math:`(N, 3)`
        - Output: :math:`(N, 4, 4)`

    Example:
        >>> input = torch.rand(1, 3)  # Nx3
        >>> output = tgm.angle_axis_to_rotation_matrix(input)  # Nx4x4
    """
    def _compute_rotation_matrix(angle_axis, theta2, eps=1e-6):
        # We want to be careful to only evaluate the square root if the
        # norm of the angle_axis vector is greater than zero. Otherwise
        # we get a division by zero.
        k_one = 1.0
        theta = torch.sqrt(theta2)
        wxyz = angle_axis / (theta + eps)
        wx, wy, wz = torch.chunk(wxyz, 3, dim=1)
        cos_theta = torch.cos(theta)
        sin_theta = torch.sin(theta)

        r00 = cos_theta + wx * wx * (k_one - cos_theta)
        r10 = wz * sin_theta + wx * wy * (k_one - cos_theta)
        r20 = -wy * sin_theta + wx * wz * (k_one - cos_theta)
        r01 = wx * wy * (k_one - cos_theta) - wz * sin_theta
        r11 = cos_theta + wy * wy * (k_one - cos_theta)
        r21 = wx * sin_theta + wy * wz * (k_one - cos_theta)
        r02 = wy * sin_theta + wx * wz * (k_one - cos_theta)
        r12 = -wx * sin_theta + wy * wz * (k_one - cos_theta)
        r22 = cos_theta + wz * wz * (k_one - cos_theta)
        rotation_matrix = torch.cat(
            [r00, r01, r02, r10, r11, r12, r20, r21, r22], dim=1)
        return rotation_matrix.view(-1, 3, 3)

    def _compute_rotation_matrix_taylor(angle_axis):
        rx, ry, rz = torch.chunk(angle_axis, 3, dim=1)
        k_one = torch.ones_like(rx)
        rotation_matrix = torch.cat(
            [k_one, -rz, ry, rz, k_one, -rx, -ry, rx, k_one], dim=1)
        return rotation_matrix.view(-1, 3, 3)

    # stolen from ceres/rotation.h

    _angle_axis = torch.unsqueeze(angle_axis, dim=1)
    theta2 = torch.matmul(_angle_axis, _angle_axis.transpose(1, 2))
    theta2 = torch.squeeze(theta2, dim=1)

    # compute rotation matrices
    rotation_matrix_normal = _compute_rotation_matrix(angle_axis, theta2)
    rotation_matrix_taylor = _compute_rotation_matrix_taylor(angle_axis)

    # create mask to handle both cases
    eps = 1e-6
    mask = (theta2 > eps).view(-1, 1, 1).to(theta2.device)
    mask_pos = (mask).type_as(theta2)
    mask_neg = (mask == False).type_as(theta2)  # noqa

    # create output pose matrix
    batch_size = angle_axis.shape[0]
    rotation_matrix = torch.eye(4).to(angle_axis.device).type_as(angle_axis)
    rotation_matrix = rotation_matrix.view(1, 4, 4).repeat(batch_size, 1, 1)
    # fill output matrix with masked values
    rotation_matrix[..., :3, :3] = \
        mask_pos * rotation_matrix_normal + mask_neg * rotation_matrix_taylor
    return rotation_matrix  # Nx4x4


def rtvec_to_pose(rtvec):
    """
    Convert axis-angle rotation and translation vector to 4x4 pose matrix

    Args:
        rtvec (Tensor): Rodrigues vector transformations

    Returns:
        Tensor: transformation matrices

    Shape:
        - Input: :math:`(N, 6)`
        - Output: :math:`(N, 4, 4)`

    Example:
        >>> input = torch.rand(3, 6)  # Nx6
        >>> output = tgm.rtvec_to_pose(input)  # Nx4x4
    """
    assert rtvec.shape[-1] == 6, 'rtvec=[rx, ry, rz, tx, ty, tz]'
    pose = angle_axis_to_rotation_matrix(rtvec[..., :3])
    pose[..., :3, 3] = rtvec[..., 3:]
    return pose


def rotation_matrix_to_angle_axis(rotation_matrix):
    """Convert 3x4 rotation matrix to Rodrigues vector

    Args:
        rotation_matrix (Tensor): rotation matrix.

    Returns:
        Tensor: Rodrigues vector transformation.

    Shape:
        - Input: :math:`(N, 3, 4)`
        - Output: :math:`(N, 3)`

    Example:
        >>> input = torch.rand(2, 3, 4)  # Nx4x4
        >>> output = tgm.rotation_matrix_to_angle_axis(input)  # Nx3
    """
    # todo add check that matrix is a valid rotation matrix
    quaternion = rotation_matrix_to_quaternion(rotation_matrix)
    return quaternion_to_angle_axis(quaternion)


def rotation_matrix_to_quaternion(rotation_matrix, eps=1e-6):
    """Convert 3x4 rotation matrix to 4d quaternion vector

    This algorithm is based on algorithm described in
    https://github.com/KieranWynn/pyquaternion/blob/master/pyquaternion/quaternion.py#L201

    Args:
        rotation_matrix (Tensor): the rotation matrix to convert.

    Return:
        Tensor: the rotation in quaternion

    Shape:
        - Input: :math:`(N, 3, 4)`
        - Output: :math:`(N, 4)`

    Example:
        >>> input = torch.rand(4, 3, 4)  # Nx3x4
        >>> output = tgm.rotation_matrix_to_quaternion(input)  # Nx4
    """
    if not torch.is_tensor(rotation_matrix):
        raise TypeError("Input type is not a torch.Tensor. Got {}".format(
            type(rotation_matrix)))

    if len(rotation_matrix.shape) > 3:
        raise ValueError(
            "Input size must be a three dimensional tensor. Got {}".format(
                rotation_matrix.shape))
    if not rotation_matrix.shape[-2:] == (3, 4):
        raise ValueError(
            "Input size must be a N x 3 x 4  tensor. Got {}".format(
                rotation_matrix.shape))

    rmat_t = torch.transpose(rotation_matrix, 1, 2)

    mask_d2 = rmat_t[:, 2, 2] < eps

    mask_d0_d1 = rmat_t[:, 0, 0] > rmat_t[:, 1, 1]
    mask_d0_nd1 = rmat_t[:, 0, 0] < -rmat_t[:, 1, 1]

    t0 = 1 + rmat_t[:, 0, 0] - rmat_t[:, 1, 1] - rmat_t[:, 2, 2]
    q0 = torch.stack([rmat_t[:, 1, 2] - rmat_t[:, 2, 1],
                      t0, rmat_t[:, 0, 1] + rmat_t[:, 1, 0],
                      rmat_t[:, 2, 0] + rmat_t[:, 0, 2]], -1)
    t0_rep = t0.repeat(4, 1).t()

    t1 = 1 - rmat_t[:, 0, 0] + rmat_t[:, 1, 1] - rmat_t[:, 2, 2]
    q1 = torch.stack([rmat_t[:, 2, 0] - rmat_t[:, 0, 2],
                      rmat_t[:, 0, 1] + rmat_t[:, 1, 0],
                      t1, rmat_t[:, 1, 2] + rmat_t[:, 2, 1]], -1)
    t1_rep = t1.repeat(4, 1).t()

    t2 = 1 - rmat_t[:, 0, 0] - rmat_t[:, 1, 1] + rmat_t[:, 2, 2]
    q2 = torch.stack([rmat_t[:, 0, 1] - rmat_t[:, 1, 0],
                      rmat_t[:, 2, 0] + rmat_t[:, 0, 2],
                      rmat_t[:, 1, 2] + rmat_t[:, 2, 1], t2], -1)
    t2_rep = t2.repeat(4, 1).t()

    t3 = 1 + rmat_t[:, 0, 0] + rmat_t[:, 1, 1] + rmat_t[:, 2, 2]
    q3 = torch.stack([t3, rmat_t[:, 1, 2] - rmat_t[:, 2, 1],
                      rmat_t[:, 2, 0] - rmat_t[:, 0, 2],
                      rmat_t[:, 0, 1] - rmat_t[:, 1, 0]], -1)
    t3_rep = t3.repeat(4, 1).t()

    mask_c0 = mask_d2 * mask_d0_d1
    mask_c1 = mask_d2 * torch.logical_not(mask_d0_d1)
    mask_c2 = torch.logical_not(mask_d2) * mask_d0_nd1
    mask_c3 = torch.logical_not(mask_d2) * torch.logical_not(mask_d0_nd1)
    mask_c0 = mask_c0.view(-1, 1).type_as(q0)
    mask_c1 = mask_c1.view(-1, 1).type_as(q1)
    mask_c2 = mask_c2.view(-1, 1).type_as(q2)
    mask_c3 = mask_c3.view(-1, 1).type_as(q3)

    q = q0 * mask_c0 + q1 * mask_c1 + q2 * mask_c2 + q3 * mask_c3
    q /= torch.sqrt(t0_rep * mask_c0 + t1_rep * mask_c1 +  # noqa
                    t2_rep * mask_c2 + t3_rep * mask_c3)  # noqa
    q *= 0.5
    return q


def quaternion_to_angle_axis(quaternion) -> torch.Tensor:
    """Convert quaternion vector to angle axis of rotation.

    Adapted from ceres C++ library: ceres-solver/include/ceres/rotation.h

    Args:
        quaternion (torch.Tensor): tensor with quaternions.

    Return:
        torch.Tensor: tensor with angle axis of rotation.

    Shape:
        - Input: :math:`(*, 4)` where `*` means, any number of dimensions
        - Output: :math:`(*, 3)`

    Example:
        >>> quaternion = torch.rand(2, 4)  # Nx4
        >>> angle_axis = tgm.quaternion_to_angle_axis(quaternion)  # Nx3
    """
    if not torch.is_tensor(quaternion):
        raise TypeError("Input type is not a torch.Tensor. Got {}".format(
            type(quaternion)))

    if not quaternion.shape[-1] == 4:
        raise ValueError("Input must be a tensor of shape Nx4 or 4. Got {}"
                         .format(quaternion.shape))
    # unpack input and compute conversion
    q1 = quaternion[..., 1]
    q2 = quaternion[..., 2]
    q3 = quaternion[..., 3]
    sin_squared_theta = q1 * q1 + q2 * q2 + q3 * q3

    sin_theta = torch.sqrt(sin_squared_theta)
    cos_theta = quaternion[..., 0]
    two_theta = 2.0 * torch.where(
        cos_theta < 0.0,
        torch.atan2(-sin_theta, -cos_theta),
        torch.atan2(sin_theta, cos_theta))

    k_pos = two_theta / sin_theta
    k_neg = 2.0 * torch.ones_like(sin_theta)
    k = torch.where(sin_squared_theta > 0.0, k_pos, k_neg)

    angle_axis = torch.zeros_like(quaternion)[..., :3]
    angle_axis[..., 0] += q1 * k
    angle_axis[..., 1] += q2 * k
    angle_axis[..., 2] += q3 * k
    return angle_axis

# based on:
# https://github.com/facebookresearch/QuaterNet/blob/master/common/quaternion.py#L138


def angle_axis_to_quaternion(angle_axis) -> torch.Tensor:
    """Convert an angle axis to a quaternion.

    Adapted from ceres C++ library: ceres-solver/include/ceres/rotation.h

    Args:
        angle_axis (torch.Tensor): tensor with angle axis.

    Return:
        torch.Tensor: tensor with quaternion.

    Shape:
        - Input: :math:`(*, 3)` where `*` means, any number of dimensions
        - Output: :math:`(*, 4)`

    Example:
        >>> angle_axis = torch.rand(2, 4)  # Nx4
        >>> quaternion = tgm.angle_axis_to_quaternion(angle_axis)  # Nx3
    """
    if not torch.is_tensor(angle_axis):
        raise TypeError("Input type is not a torch.Tensor. Got {}".format(
            type(angle_axis)))

    if not angle_axis.shape[-1] == 3:
        raise ValueError("Input must be a tensor of shape Nx3 or 3. Got {}"
                         .format(angle_axis.shape))
    # unpack input and compute conversion
    a0 = angle_axis[..., 0:1]
    a1 = angle_axis[..., 1:2]
    a2 = angle_axis[..., 2:3]
    theta_squared = a0 * a0 + a1 * a1 + a2 * a2

    theta = torch.sqrt(theta_squared)
    half_theta = theta * 0.5

    mask = theta_squared > 0.0
    ones = torch.ones_like(half_theta)

    k_neg = 0.5 * ones
    k_pos = torch.sin(half_theta) / theta
    k = torch.where(mask, k_pos, k_neg)
    w = torch.where(mask, torch.cos(half_theta), ones)

    quaternion = torch.zeros_like(angle_axis)
    quaternion[..., 0:1] += a0 * k
    quaternion[..., 1:2] += a1 * k
    quaternion[..., 2:3] += a2 * k
    return torch.cat([w, quaternion], dim=-1)

# TODO: add below funtionalities
#  - pose_to_rtvec


# layer api


class RadToDeg(nn.Module):
    r"""Creates an object that converts angles from radians to degrees.

    Args:
        tensor (Tensor): Tensor of arbitrary shape.

    Returns:
        Tensor: Tensor with same shape as input.

    Examples::

        >>> input = tgm.pi * torch.rand(1, 3, 3)
        >>> output = tgm.RadToDeg()(input)
    """

    def __init__(self):
        super(RadToDeg, self).__init__()

    def forward(self, input):
        return rad2deg(input)


class DegToRad(nn.Module):
    r"""Function that converts angles from degrees to radians.

    Args:
        tensor (Tensor): Tensor of arbitrary shape.

    Returns:
        Tensor: Tensor with same shape as input.

    Examples::

        >>> input = 360. * torch.rand(1, 3, 3)
        >>> output = tgm.DegToRad()(input)
    """

    def __init__(self):
        super(DegToRad, self).__init__()

    def forward(self, input):
        return deg2rad(input)


class ConvertPointsFromHomogeneous(nn.Module):
    r"""Creates a transformation that converts points from homogeneous to
    Euclidean space.

    Args:
        points (Tensor): tensor of N-dimensional points.

    Returns:
        Tensor: tensor of N-1-dimensional points.

    Shape:
        - Input: :math:`(B, D, N)` or :math:`(D, N)`
        - Output: :math:`(B, D, N + 1)` or :math:`(D, N + 1)`

    Examples::

        >>> input = torch.rand(2, 4, 3)  # BxNx3
        >>> transform = tgm.ConvertPointsFromHomogeneous()
        >>> output = transform(input)  # BxNx2
    """

    def __init__(self):
        super(ConvertPointsFromHomogeneous, self).__init__()

    def forward(self, input):
        return convert_points_from_homogeneous(input)


class ConvertPointsToHomogeneous(nn.Module):
    r"""Creates a transformation to convert points from Euclidean to
    homogeneous space.

    Args:
        points (Tensor): tensor of N-dimensional points.

    Returns:
        Tensor: tensor of N+1-dimensional points.

    Shape:
        - Input: :math:`(B, D, N)` or :math:`(D, N)`
        - Output: :math:`(B, D, N + 1)` or :math:`(D, N + 1)`

    Examples::

        >>> input = torch.rand(2, 4, 3)  # BxNx3
        >>> transform = tgm.ConvertPointsToHomogeneous()
        >>> output = transform(input)  # BxNx4
    """

    def __init__(self):
        super(ConvertPointsToHomogeneous, self).__init__()

    def forward(self, input):
        return convert_points_to_homogeneous(input)

D:\Projects\smplify-x\src\human-body-prior\src\human_body_prior\tools\__init__.py
# -*- coding: utf-8 -*-
#
# Copyright (C) 2019 Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG),
# acting on behalf of its Max Planck Institute for Intelligent Systems and the
# Max Planck Institute for Biological Cybernetics. All rights reserved.
#
# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights
# on this computer program. You can only use this computer program if you have closed a license agreement
# with MPG or you get the right to use the computer program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and liable to prosecution.
# Contact: ps-license@tuebingen.mpg.de
#
#
# If you use this code in a research publication please consider citing the following:
#
# Expressive Body Capture: 3D Hands, Face, and Body from a Single Image <https://arxiv.org/abs/1904.05866>
#
#
# Code Developed by:
# Nima Ghorbani <https://nghorbani.github.io/>
#
# 2020.12.12


D:\Projects\smplify-x\src\human-body-prior\src\human_body_prior\train\README.md
# Train VPoser from Scratch
To train your own VPoser with new configuration duplicate the provided **V02_05** folder while setting a new experiment ID 
and change the settings as you desire. 
First you would need to download the 
[AMASS](https://amass.is.tue.mpg.de/) dataset, then following the [data preparation tutorial](../data/README.md)
prepare the data for training. 
Following is a code snippet for training that can be found in the [example training experiment](https://github.com/nghorbani/human_body_prior/blob/master/src/human_body_prior/train/V02_05/V02_05.py):

```python
import glob
import os.path as osp

from human_body_prior.tools.configurations import load_config
from human_body_prior.train.vposer_trainer import train_vposer_once

def main():
    expr_id = 'V02_05'

    default_ps_fname = glob.glob(osp.join(osp.dirname(__file__), '*.yaml'))[0]

    vp_ps = load_config(default_ps_fname)

    vp_ps.train_parms.batch_size = 128

    vp_ps.general.expr_id = expr_id

    total_jobs = []
    total_jobs.append(vp_ps.toDict().copy())

    print('#training_jobs to be done: {}'.format(len(total_jobs)))
    if len(total_jobs) == 0:
        print('No jobs to be done')
        return

    for job in total_jobs:
        train_vposer_once(job)
``` 
The above code uses yaml configuration files to handle experiment settings. 
It loads the default settings in *<expr_id>.yaml* and overloads it with your new args. 

The training code, will dump a log file along with tensorboard readable events file.

D:\Projects\smplify-x\src\human-body-prior\src\human_body_prior\train\V02_05\V02_05.py
# -*- coding: utf-8 -*-
#
# Copyright (C) 2019 Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG),
# acting on behalf of its Max Planck Institute for Intelligent Systems and the
# Max Planck Institute for Biological Cybernetics. All rights reserved.
#
# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights
# on this computer program. You can only use this computer program if you have closed a license agreement
# with MPG or you get the right to use the computer program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and liable to prosecution.
# Contact: ps-license@tuebingen.mpg.de
#
#
# If you use this code in a research publication please consider citing the following:
#
# Expressive Body Capture: 3D Hands, Face, and Body from a Single Image <https://arxiv.org/abs/1904.05866>
#
#
# Code Developed by:
# Nima Ghorbani <https://nghorbani.github.io/>
#
# 2020.12.12

import glob
import os.path as osp

from human_body_prior.tools.configurations import load_config
from human_body_prior.train.vposer_trainer import train_vposer_once

def main():
    expr_id = 'V02_05'

    default_ps_fname = glob.glob(osp.join(osp.dirname(__file__), '*.yaml'))[0]

    vp_ps = load_config(default_ps_fname)

    vp_ps.train_parms.batch_size = 128

    vp_ps.general.expr_id = expr_id

    total_jobs = []
    total_jobs.append(vp_ps.toDict().copy())

    print('#training_jobs to be done: {}'.format(len(total_jobs)))
    if len(total_jobs) == 0:
        print('No jobs to be done')
        return

    for job in total_jobs:
        train_vposer_once(job)


if __name__ == '__main__':
    main()

D:\Projects\smplify-x\src\human-body-prior\src\human_body_prior\train\V02_05\__init__.py
# -*- coding: utf-8 -*-
#
# Copyright (C) 2019 Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG),
# acting on behalf of its Max Planck Institute for Intelligent Systems and the
# Max Planck Institute for Biological Cybernetics. All rights reserved.
#
# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights
# on this computer program. You can only use this computer program if you have closed a license agreement
# with MPG or you get the right to use the computer program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and liable to prosecution.
# Contact: ps-license@tuebingen.mpg.de
#
#
# If you use this code in a research publication please consider citing the following:
#
# Expressive Body Capture: 3D Hands, Face, and Body from a Single Image <https://arxiv.org/abs/1904.05866>
#
#
# Code Developed by:
# Nima Ghorbani <https://nghorbani.github.io/>
#
# 2020.12.12


D:\Projects\smplify-x\src\human-body-prior\src\human_body_prior\train\vposer_trainer.py
# -*- coding: utf-8 -*-
#
# Copyright (C) 2019 Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG),
# acting on behalf of its Max Planck Institute for Intelligent Systems and the
# Max Planck Institute for Biological Cybernetics. All rights reserved.
#
# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights
# on this computer program. You can only use this computer program if you have closed a license agreement
# with MPG or you get the right to use the computer program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and liable to prosecution.
# Contact: ps-license@tuebingen.mpg.de
#
#
# If you use this code in a research publication please consider citing the following:
#
# Expressive Body Capture: 3D Hands, Face, and Body from a Single Image <https://arxiv.org/abs/1904.05866>
#
#
# Code Developed by:
# Nima Ghorbani <https://nghorbani.github.io/>
#
# 2020.12.12

# from pytorch_lightning import Trainer

import glob
import os
import os.path as osp
from datetime import datetime as dt
from pytorch_lightning.plugins import DDPPlugin

import numpy as np
import pytorch_lightning as pl
import torch
from human_body_prior.body_model.body_model import BodyModel
from human_body_prior.data.dataloader import VPoserDS
from human_body_prior.data.prepare_data import dataset_exists
from human_body_prior.data.prepare_data import prepare_vposer_datasets
from human_body_prior.models.vposer_model import VPoser
from human_body_prior.tools.angle_continuous_repres import geodesic_loss_R
from human_body_prior.tools.configurations import load_config, dump_config
from human_body_prior.tools.omni_tools import copy2cpu as c2c
from human_body_prior.tools.omni_tools import get_support_data_dir
from human_body_prior.tools.omni_tools import log2file
from human_body_prior.tools.omni_tools import make_deterministic
from human_body_prior.tools.omni_tools import makepath
from human_body_prior.tools.rotation_tools import aa2matrot
from human_body_prior.visualizations.training_visualization import vposer_trainer_renderer
from pytorch_lightning.callbacks import LearningRateMonitor
from pytorch_lightning.callbacks.early_stopping import EarlyStopping

from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint
from pytorch_lightning.core import LightningModule
from pytorch_lightning.loggers import TensorBoardLogger
from pytorch_lightning.utilities import rank_zero_only
from torch import optim as optim_module
from torch.optim import lr_scheduler as lr_sched_module
from torch.utils.data import DataLoader


class VPoserTrainer(LightningModule):
    """

    It includes all data loading and train / val logic., and it is used for both training and testing models.
    """

    def __init__(self, _config):
        super(VPoserTrainer, self).__init__()

        _support_data_dir = get_support_data_dir()

        vp_ps = load_config(**_config)

        make_deterministic(vp_ps.general.rnd_seed)

        self.expr_id = vp_ps.general.expr_id
        self.dataset_id = vp_ps.general.dataset_id

        self.work_dir = vp_ps.logging.work_dir = makepath(vp_ps.general.work_basedir, self.expr_id)
        self.dataset_dir = vp_ps.logging.dataset_dir = osp.join(vp_ps.general.dataset_basedir, vp_ps.general.dataset_id)

        self._log_prefix = '[{}]'.format(self.expr_id)
        self.text_logger = log2file(prefix=self._log_prefix)

        self.seq_len = vp_ps.data_parms.num_timeseq_frames

        self.vp_model = VPoser(vp_ps)

        with torch.no_grad():

            self.bm_train = BodyModel(vp_ps.body_model.bm_fname)

        if vp_ps.logging.render_during_training:
            self.renderer = vposer_trainer_renderer(self.bm_train, vp_ps.logging.num_bodies_to_display)
        else:
            self.renderer = None

        self.example_input_array = {'pose_body':torch.ones(vp_ps.train_parms.batch_size, 63),}
        self.vp_ps = vp_ps

    def forward(self, pose_body):

        return self.vp_model(pose_body)

    def _get_data(self, split_name):

        assert split_name in ('train', 'vald', 'test')

        split_name = split_name.replace('vald', 'vald')

        assert dataset_exists(self.dataset_dir), FileNotFoundError('Dataset does not exist dataset_dir = {}'.format(self.dataset_dir))
        dataset = VPoserDS(osp.join(self.dataset_dir, split_name), data_fields = ['pose_body'])

        assert len(dataset) != 0, ValueError('Dataset has nothing in it!')

        return DataLoader(dataset,
                          batch_size=self.vp_ps.train_parms.batch_size,
                          shuffle=True if split_name == 'train' else False,
                          num_workers=self.vp_ps.data_parms.num_workers,
                          pin_memory=True)

    @rank_zero_only
    def on_train_start(self):
        if self.global_rank != 0: return
        self.train_starttime = dt.now().replace(microsecond=0)

        ######## make a backup of vposer
        git_repo_dir = os.path.abspath(__file__).split('/')
        git_repo_dir = '/'.join(git_repo_dir[:git_repo_dir.index('human_body_prior') + 1])
        starttime = dt.strftime(self.train_starttime, '%Y_%m_%d_%H_%M_%S')
        archive_path = makepath(self.work_dir, 'code', 'vposer_{}.tar.gz'.format(starttime), isfile=True)
        cmd = 'cd %s && git ls-files -z | xargs -0 tar -czf %s' % (git_repo_dir, archive_path)
        os.system(cmd)
        ########
        self.text_logger('Created a git archive backup at {}'.format(archive_path))
        dump_config(self.vp_ps, osp.join(self.work_dir, '{}.yaml'.format(self.expr_id)))

    def train_dataloader(self):
        return self._get_data('train')

    def val_dataloader(self):
        return self._get_data('vald')

    def configure_optimizers(self):
        params_count = lambda params: sum(p.numel() for p in params if p.requires_grad)

        gen_params = [a[1] for a in self.vp_model.named_parameters() if a[1].requires_grad]
        gen_optimizer_class = getattr(optim_module, self.vp_ps.train_parms.gen_optimizer.type)
        gen_optimizer = gen_optimizer_class(gen_params, **self.vp_ps.train_parms.gen_optimizer.args)

        self.text_logger('Total Trainable Parameters Count in vp_model is %2.2f M.' % (params_count(gen_params) * 1e-6))

        lr_sched_class = getattr(lr_sched_module, self.vp_ps.train_parms.lr_scheduler.type)

        gen_lr_scheduler = lr_sched_class(gen_optimizer, **self.vp_ps.train_parms.lr_scheduler.args)

        schedulers = [
            {
                'scheduler': gen_lr_scheduler,
                'monitor': 'val_loss',
                'interval': 'epoch',
                'frequency': 1
            },
        ]
        return [gen_optimizer], schedulers

    def _compute_loss(self, dorig, drec):
        l1_loss = torch.nn.L1Loss(reduction='mean')
        geodesic_loss = geodesic_loss_R(reduction='mean')

        bs, latentD = drec['poZ_body_mean'].shape
        device = drec['poZ_body_mean'].device

        loss_kl_wt = self.vp_ps.train_parms.loss_weights.loss_kl_wt
        loss_rec_wt = self.vp_ps.train_parms.loss_weights.loss_rec_wt
        loss_matrot_wt = self.vp_ps.train_parms.loss_weights.loss_matrot_wt
        loss_jtr_wt = self.vp_ps.train_parms.loss_weights.loss_jtr_wt

        # q_z = torch.distributions.normal.Normal(drec['mean'], drec['std'])
        q_z = drec['q_z']
        # dorig['fullpose'] = torch.cat([dorig['root_orient'], dorig['pose_body']], dim=-1)

        # Reconstruction loss - L1 on the output mesh
        with torch.no_grad():
            bm_orig = self.bm_train(pose_body=dorig['pose_body'])

        bm_rec = self.bm_train(pose_body=drec['pose_body'].contiguous().view(bs, -1))

        v2v = l1_loss(bm_rec.v, bm_orig.v)

        # KL loss
        p_z = torch.distributions.normal.Normal(
            loc=torch.zeros((bs, latentD), device=device, requires_grad=False),
            scale=torch.ones((bs, latentD), device=device, requires_grad=False))
        weighted_loss_dict = {
            'loss_kl':loss_kl_wt * torch.mean(torch.sum(torch.distributions.kl.kl_divergence(q_z, p_z), dim=[1])),
            'loss_mesh_rec': loss_rec_wt * v2v
        }

        if (self.current_epoch < self.vp_ps.train_parms.keep_extra_loss_terms_until_epoch):
            # breakpoint()
            weighted_loss_dict['matrot'] = loss_matrot_wt * geodesic_loss(drec['pose_body_matrot'].view(-1,3,3), aa2matrot(dorig['pose_body'].view(-1, 3)))
            weighted_loss_dict['jtr'] = loss_jtr_wt * l1_loss(bm_rec.Jtr, bm_orig.Jtr)

        weighted_loss_dict['loss_total'] = torch.stack(list(weighted_loss_dict.values())).sum()

        with torch.no_grad():
            unweighted_loss_dict = {'v2v': torch.sqrt(torch.pow(bm_rec.v-bm_orig.v, 2).sum(-1)).mean()}
            unweighted_loss_dict['loss_total'] = torch.cat(
                list({k: v.view(-1) for k, v in unweighted_loss_dict.items()}.values()), dim=-1).sum().view(1)

        return {'weighted_loss': weighted_loss_dict, 'unweighted_loss': unweighted_loss_dict}

    def training_step(self, batch, batch_idx, optimizer_idx=None):

        drec = self(batch['pose_body'].view(-1, 63))

        loss = self._compute_loss(batch, drec)

        train_loss = loss['weighted_loss']['loss_total']

        tensorboard_logs = {'train_loss': train_loss}
        progress_bar = {k: c2c(v) for k, v in loss['weighted_loss'].items()}
        return {'loss': train_loss, 'progress_bar':progress_bar,  'log': tensorboard_logs}

    def validation_step(self, batch, batch_idx):

        drec = self(batch['pose_body'].view(-1, 63))

        loss = self._compute_loss(batch, drec)
        val_loss = loss['unweighted_loss']['loss_total']

        if self.renderer is not None and self.global_rank == 0 and batch_idx % 500==0 and np.random.rand()>0.5:
            out_fname = makepath(self.work_dir, 'renders/vald_rec_E{:03d}_It{:04d}_val_loss_{:.2f}.png'.format(self.current_epoch, batch_idx, val_loss.item()), isfile=True)
            self.renderer([batch, drec], out_fname = out_fname)
            dgen = self.vp_model.sample_poses(self.vp_ps.logging.num_bodies_to_display)
            out_fname = makepath(self.work_dir, 'renders/vald_gen_E{:03d}_I{:04d}.png'.format(self.current_epoch, batch_idx), isfile=True)
            self.renderer([dgen], out_fname = out_fname)


        progress_bar = {'v2v': val_loss}
        return {'val_loss': c2c(val_loss), 'progress_bar': progress_bar, 'log': progress_bar}

    def validation_epoch_end(self, outputs):
        metrics = {'val_loss': np.nanmean(np.concatenate([v['val_loss'] for v in outputs])) }

        if self.global_rank == 0:

            self.text_logger('Epoch {}: {}'.format(self.current_epoch, ', '.join('{}:{:.2f}'.format(k, v) for k, v in metrics.items())))
            self.text_logger('lr is {}'.format([pg['lr'] for opt in self.trainer.optimizers for pg in opt.param_groups]))

        metrics = {k: torch.as_tensor(v) for k, v in metrics.items()}

        return {'val_loss': metrics['val_loss'], 'log': metrics}


    @rank_zero_only
    def on_train_end(self):

        self.train_endtime = dt.now().replace(microsecond=0)
        endtime = dt.strftime(self.train_endtime, '%Y_%m_%d_%H_%M_%S')
        elapsedtime = self.train_endtime - self.train_starttime
        self.vp_ps.logging.best_model_fname = self.trainer.checkpoint_callback.best_model_path

        self.text_logger('Epoch {} - Finished training at {} after {}'.format(self.current_epoch, endtime, elapsedtime))
        self.text_logger('best_model_fname: {}'.format(self.vp_ps.logging.best_model_fname))

        dump_config(self.vp_ps, osp.join(self.work_dir, '{}_{}.yaml'.format(self.expr_id, self.dataset_id)))
        self.hparams = self.vp_ps.toDict()

    @rank_zero_only
    def prepare_data(self):
        '''' Similar to standard AMASS dataset preparation pipeline:
        Donwload npz file, corresponding to body data from https://amass.is.tue.mpg.de/ and place them under amass_dir
        '''
        self.text_logger = log2file(makepath(self.work_dir, '{}.log'.format(self.expr_id), isfile=True), prefix=self._log_prefix)

        prepare_vposer_datasets(self.dataset_dir, self.vp_ps.data_parms.amass_splits, self.vp_ps.data_parms.amass_dir, logger=self.text_logger)


def create_expr_message(ps):
    expr_msg = '[{}] batch_size = {}.'.format(ps.general.expr_id, ps.train_parms.batch_size)

    return expr_msg


def train_vposer_once(_config):

    resume_training_if_possible = True

    model = VPoserTrainer(_config)
    model.vp_ps.logging.expr_msg = create_expr_message(model.vp_ps)
    # model.text_logger(model.vp_ps.logging.expr_msg.replace(". ", '.\n'))
    dump_config(model.vp_ps, osp.join(model.work_dir, '{}.yaml'.format(model.expr_id)))

    logger = TensorBoardLogger(model.work_dir, name='tensorboard')
    lr_monitor = LearningRateMonitor()

    snapshots_dir = osp.join(model.work_dir, 'snapshots')
    checkpoint_callback = ModelCheckpoint(
        dirpath=makepath(snapshots_dir, isfile=True),
        filename="%s_{epoch:02d}_{val_loss:.2f}" % model.expr_id,
        save_top_k=1,
        verbose=True,
        monitor='val_loss',
        mode='min',
    )
    early_stop_callback = EarlyStopping(**model.vp_ps.train_parms.early_stopping)

    resume_from_checkpoint = None
    if resume_training_if_possible:
        available_ckpts = sorted(glob.glob(osp.join(snapshots_dir, '*.ckpt')), key=os.path.getmtime)
        if len(available_ckpts)>0:
            resume_from_checkpoint = available_ckpts[-1]
            model.text_logger('Resuming the training from {}'.format(resume_from_checkpoint))

    trainer = pl.Trainer(gpus=1,
                         weights_summary='top',
                         distributed_backend = 'ddp',
                         # replace_sampler_ddp=False,
                         # accumulate_grad_batches=4,
                         # profiler=False,
                         # overfit_batches=0.05,
                         # fast_dev_run = True,
                         # limit_train_batches=0.02,
                         # limit_val_batches=0.02,
                         # num_sanity_val_steps=2,
                         plugins=[DDPPlugin(find_unused_parameters=False)],

                         callbacks=[lr_monitor, early_stop_callback, checkpoint_callback],

                         max_epochs=model.vp_ps.train_parms.num_epochs,
                         logger=logger,
                         resume_from_checkpoint=resume_from_checkpoint
                         )

    trainer.fit(model)


D:\Projects\smplify-x\src\human-body-prior\src\human_body_prior\train\__init__.py
# -*- coding: utf-8 -*-
#
# Copyright (C) 2019 Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG),
# acting on behalf of its Max Planck Institute for Intelligent Systems and the
# Max Planck Institute for Biological Cybernetics. All rights reserved.
#
# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights
# on this computer program. You can only use this computer program if you have closed a license agreement
# with MPG or you get the right to use the computer program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and liable to prosecution.
# Contact: ps-license@tuebingen.mpg.de
#
#
# If you use this code in a research publication please consider citing the following:
#
# Expressive Body Capture: 3D Hands, Face, and Body from a Single Image <https://arxiv.org/abs/1904.05866>
#
#
# Code Developed by:
# Nima Ghorbani <https://nghorbani.github.io/>
#
# 2018.01.02


D:\Projects\smplify-x\src\human-body-prior\src\human_body_prior\visualizations\training_visualization.py
# -*- coding: utf-8 -*-
#
# Copyright (C) 2019 Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG),
# acting on behalf of its Max Planck Institute for Intelligent Systems and the
# Max Planck Institute for Biological Cybernetics. All rights reserved.
#
# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights
# on this computer program. You can only use this computer program if you have closed a license agreement
# with MPG or you get the right to use the computer program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and liable to prosecution.
# Contact: ps-license@tuebingen.mpg.de
#
#
# If you use this code in a research publication please consider citing the following:
#
# Expressive Body Capture: 3D Hands, Face, and Body from a Single Image <https://arxiv.org/abs/1904.05866>
#
#
# Code Developed by:
# Nima Ghorbani <https://nghorbani.github.io/>
#
# 2020.12.12

def pyrenderer(imw=2048, imh=2048):

    from body_visualizer.mesh.mesh_viewer import MeshViewer
    import cv2

    import numpy as np
    import trimesh

    try:
        mv = MeshViewer(width=imw, height=imh, use_offscreen=True)
    except:
        import os
        os.environ['PYOPENGL_PLATFORM'] = 'egl'
        os.environ['EGL_DEVICE_ID'] = os.environ['GPU_DEVICE_ORDINAL'].split(',')[0]

        mv = MeshViewer(width=imw, height=imh, use_offscreen=True)

    mv.set_cam_trans([0, -0.5, 2.])

    def render_an_image(meshes):
        n_all = len(meshes)
        nc = int(np.sqrt(n_all))

        out_image = np.zeros([1, 1, 1, mv.width, mv.height, 4])

        scale_percent = 100./nc
        width = int(mv.width * scale_percent / 100)
        height = int(mv.height * scale_percent / 100)
        dim = (width, height)

        for rId in range(nc):
            for cId in range(nc):
                i = (nc*rId) + cId
                if i>len(meshes): break

                mesh = meshes[i]

                # mesh.apply_transform(trimesh.transformations.rotation_matrix(np.radians(-90), (1, 0, 0)))
                mesh.vertices -= np.median(np.array(mesh.vertices), axis=0)
                mv.set_dynamic_meshes([mesh])
                img = mv.render(render_wireframe=False, RGBA=True)
                img_resized = cv2.resize(img, dim, interpolation=cv2.INTER_AREA)

                out_image[0, 0, 0, (rId*width):((rId+1)*width), (cId*height):((cId+1)*height)] = cv2.cvtColor(img_resized, cv2.COLOR_BGRA2RGBA)

        return out_image.astype(np.uint8)

    return render_an_image

def vposer_trainer_renderer(bm, num_bodies_to_display=5):
    import numpy as np
    import trimesh
    import torch

    from body_visualizer.tools.vis_tools import imagearray2file, colors
    from human_body_prior.tools.omni_tools import copy2cpu as c2c
    from human_body_prior.tools.omni_tools import makepath
    from trimesh import Trimesh as Mesh
    from trimesh.util import concatenate as mesh_cat

    renderer = pyrenderer(1024, 1024)

    faces = c2c(bm.f)

    def render_once(body_parms, body_colors=[colors['grey'], colors['brown-light']], out_fname=None):
        '''

        :param body_parms: list of dictionaries of body parameters.
        :param body_colors: list of np arrays of color rgb values
        :param movie_outpath: a mp4 path
        :return:
        '''

        if out_fname is not None: makepath(out_fname, isfile=True)
        assert len(body_parms) <= len(body_colors), ValueError('Not enough colors provided for #{} body_parms'.format(len(body_parms)))

        bs = body_parms[0]['pose_body'].shape[0]

        body_ids = np.random.choice(bs, num_bodies_to_display)

        body_evals = [c2c(bm(root_orient=v['root_orient'].view(bs, -1) if 'root_orient' in v else torch.zeros(bs, 3).type_as(v['pose_body']),
                         pose_body=v['pose_body'].contiguous().view(bs, -1)).v) for v in body_parms]
        num_verts = body_evals[0].shape[1]

        render_meshes = []
        for bId in body_ids:
            concat_cur_meshes = None
            for body, body_color in zip(body_evals, body_colors):
                cur_body_mesh = Mesh(body[bId], faces, vertex_colors=np.ones([num_verts, 3]) * body_color)
                concat_cur_meshes = cur_body_mesh if concat_cur_meshes is None else mesh_cat(concat_cur_meshes, cur_body_mesh)
            render_meshes.append(concat_cur_meshes)

        img = renderer(render_meshes)

        if out_fname is not None: imagearray2file(img, out_fname, fps=10)


        return

    return render_once


D:\Projects\smplify-x\src\human-body-prior\src\human_body_prior\visualizations\__init__.py
# -*- coding: utf-8 -*-
#
# Copyright (C) 2019 Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG),
# acting on behalf of its Max Planck Institute for Intelligent Systems and the
# Max Planck Institute for Biological Cybernetics. All rights reserved.
#
# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights
# on this computer program. You can only use this computer program if you have closed a license agreement
# with MPG or you get the right to use the computer program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and liable to prosecution.
# Contact: ps-license@tuebingen.mpg.de
#
#
# If you use this code in a research publication please consider citing the following:
#
# Expressive Body Capture: 3D Hands, Face, and Body from a Single Image <https://arxiv.org/abs/1904.05866>
#
#
# Code Developed by:
# Nima Ghorbani <https://nghorbani.github.io/>
#
# 2020.12.12


D:\Projects\smplify-x\src\human-body-prior\src\human_body_prior\__init__.py
# -*- coding: utf-8 -*-
#
# Copyright (C) 2019 Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG),
# acting on behalf of its Max Planck Institute for Intelligent Systems and the
# Max Planck Institute for Biological Cybernetics. All rights reserved.
#
# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights
# on this computer program. You can only use this computer program if you have closed a license agreement
# with MPG or you get the right to use the computer program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and liable to prosecution.
# Contact: ps-license@tuebingen.mpg.de
#
#
# If you use this code in a research publication please consider citing the following:
#
# Expressive Body Capture: 3D Hands, Face, and Body from a Single Image <https://arxiv.org/abs/1904.05866>
#
#
# Code Developed by:
# Nima Ghorbani <https://nghorbani.github.io/>
#
# 2018.01.02


D:\Projects\smplify-x\src\human-body-prior\src\__init__.py
# -*- coding: utf-8 -*-
#
# Copyright (C) 2019 Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG),
# acting on behalf of its Max Planck Institute for Intelligent Systems and the
# Max Planck Institute for Biological Cybernetics. All rights reserved.
#
# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights
# on this computer program. You can only use this computer program if you have closed a license agreement
# with MPG or you get the right to use the computer program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and liable to prosecution.
# Contact: ps-license@tuebingen.mpg.de
#
#
# If you use this code in a research publication please consider citing the following:
#
# Expressive Body Capture: 3D Hands, Face, and Body from a Single Image <https://arxiv.org/abs/1904.05866>
#
#
# Code Developed by:
# Nima Ghorbani <https://nghorbani.github.io/>
#
# 2020.12.12


D:\Projects\smplify-x\src\human-body-prior\tests\test_rotations.py
# -*- coding: utf-8 -*-
#
# Copyright (C) 2019 Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG),
# acting on behalf of its Max Planck Institute for Intelligent Systems and the
# Max Planck Institute for Biological Cybernetics. All rights reserved.
#
# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights
# on this computer program. You can only use this computer program if you have closed a license agreement
# with MPG or you get the right to use the computer program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and liable to prosecution.
# Contact: ps-license@tuebingen.mpg.de
#
#
# If you use this code in a research publication please consider citing the following:
#
# Expressive Body Capture: 3D Hands, Face, and Body from a Single Image <https://arxiv.org/abs/1904.05866>
#
#
# Code Developed by:
# Nima Ghorbani <https://nghorbani.github.io/>
#
# 2018.12.13

import unittest

from human_body_prior.tools.omni_tools import copy2cpu as c2c
from human_body_prior.train.vposer_smpl import VPoser

import numpy as np
import cv2
import torch


class TestRotationConversions(unittest.TestCase):

    def test_aa2matrot(self):
        aa = np.random.randn(10, 3)
        cv2_matrot = []
        for id in range(aa.shape[0]):
            cv2_matrot.append(cv2.Rodrigues(aa[id:id+1])[0])
        cv2_matrot = np.array(cv2_matrot).reshape(-1,9)

        vposer_matrot = c2c(VPoser.aa2matrot(torch.tensor(aa))).reshape(-1,9)
        self.assertAlmostEqual(np.square((vposer_matrot - cv2_matrot)).sum(), 0.0)

    def test_matrot2aa(self):
        np.random.seed(100)
        aa = np.random.randn(10, 3)
        matrot = c2c(VPoser.aa2matrot(torch.tensor(aa))).reshape(-1,9)

        cv2_aa = []
        for id in range(matrot.shape[0]):
            cv2_aa.append(cv2.Rodrigues(matrot[id].reshape(3,3))[0])
        cv2_aa = np.array(cv2_aa).reshape(-1,3)

        vposer_aa = c2c(VPoser.matrot2aa(torch.tensor(matrot))).reshape(-1,3)
        self.assertAlmostEqual(np.square((vposer_aa - cv2_aa)).sum(), 0.0)


if __name__ == '__main__':
    unittest.main()

D:\Projects\smplify-x\src\human-body-prior\tests\test_vposer.py
# -*- coding: utf-8 -*-
#
# Copyright (C) 2019 Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG),
# acting on behalf of its Max Planck Institute for Intelligent Systems and the
# Max Planck Institute for Biological Cybernetics. All rights reserved.
#
# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights
# on this computer program. You can only use this computer program if you have closed a license agreement
# with MPG or you get the right to use the computer program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and liable to prosecution.
# Contact: ps-license@tuebingen.mpg.de
#
#
# If you use this code in a research publication please consider citing the following:
#
# Expressive Body Capture: 3D Hands, Face, and Body from a Single Image <https://arxiv.org/abs/1904.05866>
#
#
# Code Developed by:
# Nima Ghorbani <https://nghorbani.github.io/>
#
# 2018.01.02

import unittest

from human_body_prior.train.vposer_smpl import VPoser
from human_body_prior.tools.omni_tools import copy2cpu as c2c
from configer import Configer

import numpy as np

class TestDistances(unittest.TestCase):
    def setUp(self):
        import torch
        torch.manual_seed(100)

    def test_samples(self):
        ''' given the same network weights, the random pose generator must produce the same pose for a seed'''
        ps = Configer(default_ps_fname='../human_body_prior/train/V02_00.yaml')
        vposer = VPoser(num_neurons=ps.num_neurons, latentD=ps.latentD, data_shape = ps.data_shape)
        body_pose_rnd = vposer.sample_poses(num_poses=1, seed=100)

        body_pose_gt = np.load('samples/body_pose_rnd.npz')['data']
        self.assertAlmostEqual(np.square((c2c(body_pose_rnd) - body_pose_gt)).sum(), 0.0)

if __name__ == '__main__':
    unittest.main()

D:\Projects\smplify-x\src\human-body-prior\tutorials\ik_example_joints.py
# -*- coding: utf-8 -*-
#
# Copyright (C) 2019 Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG),
# acting on behalf of its Max Planck Institute for Intelligent Systems and the
# Max Planck Institute for Biological Cybernetics. All rights reserved.
#
# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights
# on this computer program. You can only use this computer program if you have closed a license agreement
# with MPG or you get the right to use the computer program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and liable to prosecution.
# Contact: ps-license@tuebingen.mpg.de
#
#
# If you use this code in a research publication please consider citing the following:
#
# Expressive Body Capture: 3D Hands, Face, and Body from a Single Image <https://arxiv.org/abs/1904.05866>
#
#
# Code Developed by:
# Nima Ghorbani <https://nghorbani.github.io/>
#
# 2021.02.12

from typing import Union

import numpy as np
import torch
from colour import Color
from human_body_prior.body_model.body_model import BodyModel
from torch import nn

from human_body_prior.models.ik_engine import IK_Engine
from os import path as osp


class SourceKeyPoints(nn.Module):
    def __init__(self,
                 bm: Union[str, BodyModel],
                 n_joints: int=22,
                 kpts_colors: Union[np.ndarray, None] = None ,
                 ):
        super(SourceKeyPoints, self).__init__()

        self.bm = BodyModel(bm, persistant_buffer=False) if isinstance(bm, str) else bm
        self.bm_f = []#self.bm.f
        self.n_joints = n_joints
        self.kpts_colors = np.array([Color('grey').rgb for _ in range(n_joints)]) if kpts_colors == None else kpts_colors

    def forward(self, body_parms):
        new_body = self.bm(**body_parms)


        return {'source_kpts':new_body.Jtr[:,:self.n_joints], 'body': new_body}


support_dir = '../support_data/dowloads'
vposer_expr_dir = osp.join(support_dir,'vposer_v2_05') #'TRAINED_MODEL_DIRECTORY'  in this directory the trained model along with the model code exist
bm_fname =  osp.join(support_dir,'models/smplx/neutral/model.npz')#'PATH_TO_SMPLX_model.npz'  obtain from https://smpl-x.is.tue.mpg.de/downloads
sample_amass_fname = osp.join(support_dir, 'amass_sample.npz')# a sample npz file from AMASS

comp_device = torch.device('cuda')

sample_amass = np.load(sample_amass_fname)
print('sample_amass keys: ', list(sample_amass.keys()))
n_joints = 22

target_bm = BodyModel(bm_fname)(**{
    'pose_body': torch.tensor(sample_amass['poses'][:,3:66]).type(torch.float),
    'root_orient': torch.tensor(sample_amass['poses'][:,:3]).type(torch.float),
    'trans': torch.tensor(sample_amass['trans']).type(torch.float),
})

red = Color("red")
blue = Color("blue")
kpts_colors = [c.rgb for c in list(red.range_to(blue, n_joints))]

# create source and target key points and make sure they are index aligned
data_loss = torch.nn.MSELoss(reduction='sum')

stepwise_weights = [
    {'data': 10., 'poZ_body': .01, 'betas': .5},
                    ]

optimizer_args = {'type':'LBFGS', 'max_iter':300, 'lr':1, 'tolerance_change': 1e-4, 'history_size':200}
ik_engine = IK_Engine(vposer_expr_dir=vposer_expr_dir,
                      verbosity=2,
                      display_rc= (2, 2),
                      data_loss=data_loss,
                      stepwise_weights=stepwise_weights,
                      optimizer_args=optimizer_args).to(comp_device)

from human_body_prior.tools.omni_tools import create_list_chunks
frame_ids = np.arange(len(target_bm.v))
np.random.shuffle(frame_ids)
batch_size = 4

for rnd_frame_ids in create_list_chunks(frame_ids, batch_size, overlap_size=0, cut_smaller_batches=False):

    print(rnd_frame_ids)
    target_pts = target_bm.Jtr[rnd_frame_ids,:n_joints].detach().to(comp_device)
    source_pts = SourceKeyPoints(bm=bm_fname, n_joints=n_joints, kpts_colors=kpts_colors).to(comp_device)

    ik_res = ik_engine(source_pts, target_pts)

    ik_res_detached = {k: v.detach() for k, v in ik_res.items()}
    nan_mask = torch.isnan(ik_res_detached['trans']).sum(-1) != 0
    if nan_mask.sum() != 0: raise ValueError('Sum results were NaN!')


D:\Projects\smplify-x\src\human-body-prior\tutorials\ik_example_mocap.py
# -*- coding: utf-8 -*-
#
# Copyright (C) 2019 Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG),
# acting on behalf of its Max Planck Institute for Intelligent Systems and the
# Max Planck Institute for Biological Cybernetics. All rights reserved.
#
# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights
# on this computer program. You can only use this computer program if you have closed a license agreement
# with MPG or you get the right to use the computer program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and liable to prosecution.
# Contact: ps-license@tuebingen.mpg.de
#
#
# If you use this code in a research publication please consider citing the following:
#
# Expressive Body Capture: 3D Hands, Face, and Body from a Single Image <https://arxiv.org/abs/1904.05866>
#
#
# Code Developed by:
# Nima Ghorbani <https://nghorbani.github.io/>
#
# 2021.02.12
import torch
from human_body_prior.tools.omni_tools import copy2cpu as c2c

from human_body_prior.models.ik_engine import IK_Engine
from torch import nn
from typing import Union, Iterable
from human_body_prior.body_model.body_model import BodyModel
import numpy as np
from colour import Color

import pickle
from typing import Union

import numpy as np

import torch
from colour import Color
from human_body_prior.body_model.body_model import BodyModel
from torch import nn

from human_body_prior.models.ik_engine import IK_Engine
from os import path as osp

def compute_vertex_normal_batched(vertices, indices):
    from pytorch3d.structures import Meshes
    return Meshes(verts=vertices, faces=indices.expand(len(vertices), -1, -1)).verts_normals_packed().view(-1, vertices.shape[1],3)

class SourceKeyPoints(nn.Module):
    def __init__(self,
                 bm: Union[str, BodyModel],
                 vids: Iterable[int],
                 kpts_colors: Union[np.ndarray, None] = None ,
                 ):
        super(SourceKeyPoints, self).__init__()

        self.bm = BodyModel(bm, persistant_buffer=False) if isinstance(bm, str) else bm
        self.bm_f = []#self.bm.f
        self.vids = vids
        self.kpts_colors = np.array([Color('grey').rgb for _ in vids]) if kpts_colors == None else kpts_colors

    def forward(self, body_parms):
        new_body = self.bm(**body_parms)

        vn = compute_vertex_normal_batched(new_body.v, new_body.f)
        virtual_markers = new_body.v[:, self.vids] + 0.0095 * vn[:, self.vids]

        return {'source_kpts':virtual_markers, 'body': new_body}

support_dir = '../support_data/dowloads'
vposer_expr_dir = osp.join(support_dir,'vposer_v2_05') #'TRAINED_MODEL_DIRECTORY'  in this directory the trained model along with the model code exist
bm_fname =  osp.join(support_dir,'models/smplx/neutral/model.npz')#'PATH_TO_SMPLX_model.npz'  obtain from https://smpl-x.is.tue.mpg.de/downloads
sample_amass_fname = osp.join(support_dir, 'amass_sample.npz')# a sample npz file from AMASS

comp_device = torch.device('cuda')


sample_amass = np.load(sample_amass_fname, allow_pickle=True)

print('sample_amass keys: ', list(sample_amass.keys()))

vids = sample_amass['vids'].tolist()

red = Color("red")
blue = Color("blue")
kpts_colors = [c.rgb for c in list(red.range_to(blue, len(vids)))]

# create source and target key points and make sure they are index aligned
data_loss = torch.nn.MSELoss(reduction='sum')

stepwise_weights = [
    {'data': 10., 'poZ_body': .01, 'betas': .5},
                    ]
# optimizer_args = {'type':'ADAM', 'max_iter':500, 'lr':1e-1, 'tolerance_change': 1e-5}
optimizer_args = {'type':'LBFGS', 'max_iter':300, 'lr':1, 'tolerance_change': 1e-4, 'history_size':200}
ik_engine = IK_Engine(vposer_expr_dir=vposer_expr_dir,
                      verbosity=2,
                      display_rc=(2, 2),
                      data_loss=data_loss,
                      stepwise_weights=stepwise_weights,
                      optimizer_args=optimizer_args).to(comp_device)

markers_orig = sample_amass['markers']
rnd_frame_ids = np.random.choice(len(markers_orig), 10, replace=False)

target_pts = markers_orig[rnd_frame_ids]
target_pts = torch.Tensor(target_pts).type(torch.float).to(comp_device)
source_pts = SourceKeyPoints(bm=bm_fname, vids=vids, kpts_colors=kpts_colors).to(comp_device)

ik_res = ik_engine(source_pts, target_pts)

ik_res_detached = {k: v.detach() for k, v in ik_res.items()}
nan_mask = torch.isnan(ik_res_detached['trans']).sum(-1) != 0
if nan_mask.sum() != 0: raise ValueError('Sum results were NaN!')

D:\Projects\smplify-x\src\human-body-prior\tutorials\mdm_motion2smpl.py
# -*- coding: utf-8 -*-
#
# Code Developed by:
# Nima Ghorbani <https://nghorbani.github.io/>
#
# 2022.10.20
# SMPL-X Solver for MDM: Human Motion Diffusion Model

import os.path as osp
from pathlib import Path
from typing import List, Dict
from typing import Union

import numpy as np
import torch
from colour import Color
from loguru import logger
from scipy.spatial.transform import Rotation as R
from torch import nn

from human_body_prior.body_model.body_model import BodyModel
from human_body_prior.models.ik_engine import IK_Engine
from human_body_prior.tools.omni_tools import copy2cpu as c2c
from human_body_prior.tools.omni_tools import create_list_chunks
from tqdm import tqdm
from body_visualizer.tools.vis_tools import render_smpl_params
from body_visualizer.tools.vis_tools import imagearray2file
import os.path as osp
from glob import glob

import numpy as np
import torch
from loguru import logger
from human_body_prior.tools.omni_tools import get_support_data_dir

from body_visualizer.tools.vis_tools import imagearray2file
from body_visualizer.tools.vis_tools import render_smpl_params
from human_body_prior.body_model.body_model import BodyModel
from human_body_prior.tools.omni_tools import get_support_data_dir
class SourceKeyPoints(nn.Module):
    def __init__(self,
                 bm: Union[str, BodyModel],
                 n_joints: int = 22,
                 kpts_colors: Union[np.ndarray, None] = None,
                 num_betas=16
                 ):
        super(SourceKeyPoints, self).__init__()

        self.bm = BodyModel(bm, num_betas=num_betas, persistant_buffer=False) if isinstance(bm, str) else bm
        self.bm_f = []  # self.bm.f
        self.n_joints = n_joints
        self.kpts_colors = np.array(
            [Color('grey').rgb for _ in range(n_joints)]) if kpts_colors == None else kpts_colors

    def forward(self, body_parms):
        new_body = self.bm(**body_parms)

        return {'source_kpts': new_body.Jtr[:, :self.n_joints], 'body': new_body}


def transform_smpl_coordinate(bm_fname: Path, trans: np.ndarray,
                              root_orient: np.ndarray, betas: np.ndarray, rotxyz: Union[np.ndarray, List]) -> Dict:
    """
    rotates smpl parameters while taking into account non-zero center of rotation for smpl
    Parameters
    ----------
    bm_fname: body model filename
    trans: Nx3
    root_orient: Nx3
    betas: num_betas
    rotxyz: desired XYZ rotation in degrees

    Returns
    -------

    """
    if isinstance(rotxyz, list):
        rotxyz = np.array(rotxyz).reshape(1, 3)
    if betas.ndim == 1: betas = betas[None]
    if betas.ndim == 2 and betas.shape[0] != 1:
        logger.warning(
            f'betas should be the same for the entire sequence. 2D np.array with 1 x num_betas: {betas.shape}. taking the mean')
        betas = np.mean(betas, keepdims=True, axis=0)
    transformation_euler = np.deg2rad(rotxyz)

    coord_change_matrot = R.from_euler('XYZ', transformation_euler.reshape(1, 3)).as_matrix().reshape(3, 3)
    bm = BodyModel(bm_fname=bm_fname,
                   num_betas=betas.shape[1])
    pelvis_offset = c2c(bm(**{'betas': torch.from_numpy(betas).type(torch.float32)}).Jtr[[0], 0])

    root_matrot = R.from_rotvec(root_orient).as_matrix().reshape([-1, 3, 3])

    transformed_root_orient_matrot = np.matmul(coord_change_matrot, root_matrot.T).T
    transformed_root_orient = R.from_matrix(transformed_root_orient_matrot).as_rotvec()
    transformed_trans = np.matmul(coord_change_matrot, (trans + pelvis_offset).T).T - pelvis_offset

    return {'root_orient': transformed_root_orient.astype(np.float32),
            'trans': transformed_trans.astype(np.float32), }


def convert_mdm_mp4_to_amass_npz(skeleton_movie_fname, out_fname=None, save_render=False, comp_device='cuda:0',
                                 surface_model_type = 'smplx', gender = 'neutral', batch_size=128, verbosity=0):
    """

    :param skeleton_movie_fname: either a result npy file or a motion numpy array [nframes, njoints, 3]
    :param surface_model_type:
    :param gender:
    :param batch_size:
    :param verbosity: 0: silent, 1: text, 2: visual with psbody.mesh
    :return:
    """

    support_base_dir = get_support_data_dir()
    support_dir = osp.join(support_base_dir, 'dowloads')#'../../../support_data/dowloads'
    logger.info(f'found support_dir: {support_dir}')
    # 'TRAINED_MODEL_DIRECTORY'  in this directory the trained model along with the model code exist
    vposer_expr_dir = osp.join(support_dir,'vposer_v2_05')

    # 'PATH_TO_SMPLX_model.npz'  obtain from https://smpl-x.is.tue.mpg.de/downloads
    bm_fname = osp.join(support_dir, f'models/{surface_model_type}/{gender}/model.npz')

    if isinstance(skeleton_movie_fname, np.ndarray):
        assert out_fname is not None, 'when passing motion file out_fname should be provided'
        motion = skeleton_movie_fname
    else:
        assert osp.exists(skeleton_movie_fname), skeleton_movie_fname
        parsed_name = osp.basename(skeleton_movie_fname).replace('.mp4', '').replace('sample', '').replace('rep', '')

        sample_idx, rep_idx = [int(e) for e in parsed_name.split('_')]
        mdm_fname = osp.join(osp.dirname(skeleton_movie_fname), 'results.npy')
        mdm_data = np.load(mdm_fname, allow_pickle=True).tolist()
        total_num_samples = mdm_data['num_samples']
        absl_idx = rep_idx * total_num_samples + sample_idx
        motion = mdm_data['motion'][absl_idx].transpose(2, 0, 1)  # [nframes, njoints, 3]

    if out_fname is None:
        out_fname = skeleton_movie_fname.replace('.mp4', '.npz')

    render_out_fname = out_fname.replace('.npz', f'_{surface_model_type}.mp4')
    if osp.exists(render_out_fname):
        logger.warning(f'render output already exists: {render_out_fname}. skipping...')
        return
    n_joints = 22
    num_betas = 16

    if osp.exists(out_fname):
        d = np.load(out_fname)
    else:
        # comp_device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


        red = Color("red")
        blue = Color("blue")
        kpts_colors = [c.rgb for c in list(red.range_to(blue, n_joints))]

        # create source and target key points and make sure they are index aligned
        data_loss = torch.nn.MSELoss(reduction='sum')

        stepwise_weights = [
            {'data': 10., 'poZ_body': .01, 'betas': .5},
        ]

        optimizer_args = {'type': 'LBFGS', 'max_iter': 300, 'lr': 1, 'tolerance_change': 1e-4, 'history_size': 200}
        ik_engine = IK_Engine(vposer_expr_dir=vposer_expr_dir,
                              verbosity=verbosity,
                              display_rc=(2, 2),
                              data_loss=data_loss,
                              num_betas=num_betas,
                              stepwise_weights=stepwise_weights,
                              optimizer_args=optimizer_args).to(comp_device)

        all_results = {}
        batched_frames = create_list_chunks(np.arange(len(motion)), batch_size, overlap_size=0, cut_smaller_batches=False)
        if verbosity<2:
            batched_frames = tqdm(batched_frames, desc='VPoser Advanced IK')
        for cur_frame_ids in batched_frames:

            target_pts = torch.from_numpy(motion[cur_frame_ids, :n_joints]).to(comp_device)
            source_pts = SourceKeyPoints(bm=bm_fname, n_joints=n_joints, kpts_colors=kpts_colors, num_betas=num_betas).to(
                comp_device)

            ik_res = ik_engine(source_pts, target_pts, {})

            ik_res_detached = {k: c2c(v) for k, v in ik_res.items()}
            nan_mask = np.isnan(ik_res_detached['trans']).sum(-1) != 0
            if nan_mask.sum() != 0: raise ValueError('Sum results were NaN!')
            for k, v in ik_res_detached.items():
                if k not in all_results: all_results[k] = []
                all_results[k].append(v)

        d = {k: np.concatenate(v, axis=0) for k, v in all_results.items()}
        d['betas'] = np.median(d['betas'], axis=0)

        transformed_d = transform_smpl_coordinate(bm_fname=bm_fname, trans=d['trans'], root_orient=d['root_orient'],
                                                  betas=d['betas'], rotxyz=[90, 0, 0])
        d.update(transformed_d)
        d['poses'] = np.concatenate([d['root_orient'], d['pose_body'], np.zeros([len(d['pose_body']), 99])], axis=1)

        d['surface_model_type'] = surface_model_type
        d['gender'] = gender
        d['mocap_frame_rate'] = 30
        d['num_betas'] = num_betas
        np.savez(out_fname, **d)
        logger.success(f'created: {out_fname}')

    if save_render:
        bm = BodyModel(bm_fname=bm_fname, num_betas=num_betas)
        smpl_dict = np.load(bm_fname)
        mean_pose_hand = np.repeat(np.concatenate([smpl_dict['hands_meanl'], smpl_dict['hands_meanr']])[None], axis=0, repeats=len(motion))

        body_parms = {**d, 'betas': np.repeat(d['betas'][None], axis=0, repeats=len(motion)), 'pose_hand':mean_pose_hand}
        body_parms = {k:torch.from_numpy(v) for k,v in body_parms.items() if k in ['root_orient', 'trans', 'pose_body', 'pose_hand']}

        img_array = render_smpl_params(bm, body_parms, [-90, 0, 0])[None, None]
        imagearray2file(img_array, outpath=render_out_fname, fps=30)
        logger.success(f'created: {render_out_fname}')

    logger.info(f'You can visualize these results as any amass npz file or in Blender via blender_smplx_addon.')

if __name__ == '__main__':
    import argparse
    from glob import glob
    parser = argparse.ArgumentParser()
    parser.add_argument("--input", type=str, help='skeleton movie.mp4 filename that is to be converted into SMPL')
    parser.add_argument("--pattern", type=str, help='filename pattern for skeleton */*/movies.mp4 to be converted into SMPL')
    parser.add_argument("--batch_size", type=int, default=128, help='batch size for inverse kinematics')
    parser.add_argument("--model_type", type=str, default='smplx', help='model_type; e.g. smplx/smpl')
    parser.add_argument("--device", type=str, default='cuda:0', help='computation device')
    parser.add_argument("--gender", type=str, default='neutral', help='gender; e.g. neutral')
    parser.add_argument("--save_render", type=bool, default=True, help='render IK results')
    parser.add_argument("--verbosity", type=int, default=0, help='0: silent, 1: text, 2: display')
    params = parser.parse_args()
    # params = {
    #     'input':'/home/nima/opt/code-repos/motion-diffusion-model/save/humanml_trans_enc_512/samples_humanml_trans_enc_512_000200000_seed10/sample00_rep00.mp4',
    #          'save_render': True
    #           }
    if (params.input is None) and (params.pattern is None):
        raise ValueError('either input or pattern should be provided')
    if not params.input is None:
        convert_mdm_mp4_to_amass_npz(skeleton_movie_fname=params.input,
                                 surface_model_type=params.model_type,
                                 gender=params.gender,
                                 batch_size=params.batch_size,
                                 save_render=params.save_render,
                                 verbosity=params.verbosity)
    else:
        assert params.pattern is not None
        mp4_fnames = glob(params.pattern)
        print(f'found {len(mp4_fnames)} mp4 files')
        for mp4_fname in mp4_fnames:

            if mp4_fname.endswith('_smplx.mp4'):
                print(f'skipping smplx render file: {mp4_fname}')
                continue

            convert_mdm_mp4_to_amass_npz(skeleton_movie_fname=mp4_fname,
                                         surface_model_type=params.model_type,
                                         gender=params.gender,
                                         batch_size=params.batch_size,
                                         save_render=params.save_render,
                                         verbosity=params.verbosity)


D:\Projects\smplify-x\src\human-body-prior\tutorials\vposer.ipynb
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VPoser\n",
    "The original body pose space of [SMPL](http://smpl.is.tue.mpg.de/) is not bounded to natural human pose space.\n",
    "That means you can put a vector value as the pose of a SMPL body model and get a broken body, which might not even look like a human.\n",
    "To address this issue we replace the original pose space of SMPL with VPoser's latent space (poZ) with known distribution and\n",
    "correspondence to natural human pose manifold.\n",
    "\n",
    "The original **body pose** of SMPL is composed of axis-angle representation of 21 joints which in total makes a vector of 63 elements; i.e. 3 rotation reprsentation for each joint.\n",
    "On the other hand, **body poZ**, VPoser's latent space representation for SMPL body, has in total 32 elements with a spherical Gaussian distribution.\n",
    "This means if one samples a 32 dimensional random vector from a Normal distribution and pass it through VPoser's decoder the result would be a viable human joint configuration in axis-angle representation.\n",
    "We introduce \"body poZ\" as a new representation of human body pose that is fully differentiable and can be used in an end-to-end deep learning pipeline.\n",
    "\n",
    "In this tutorial we demonstrate how to encode an original body pose into poZ, decode it back to pose and furthermore generate random novel poses from VPoser.\n",
    " \n",
    "First you need to obtain a trained VPoser and a variation of SMPL model, here we use SMPLx, from https://smpl-x.is.tue.mpg.de/downloads.\n",
    "\n",
    "Put the obtained VPoser and body models in a folder, here we assume respectively\n",
    "\n",
    "'GITHUB_CLONE_ROOT/human_body_prior/support_data/dowloads/vposer_vXX', and\n",
    "'GITHUB_CLONE_ROOT/human_body_prior/support_data/dowloads/models/smplx/GENDER/model.npz'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from body_visualizer.tools.vis_tools import render_smpl_params\n",
    "from body_visualizer.tools.vis_tools import imagearray2file\n",
    "from body_visualizer.tools.vis_tools import show_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../support_data/dowloads/vposer_v2_05\n",
      "../support_data/dowloads/models/smplx/neutral/model.npz\n",
      "../support_data/dowloads/amass_sample.npz\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#This tutorial requires 'vposer_v2_05'\n",
    "\n",
    "from os import path as osp\n",
    "support_dir = '../support_data/dowloads'\n",
    "expr_dir = osp.join(support_dir,'vposer_v2_05') #'TRAINED_MODEL_DIRECTORY'  in this directory the trained model along with the model code exist\n",
    "bm_fname =  osp.join(support_dir,'models/smplx/neutral/model.npz')#'PATH_TO_SMPLX_model.npz'  obtain from https://smpl-x.is.tue.mpg.de/downloads\n",
    "sample_amass_fname = osp.join(support_dir, 'amass_sample.npz')# a sample npz file from AMASS\n",
    "\n",
    "print(expr_dir)\n",
    "print(bm_fname)\n",
    "print(sample_amass_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Loading SMPLx Body Model\n",
    "from human_body_prior.body_model.body_model import BodyModel\n",
    "\n",
    "bm = BodyModel(bm_fname=bm_fname).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading VPoser Body Pose Prior\n",
    "from human_body_prior.tools.model_loader import load_model\n",
    "from human_body_prior.models.vposer_model import VPoser\n",
    "vp, ps = load_model(expr_dir, model_code=VPoser,\n",
    "                              remove_words_in_model_weights='vp_model.',\n",
    "                              disable_grad=True)\n",
    "vp = vp.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding a body_pose (pose>poZ)\n",
    "We will load an [AMASS](http://amass.is.tue.mpg.de/) sample and place the body pose on the right device for batch processing. To learn more on AMASS data loading refer to [link](https://github.com/nghorbani/amass/blob/master/notebooks/01-AMASS_Visualization.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amass_body_pose.shape torch.Size([500, 63])\n"
     ]
    }
   ],
   "source": [
    "# Prepare the pose_body from amass sample\n",
    "amass_body_pose = np.load(sample_amass_fname)['poses'][:, 3:66]\n",
    "amass_body_pose = torch.from_numpy(amass_body_pose).type(torch.float).to('cuda')\n",
    "print('amass_body_pose.shape', amass_body_pose.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amass_body_poZ.shape torch.Size([500, 32])\n"
     ]
    }
   ],
   "source": [
    "amass_body_poZ = vp.encode(amass_body_pose).mean\n",
    "print('amass_body_poZ.shape', amass_body_poZ.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding a body_poZ (poZ>pose)\n",
    "We will decode the same poZ in order to reconstruct the pose and will visualize it for a random frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amass_body_pose_rec.shape torch.Size([500, 32])\n"
     ]
    }
   ],
   "source": [
    "amass_body_pose_rec = vp.decode(amass_body_poZ)['pose_body'].contiguous().view(-1, 63)\n",
    "print('amass_body_pose_rec.shape', amass_body_poZ.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 63])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Let's visualize the original pose and the reconstructed one:\n",
    "\n",
    "t = np.random.choice(len(amass_body_pose))\n",
    "\n",
    "all_pose_body = torch.stack([amass_body_pose[t], amass_body_pose_rec[t]])\n",
    "\n",
    "all_pose_body.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA94AAAINCAYAAADIsKceAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAuIwAALiMBeKU/dgAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOzdWXMc533+/WtmehYMFmIhQYAbSEkUSVOUZJlaLMl/V9nlxKlKJUc5yavJ81ZylKOcuJJUUl4kLxJliSZFiZtEgQSxkNj3WXqZfg5Yd+ueRs9gQLKJRd9P1RQWzvT0wCX/7uteM2EYCgAAAAAApCO72zcAAAAAAMBBRvAGAAAAACBFBG8AAAAAAFJE8AYAAAAAIEUEbwAAAAAAUkTwBgAAAAAgRQRvAAAAAABSRPAGAAAAACBFBG8AAAAAAFJE8AYAAAAAIEUEbwAAAAAAUkTwBgAAAAAgRQRvAAAAAABSRPAGAAAAACBFBG8AAAAAAFJE8AYAAAAAIEUEbwAAAAAAUkTwBgAAAAAgRQRvAAAAAABSRPAGAAAAACBFBG8AAAAAAFJE8AYAAAAAIEUEbwAAAAAAUkTwBgAAAAAgRQRvAAAAAABSRPAGAAAAACBFBG8AAAAAAFJE8AYAAAAAIEUEbwAAAAAAUuTs4nuHu/jeAAAAAIAfnsxuvCkj3gAAAAAApIjgDQAAAABAigjeAAAAAACkiOANAAAAAECKCN4AAAAAAKSI4A0AAAAAQIoI3gAAAAAApIjgDQAAAABAigjeAAAAAACkiOANAAAAAECKCN4AAAAAAKSI4A0AAAAAQIoI3gAAAAAApIjgDQAAAABAigjeAAAAAACkiOANAAAAAECKCN4AAAAAAKSI4A0AAAAAQIoI3gAAAAAApIjgDQAAAABAigjeAAAAAACkiOANAAAAAECKCN4AAAAAAKSI4A0AAAAAQIoI3gAAAAAApIjgDQAAAABAigjeAAAAAACkiOANAAAAAECKCN4AAAAAAKSI4A0AAAAAQIoI3gAAAAAApIjgDQAAAABAigjeAAAAAACkiOANAAAAAECKCN4AAAAAAKSI4A0AAAAAQIoI3gAAAAAApIjgDQAAAABAigjeAAAAAACkiOANAAAAAECKCN4AAAAAAKSI4A0AAAAAQIoI3gAAAAAApIjgDQAAAABAigjeAAAAAACkiOANAAAAAECKCN4AAAAAAKSI4A0AAAAAQIoI3gAAAAAApIjgDQAAAABAigjeAAAAAACkiOANAAAAAECKCN4AAAAAAKSI4A0AAAAAQIoI3gAAAAAApIjgDQAAAABAigjeAAAAAACkiOANAAAAAECKCN4AAAAAAKSI4A0AAAAAQIoI3gAAAAAApIjgDQAAAABAigjeAAAAAACkiOANAAAAAECKCN4AAAAAAKSI4A0AAAAAQIoI3gAAAAAApIjgDQAAAABAigjeAAAAAACkiOANAAAAAECKCN4AAAAAAKSI4A0AAAAAQIoI3gAAAAAApIjgDQAAAABAigjeAAAAAACkiOANAAAAAECKCN4AAAAAAKSI4A0AAAAAQIoI3gAAAAAApIjgDQAAAABAigjeAAAAAACkiOANAAAAAECKCN4AAAAAAKSI4A0AAAAAQIoI3gAAAAAApIjgDQAAAABAigjeAAAAAACkiOANAAAAAECKCN4AAAAAAKSI4A0AAAAAQIoI3gAAAAAApIjgDQAAAABAigjeAAAAAACkiOANAAAAAECKCN4AAAAAAKSI4A0AAAAAQIoI3gAAAAAApIjgDQAAAABAigjeAAAAAACkiOANAAAAAECKCN4AAAAAAKSI4A0AAAAAQIoI3gAAAAAApIjgDQAAAABAigjeAAAAAACkiOANAAAAAECKCN4AAAAAAKSI4A0AAAAAQIoI3gAAAAAApIjgDQAAAABAigjeAAAAAACkiOANAAAAAECKCN4AAAAAAKSI4A0AAAAAQIoI3gAAAAAApIjgDQAAAABAigjeAAAAAACkiOANAAAAAECKCN4AAAAAAKSI4A0AAAAAQIoI3gAAAAAApIjgDQAAAABAigjeAAAAAACkiOANAAAAAECKCN4AAAAAAKSI4A0AAAAAQIoI3gAAAAAApIjgDQAAAABAigjeAAAAAACkiOANAAAAAECKCN4AAAAAAKSI4A0AAAAAQIoI3gAAAAAApIjgDQAAAABAigjeAAAAAACkiOANAAAAAECKCN4AAAAAAKSI4A0AAAAAQIoI3gAAAAAApIjgDQAAAABAigjeAAAAAACkiOANAAAAAECKCN4AAAAAAKSI4A0AAAAAQIoI3gAAAAAApIjgDQAAAABAigjeAAAAAACkiOANAAAAAECKCN4AAAAAAKSI4A0AAAAAQIoI3gAAAAAApIjgDQAAAABAigjeAAAAAACkiOANAAAAAECKCN4AAAAAAKSI4A0AAAAAQIoI3gAAAAAApIjgDQAAAABAigjeAAAAAACkiOANAAAAAECKCN4AAAAAAKSI4A0AAAAAQIoI3gAAAAAApIjgDQAAAABAigjeAAAAAACkiOANAAAAAECKCN4AAAAAAKSI4A0AAAAAQIoI3gAAAAAApIjgDQAAAABAigjeAAAAAACkiOANAAAAAECKCN4AAAAAAKSI4A0AAAAAQIoI3gAAAAAApIjgDQAAAABAipzdvgEA6QrDUEEQNP0um80qm6XfDQCAJJ7nqdFoSJKCIIi+z2azymQykqRMJqN8Pq9cLrdr9wlg/yB4AwdMGIZNP1erVU1PTyuTySgMQ4VhqNHRUfX09ETPMY0IAAB+yEwNvXHjhh4/fizf9zUzM6P5+Xl5nqfjx4/LcRw1Gg319/fr8uXLOnPmTPR66imAVjLxRvoLtGtvDBxUYRjqf//3fyVJ33zzjRYXF5XL5ZTNZtVoNKJR7kajod7eXg0PD+vEiRP6+c9/vpu3DQDArgvDUL/73e/0xRdfqFqtKpvNRjU0k8k0heowDNVoNNRoNOT7vo4fP65//dd/berUBrBn7UoPGcEbOADq9boePnyomZkZ/elPf1I+n4/+LZPJRI2GMAyjEB6GYfRvJ06c0OXLl3Xq1Ck5DhNhAAA/HOvr6/rss890/fp1+b6vIAiUy+XkOE4UvO2OazN7THoyDd28plgs6sMPP9T58+fV39+/mx8JQHsEbwA7Nz8/r1u3bunWrVtyXVeVSkX5fF6O40SNBdNLb/57j/fU53I59fT06MKFC7p06RINBgDAD4KpodeuXdPa2poKhYIcx9kSuk3wNqHbfvi+L8/z5Lqu+vr6dOnSJV26dElDQ0O7/OkAtLArwZuhLWAfm5ub0927d3Xz5k3Nz8+rUCioVCo1NRjM9Di7h958bzZd8zwvWss2NjZG8AYAHHibm5uanJzUnTt3tLm5qWKxqEKhoFwu1xS64zPH4g97GvrKyopu376tfD6vDz74YJc/IYC9hG2NgX2qWq3qu+++0+3bt7W4uKhSqaRSqRSNdpuGQ6uHWbtmevYdx9Hs7Kzm5+dVqVR2++MBAJCq6elp3bt3TwsLCyoUCioWi8rn80111B7xth/xcG7qaT6f1/z8vO7evaulpaXd/ogA9hCCN7BPTU5O6u7du5qfn4966c2xJvHGQLyRYHrv7d+bqel37tzR1NTUbn88AABSU6/X9fXXX+ubb75RoVCIHkmd1/GRb3s2WSaT2dKhnc1mtbq6qmvXru32xwSwhxC8gX0oDEN99NFHmpmZiXrn7QZDUsNAUvR9q977fD6vu3fvampqasuxZAAAHBRXr16NRrpNHW0VunO5XGIttX8X78x2XVePHz/ezY8IYI8heAP70B/+8Adtbm7KcZymXvqkHvo4+ziUpB77XC6ne/fu6YsvvniRHwkAgBfmyy+/1Pz8fFNgjndMx48Qk7ae0x0P4aaeuq6r6enpF/mRAOxxBG9gH3r06JEajUbT+mx7ilvSiPd2vfW+70t60mu/ubmp5eXlXft8AACkZXx8XPV6PfrZro3ma6tHkniHtvnaaDQ0Pj7ODDIAkgjewL7SaDQ0NTWlx48fKwiCluF6u4aC+V0QBKrX68pkMqrX6/J9X2EYamNjg01hAAAHUjwItwvUrf4t/rogCBQEQdO1G42GJiYmnvFuARwUBG9gn6lWq6rVatv2oMfP7k763vd91et15XI5BUGgarWqIAjkeZ7W19e1uLiYzocAAGCXrKysqNFoNAVr+1gwO1DHj+G0j+aUntTaIAiio8ZM+DY/V6vVF/vhAOxZBG9gHwnDMDrqK+lc7vhz49/HGw1hGMp1XeXzeRWLxWjEOwxDeZ6nlZWVF/GxAAB4YVZXVxUEgaTvO6Fd1236ndHq7G7DdFrn8/moM9ss3dpuxBzAD4uz2zcAoHNhGGp+fl6NRmPbjdOSXis9mfomPVnL7ThO1FN/6NAh1ev1qMd+u+sBALAf2bWt0WioVqvJdd2m9dn5fL7p51Yd3b7va21tTYODg/I8T9VqNdp7Jf5eAH7YGPEG9pFsNqtTp05Fm7Yk9cInrfu2NRqNKHybzdjm5uYkScViMXpO0ig6AAD7nRmldhxH5XJZmUxGq6ur0SwwUydNLZSal2+Zf8tkMiqVSnJdV77va3h4WKVSKXpOfM03gB82RryBfcbzvKbN0Wq1WjRFPAgCDQ4OqlAoRGeRSs299fY6tEajoWKxqJ6eHk1PT2tgYED1ej36NxoMAICDplqtyvM89fb2qq+vT6VSSZVKRUtLSxodHVWj0ZDneVEdjXdgxzu8u7q6dP/+fR05ciQK4UEQqFgs6qWXXtqlTwlgryF4A/tIJpPRuXPn1Gg0onO7pScj1Gtra3rllVfkOI5834/CeDx8m6+mNz6Xy6lWq+nMmTOamZlp6u0HAOCg+bu/+zv19fVpcnJSlUpFQRCov79frutG+56YGWHb7Wpuamy5XJbneVF9tTdaAwCJqebAvpLJZOQ4jgYHB7WxsaFsNqv+/n4NDAyoVCppY2ND9Xp9y26tSWeTSk+mmnd3d6u7uzva2TwMQ/X09Gh0dFTHjx/ftc8KAEAazLKqTCajbDarXC7XtC7bjHQnBW/zGun7ndDz+bwGBgbkOE4U2guFgkZGRnTq1KkX++EA7FkEb2AfGhoaUjabjXrSTYA2vfXSk/Xg8fXehvm50WioXq8rn89rfn4+mmKezWZVKpXU1dW1K58PAIC0LC4uamVlRa7rRsHb1Ez7+053Je/t7VWhUNDGxkZTB/aPfvSjaA05ADDVHNiHzp49q/X19aa12F1dXapWq1vOIo2v77Z/NmvEzWi5Wa+Wz+dVKpV28yMCAJCKhw8fRjXUdELHNx1tdZ63zfM81Wo1FYtFra2tqVKpREu8BgcHde7cuRf2mQDsfQRvYB/6yU9+oiAIdP/+fW1sbERrzDzPa+qpt9lrvO3fZbNZbWxsqFQqqVqtKpPJqLe3V4ODgy/0MwEA8CLMzMzI87yo8zoIgugYzXj9jG9Mav++Xq9raWlJhw8f1tTUlAqFgsIwVF9fn06ePKmjR4++2A8GYE9jqjmwTy0vL6tWq0l60kOfy+WajhNLktSYMK8tl8vq6upSPp9PPCMcAICD4OjRo6pUKlpcXNTy8rKq1ary+fyW57XaGM1soJbJZKKp5IODg8rlcmo0Gjp16pQuXryY9scAsM8w4g3sU0tLS6pWq9GmaNKTEW/T474de+dVszOr2RSG9WgAgIPqJz/5iVZWVjQ+Pq56va5CoSDf96P6aR5Jx4iZEXJzckixWFQul1NXV1d0jBgAJGFYC9inCoWC6vW65ufnNTs7q3q9rmKxGBV+03CIMw0HO3Q7jqNsNhvt6sqINwDgoMpkMvr5z3+uQ4cOaW1tTdKTnc7NjK929dNMSzcPeyq6pC2/BwCD1jWwT/3oRz/S8ePH1dfXp97eXuXzeRUKBRWLxaaGQ7z4m4aD53nyPE+SorBtRr0fP36sb7/9djc+FgAAqcvn8xoeHlZ/f788z1Mul9PKykp0JKekpjpqrwX3PE++70dHidk7omcyGfX392t4eHiXPyGAvYbgDexTJ06c0Ntvv62xsTGtr69Hu5E7jqNqtSrXdZuCd7zhYHrkTei2p5uHYajFxUXdvn17lz8lAADp6OnpUXd3d1Qry+Xylo5re5aY67qq1+vyfb9pF3TpyaZspgYvLS1pampqNz8agD2I4A3sU729vRobG9OpU6dULpdVr9ejXcml5jXc8R57z/PUaDSUy+XkOE50bIrpsS8UCqpWq/r66693+VMCAJCO4eFhDQ0NyXVd+b4fbTBqdjC3Q7fv+9FIt9lYLX70mOM4yufz2tjY0Pz8/G5/PAB7DMEb2OdGRkb0xhtvKJPJaHV1VZlMRt3d3dEOrXbDwYRus/mLvabbnirnOI48z9PU1FQ0mg4AwEEyNjamkydPRkeDmTO4zZRxUz9N6LY3XTMh2wRvU0NN7a3ValpfX9/lTwhgLyF4A/vc0aNH9e677+q1117TwMBA06Zp9ui37/tyXTday1YqlaLdWCU1jXjn83nl83l5nqc///nPbBQDADiQDh8+rHPnzkV1ztTO+H4o8ZlijuM0nQJiXpfL5TQ7O6u//vWv+stf/hLVZAAgeAMHgOM4Onz4sBYWFrSxsaEgCKIgLT3ZZbVer8t13ain3m402A975LvRaOjKlSv63e9+J9d1d/lTAgDwfPX29mp0dDQK3SZwu66rWq0W1c4wDKPRbVM/4+zlWpVKRZ9//rm++eabXfhUAPai3L/927/t1nvv2hsDB43jOBoYGJDrulpYWGg6EsXzPFUqlWgHc7PzuTmzW1LT6Lj0/U6u0pONYx4+fCjP8zQ0NKSurq4X/wEBAEhBsViUJH3xxReSJM/ztLm5GW2iZpZmxTutTSe1+bf4RqYmwH/99dc6ceKEDh06xFGdwN7x/+3GmxK8gQPATG8bGRnR559/rkwmo0ajoVqtpkqlItd11Wg05DiOCoVC07puO3BL2rKbqwnli4uLevz4sSqViorForq7u3fp0wIA8HyY+tnd3a25uTktLCxEm62Z6efZbLbp2E3zOvPV7rw2e6qYXc8zmYzu37+vmZkZSYqWeQHYVbsSvDO7uO6EBS/AcxaGof7jP/5Djx49ajryxDQIent71dvbm7guzbw+CIKm3VuDIIgaEfl8Xj09PRoaGtLQ0JAajYYuXLig4eFhevIBAPtSEARaX1+POpcXFhbUaDS0vLysR48eyXXdaO+T+E7mdg01tdMEd9PGDoJA5XJZAwMD6u7uVrlc1uDgoF555RUdOXJkSwc4gNTtyn90zm68KYD0vP766xoaGlKtVotGrIMg0K1bt7S5ualisahCodA02p3Ucx+fRmfWfC8vL2t1dVUzMzMKgkCVSkXDw8MqlUoaHBxUb2+vstmsenp6du1vAABAp3K5nPr7+9Xf3y9JWlhY0KNHj7SysqLV1VV5nqdCoaDe3t6oPiYFb8OMkBv5fF5BEGh2dlZBEETvt76+rrGxMYVhqK6uLnV3d0fBHMDBw4g3cIDVajWtra1pcXFR//mf/6nl5WX19/err69PpVIpOjbFHq02R6cEQRA97P+fsNeuBUGgWq0WTdN7+eWXNTIyIsdxdPLkSRUKhWhNOSPiAIC9LAxD1Wo1ua6rK1eu6MqVK5qYmFCpVFI+n9fw8LDK5XI08m1vUGrvgh6vm2bPlfjxnkEQ6MiRIwqCQIODgzp8+LCOHj2qkZER9fT0qFgsMhoOpGNX/sMieAMHkAnO3377rT7++GN9+umnKpVK6u/vVy6XU19fn3p6etTV1RVtxGaYhoFpFJh1aubfzFc7lIdhGK0jD4JA2WxWIyMjOnbsmE6cOKHTp0+rXC5vCfkAAOwF5tjN27dva3JyUleuXNH09LRyuZy6urqUyWTU1dWlQ4cOqVQqNe2XYmaWmRpoOqhNvYt3bpvn2eeEm2nqxWJRx48f19tvv61XXnml6T0APDcEbwBPz/5v+Y9//KOuXbumhw8fam1tLZraNjIyokwmo2KxqGKxqFKppK6uLjmO07TO2zQMzPf29e2N10zDwff9pvswr3ddV67r6vDhw3r99dd14cIFjYyMSBKNCADArjO1zfd9/e53v9MXX3yher2u9fX16BjNfD6vcrmsbDarYrGorq6u6HQQOxTbm6pJW/dQsXc8tzdhs2eS2d+Pjo7q3Xff1auvvqpSqRRdE8AzI3gDeHqe5+n3v/+9Pvnkk2h6WrVajc719n1fw8PD0ZS4fD4fjXgXCoWmke+k0G3+vyI+1Tzesx9/jv26Uqmk48eP66233tIrr7zygv9CAAA0e/jwoT799FNdv35dfX19UbBdW1uLllKVy2UVi0W5rhuFaVM7zXIqSdG+KlJzQDa11NRiUxvNUWWG3Sa3R8SPHj2qixcv6uzZszpy5Eiqfw/gB4LgDWDnZmZmdO3aNd24cUOO46her0drz+zAbMK2Cd65XC46j9T+av7dDt4mSNu99GbHdNPQiK91S+rdN88tFot688039dOf/jQ6mgUAgBfp7t27+vLLL/Xtt9/K9/1o53I7NNv1zQRve9mUqWn5fH7L9e066Hle0y7nvu9HNbfVsZ6u60Yz1hzHUW9vr44dO6Zz587p5ZdfTvEvAxx47GoOoHM3btzQxMSEFhYWtLKyEp2vba8Hs3deldR0DIrZtTxe+M335hgye9TbXtMd39XVboQYdvC3jyWr1Wq6efOmenp6dP78+WgKHQAAabt69aoePHigpaUlra6uKgiCaBPQpNpmfmfqpR28TUe2eV586rg9wi19X2NNh7ekxL1PTJ011/J9P6r1c3NzKpVKGh0dZd8UYB8heAP7SBiGWlpaUqVS0ddff62JiQnVajXl83mVSqWmQh4PwHZxNg2EbDbb9PxWve524DaNBztoJx2pEn9/MwJvRsvX1tZ048YNFQoFnTp1iuPHAACpcV1XMzMzymazun79uqamphSGoRzHiR7xWmaPWJt6aTqs4xuexWuevRRLag7X8c7ueHi2Z6aZe8jlcgqCQPV6XTMzM/r888/1j//4jwRvYB8heAP7hOntvnnzppaXlzU9Pa0gCKJjwezpcFLzWjG78ZA0FdzuuTdf7enlQRBEjYP48pSkkXXz+yT2SLrZMTaTyWhsbIyzSwEAz129Xtfc3Jz++te/KpfLRQHchGm701r6Pvgadv00ddDUznjNtUe8JUVLv+xr2Vp1XJulWfZ17Hp9/fp1vf/++xoaGmLJFrBPELyBfaDRaGh9fV1//OMfdePGDeXzeTmOo3K5vGVH8nb7NphGgr1+2/f9punnhrmWmRJni0+Zs7+P/xx/nb37axAEGh8fj6bivfbaa8/yZwIAoInneZqZmdHVq1d169YthWGonp6epn1JJG35ar6PP+zOaBPGDVNDW3WCm5/jj6Q6mrRsy+4kD8NQf/3rX/X++++rv7+fkW9gHyB4A/vAxsaGvvvuO924cUM9PT0qFApbRpnjx30ZrQJx0pQ605gwvzM9+2a6m3m9/V5J4uu94x0D9vS+rq4uTU5O6siRI7p48SJHpQAAnoswDHX16lVdvXpVKysr6uvrawrc7epNUq3c7nUmRJsZaPENTuPPjS8DSxr5tjurG41GNEIfBIG+/PJLeZ6nd999V8eOHXvqvxOAF4PgDewDExMT+u1vf6uenh6VSqWmtdlGvAddSp7CZk+NazUt3f43Y6eBOOn5dmg3I+mmITE9Pa0vvvhCb7/99o7eBwCAJL/5zW80Pj4u13XV29vbtFmazZ4mbm+q1ulMrp2wa2B8nbiR9D72vZjp68ViUdeuXdOZM2cI3sA+wLwUYI/7/PPP9emnnyqTeXJkSS6XUz6fb1qXllS8kxoMdkMi3qho1ZOfNDL+tOKvNR0IhUJBi4uLunnz5lNfGwAA4+OPP9aDBw/k+360+WihUIiWapnwate4dvuVtBqJbjc1PC7pd/aZ3vbeKtvVWlP7C4WC7t27p3v37u3sDwTghSN4A3vY3NycpqamtLq6qnw+37SraqvgLCWPVu90Sl1c0jQ589VMgbMf8Sl2SY2J+OjD2tqavvrqq6f4SwEA8L2bN2+qVqs1dVKbh11D7RrUaU01tqudSeG5XX2162nSWnCb3XE9OTmpqampnf+RALxQBG9gD7t//74WFhbUaDSaji9pF6Q7WbsmbT9yHV+Xbb8m6WurRsJ2vfem4eM4jlzXJXgDAJ5ao9HQd999p7W1tWha9nad1VL72tmq4/lptHttvIYmhfD4/eZyOVUqFc3MzGhiYuKp7wtA+gjewB5VqVQ0Pj6u1dXVpunk8aO/bEnHlCRpd5zJdqF9u41i4iPg7UK5/f7ZbFZBEOjhw4ean59/poYNAOCHKQxDffXVV1vO2263bnu76yV9NXZSP+Pi67xbhe6kmWTx1y8vLxO8gT2O4A3sURMTE5qfn5fnedFGaknT2NqNKMc3Tetk3Vgr7V5nGhumgWDWq5lHvPFgNyDiDaIgCPTZZ59Fx50BANCJMAzl+77m5uaaNiFLWoMd10l4btW53WpdeJLtNmoztbJVDbVreXzDtfjRnwD2Fv4LBfaoycnJKHTbvfRJYdoOsza7F327qelJtgvbrf7d7Awb313dXDMequ3wbdasAQCwE57naXx8XLOzs+rq6mq7POt5zapKCtztllbZ9dgcJxZf422zzwpvtX9LLpdTrVbTwsLCc/lMANLBiDewRz169EiNRiOx4dBqSvdekHRvQRDI9335vh99H79ne9T7k08+2TOfBwCwP1QqFX388cfRZmrtOpWfdRZYGuKzx8yIt107fd/fcs+5XE7ValVzc3O7dOcAOkHwBvao2dlZBUGwZbTbFFwz/cx8bbUGrJ1Wvec7WaOWxA7fpqHgeZ5c11WtVpPneapWq4nvaz7vRx99pEql8kz3AQD4YTEbqdk7lUut10/vNIC32vU8qXYm1dJOa6y5T9d1Va1WValU5LpudO/2tUzdXF1d1UcffdTR5wDw4hG8gT0mDEM9fPhQQRA0FXO7URBf+2WPHneyW/l2DYbtdiGPX6vV5zAP01Pvuq7q9bqCIFA+n99yHVvtXIcAACAASURBVHvn2atXr6pWq23z1wIA4IkgCLS4uJh4HrekqB61mi22XQDvdAR9pyE+/r19XTt8m5poOhXi9xSGoTzP6+i9Abx4rPEG9pgwDFWpVFruvGoaDvHi2m6H1u12b221Q3mnWq33jo981+t1eZ6ncrmscrm8ZQ26HcA3Njb03XffyXEc9fX1dXwvAIAfnnq9rpWVFbmuq3w+33J9dxAETTWu093NO1kj3klot9+31ffxMG1GuO3OafNv9mcMgkBra2ttPweA3cOIN7AHJfXW21PNPc+T53nRui976tl2I9XtGhfteuuTOgE6ZToL6vV6NMW81bmq5giYfD6viYkJbWxs7Oi9AAA/PK7ranV1tWmauc0ePbb3GdlulLrTaeH2rLN4sO90x/Ok65qvpjbau7THR8t939f8/HzH1wfwYhG8gT3I931JydO6wzDU5uamPM/b0nPfbvdy+3o7WcfdqtGwk/Xh9qi353kqlUqJrzNB3IxWmM8HAEAnktZgS9/XoY2NDbmu23RMVzws7zSEd1JTn+bf7eVaQRBI0paTTuya3Gg0tLi4qEqlQu0E9iCCN7DHhGGoiYmJLVPI7Mf6+rrq9Xo04t3JmaTtpqB38nO7Ee92P9vTzev1ujY3NxOn+iWt875586aWlpZoQAAAtpXJZLaMCNsPz/OiDT7NqHfSOu+k68brX6ua2q72dlpDbfGNSlu9lzmOU5J++9vfbjmWDMDuI3gDe0wYhrp//34UvHO5XNTDbQJpuVxuOpYr6WiuJNsFcfMcew1Z0r+3ep+k68ZHEhqNhmq1WuIO7Pa9FYtFFQoFffnll7pz506bvxgA4Iduc3NTU1NTW2Z22T87jhOdrhHfnHSn+5rE36OTwJ302lbsmhkEgVzXTdw4zQ7djvNk6yYTwAHsLWyuBuxBZqp1/GGmkplzPE2joV2RbddAiD/HsKest5tyF5/iHt+sxr6ePdXcPPL5fHTv5jWmAWHOMF9aWtLKykrHfzsAwA9Pq/Braqdd10zHtX3kmAnf7cJwUphPek5SkI/X3Pgou7lu/JQSc6+mbsZrfvyzhmGoqampnf75ALwABG9gjwnDULOzs8rn84mNiFqtpjAMVa/XJUmO4yQW+HjjYCfB276Xdg2LVj8nHc/SaDQ0MjKis2fPanJyUuVyuWmtmrmOaTiY3vuNjQ1tbm7u9M8IAPgBqdVqWlhYaFn3zO9M/TQdv6buxG03kt0qfLfqgLZ/Z3dWm/oYf13897VaTfl8PuowiN+L6aDP5XKanZ1liRawBzHVHNiDarVay5Dsum7Tzqz2VO5WYTtp9/D4c1o1KNo9knYmN+KNCt/3VSwWdezYMZ0+fXrL5jZJ1zXT5hYXFzUxMfFC/vYAgP3HnJwhta5dph7VarWmU0HaLXtqVeuSammr17cahY/PaEsK8fY+Lo1GQ57nJdZM+3pmd3N7TTiA3UfwBvYQs+NqqyIfhmEUtuPr04ykXvBOeu3jr41r1XBoN5Iufd9r32g05DiOjh49qsuXL6tarUadCK0aPKb3fnl5meANAGirXX2zO6nN5qTxXc3j12hX65Ke+6w11BbfbT2TyUSd7mbjtKTOATNbbGpqiuAN7DEEb2AP8X1fjx49ajuSbNZ4Se3P3bavkbRWfLvGQlLjodUoQHy6eHy02zRuurq6NDY2pjfeeEOZTEbVarVp1D7purlcTrVajXXeAICWTId0JrN1Z3P7OUkj3vYO4O3CcbsQbr+2ne1Gw819xke7pe9H9eNT0+P3KEmPHz+OjiADsDcQvIE9yKx9jq+BNqE7vhNrUo+9sV0vfVy7hkOrQt+ux96cP2o3bBzH0QcffKAwDJumzNvs6eau62plZYXjUQAAW4RhqEqlotnZ2Zad1uZIS1M/TfBOCqetOq47GdmO31e8trU6riz+HLvWS9Lw8LAOHz7cdJqJ/Xr7Ph3HSVy3DmB38V8lsId4nqcHDx4kFvowDLcE72q1qmq1mjhde7u1YzvVbnfzViML8Y1j+vr6dPLkSeVyOf36179WNptVvV6X53lbwrk91dxxHK2vr+ujjz56ps8AADh4ZmdnNT093VTv7E5ruw6ZR7VajYK4XX86nTq+E0mz0+yf4/XSPOz7MkvNarWaNjc3W06PN8F7ZmaGqebAHkPwBvYQ3/c1MzOz5TgxqXl3cFOMXddVvV7veIOYnTQikhoEraa2x39v36d93nh8A7i33npL+Xxe9Xq9abM1w+69r9Vqun79+tP/cQEAB5LjOCoUCsrlcls2HpO+r0n2DKuRkRGVy+WmOtVqanarTuVWS73iz7Pvwa7h9qww+z5qtZrq9Xr0Gkn61a9+FXVYJ+3vYu7TTLWfnZ0leAN7DMEb2EN839fjx4+jaWLxxkN82na8YMfDd6tGQaswndRQiH+1e+PtBoT9s1mXZgfu/v5+DQwMRPeQyWT04Ycf6s0331Rvb688z4saPvY953I55fN55fN5hWGoTz75hCnnAIDI0tKSFhYWohlS9mi3lDwDywRYU3vs+mkH23jNiz866Zi2a3Q8ZMcfJvybGipJR44cUX9/v0ZGRnT+/PloqnzSqLeZKRaGof72t79pYWHhBf2vAGA7nOMN7BHmDNIgCBKnh5viagfs7u5ulUoleZ6nQqGgIAiightvaCStITPajYLHX2+mvdvf240Q3/ejUQV7tNtxHOXz+aZr9/f3q1wuKwyf7DLbarMae6Ocx48fP+VfGABwEK2vr2t1dbXl8io7cJt69s477+i7777Tw4cP5fu+HMeJapZ5TXz0PL6Pivldq/1SzHXs6yX9nBTETUiXpEKhoGw2q+HhYV26dEnj4+NyXVf5fD5xT5dsNqt8Pq+NjQ15nve8/9wAnhLBG9gjzMYwjuNEm6qZBoRd7O0e9uPHj6tQKOjx48dRsXYcZ8vGK9lsNrEXfrtp53YjIWmU2Q7epkPAbjiYtdthGOrQoUPq7+/fco3Dhw/r8OHD0fEuZldaM8U+l8tFD0nsbg4AaOJ5njzPi0Z7k3YIt2dl9fb26uLFi6pWq3r06JE8z4vqjKk9rU7tMP+2XeiOL62Kh//4VxO8Td20R74PHz6scrms3t5enThxQsViMXqeqY126DZLtFZXV+W6brp/fAAdI3gDe0StVtP8/Hw0Tc40HgxTxO1e8jNnzqirq0vLy8tyXTeaZmcKsT36nTR6njS6HH+O/XN8Gp0dyu0pdHYjwgT2M2fO6PTp01s+98WLF6PPfO/ePfm+39S4MQ0I837Ly8va2NhQT08Pu7YCAKIAa3daG/FgK0kjIyNyHEcDAwM6dOiQ5ufno4BqOrjjHeD27Kt4DU36Of416X7i09btI87sun327FkNDQ1JerKe/ejRo9FxYfYsOcPUzampKS0sLOjo0aMqlUop/i8AoBO0WoE9olqtan5+vmljtaSecknRVLlsNqvR0VG9/vrr0QZlm5ub0U7hrutGIwFmHVt8LVtSkLYlrZMLgiC6fvxh7sPzvKb7HRgY0OHDhxM/+7lz53T69OnodfF1bmY9Xi6XU6FQ0CeffKJ6vf58/vAAgH2r0WhobW2taap5q+AdhqFKpVJUXwcHBzU4OKgwDKOaZk4LMbUsXpda7Z3SqhPbHoW2R86Tjjuza7PpAIiH6kKhoGPHjkXr0+P7u5jXdnV1qbu7W7dv39a9e/ee698cwNNhxBvYI8yId19fn6Tm9WF24D106JDW1tZ0/PhxvfrqqxobG5PruspkMqpWq+rq6lK1WpXnecrn81t67e2zwc3PnbLXopnv46Pbdi9+0pq4Vo4dO6Y33nhDd+7ciUYupO8bI/b9x9eKAwB+mGZmZrSxsSHHceQ4TuLabnsEuVwu65/+6Z906NAhDQ8Pa3NzU3fv3pWkqI4VCgWFYbjlmnYn8HYnhcSnmdsbn8VH4CVF67VNJ4D0JGTHp4r39vbql7/8pf70pz8pn883zQgzHQ5mdNv3fZXLZRUKhWf+OwN4dgRvYA9YWFhoWt8dL/Txncsdx9H/+3//TydPntShQ4d08uRJOY4j3/dVrVbl+74KhYJ834/WrZkQbkJtfOfXeOPBbjTEd0s169DM/dlMyDeNCvs4tHbMaL3v+yoWi03HusSPQ5uZmWl57AsA4Ifj8ePH2tzcjGqaPdptArfZ9FOSTp06pfPnz6tYLG4ZmTZ10nGcqPaaGmrqc7uN1JLYU8bjNdWeim5qey6XU7FYjEbbk2pzNpvVyZMntba21rSnSzabVbFYjPZ6cRxHExMTGhgY0KuvvvqUf2EAzwtTzYE9YHJyUg8ePGiaZm7EN4ZpNBo6fvy4zp49q76+vqgIZzIZFYvFqDFh96jbPet2A8K8zv7a6mEHdRPkS6WSisVidNyXHeSLxaIKhcK2I93GiRMndPny5SjUJx3zIj1pWMzPzxO8AQBaXFzU5ubmltlcUnPHtZmW/d577zXtBt7d3a3h4eFodNuMdpu6Zzqs4+dv71RSgLY7xu0ZanZdbhX0R0dHm+7FXMt0NMT3hQGw+wjewC5bWlrSo0ePtLy8HI10J00ztwP4j3/8Yw0MDERTzAxTwIvFYtQzbxf2pPVl8ely7X5nRrzN0WD5fF6FQkHFYlGlUkldXV1RGLenuA8MDKirq6vt36FUKunIkSN66aWXms4zjTcYMpkMoRsAoLm5Oc3NzaleryfujWLXTfN9uVxuuoY5esuucYVCIZrGbTrC4yE4qT63C7hJR4kZSdPWTfhPumYul9Ply5ebOqRN4DYd1vb09oWFBdZ5A3sAwRvYZRMTE5qfn492UU0q7PbGMOVyWRcvXkzcodT00JtGg3mYnvz4Ji2diAfw+Ei4PQ3Pfj/7Nd3d3R2ty87n8zpy5EjTCH1SJ0A2m9X6+jrnkwLAD9jGxoZqtVq0hCoeuuOzpbq7u3Xo0KEtm5WVy+WmOhM/WixehzoN2K3+3X5eq+Ae/97+2Uw1l76fTt9ultjm5qbm5+e3+3MCSBnBG9hF6+vr+u6777S2tqZisdg0zdwuzvYxIwMDAxoZGWkKsmZdl/T99DUzEl0oFKKNypKub75PagzE/y1uu177nU5vs6cFmjVr8VEAE7ynp6dVrVZ3dH0AwMFRrVajEevtzu8uFAoaGxvTsWPHmpZzlUolDQwMNF03qa51cgrITsVrrf27TjrJe3p6mgK3eZjr2BuqAth9/JcI7AKza+mVK1c0OzurMAyjUeqkXnsTSO1jtmz5fF6jo6OJ08MltRzpfpqGQ6te+qQe+1Ybt7VjTwm0P0f880xNTalSqez4/gEA+5uZVn39+nXVarWmtdHxemTCaLFY1Ntvv93yerZWy7060W6n81bvnfQ+ppO53fu+/vrryufzTSPdptNaUjQrrVqtamlpqeN7ApAOgjewC8Iw1P/8z//o+vXrajQa0bpoMzptCrcJofYRXj//+c8TjwB7lqlvz+Pz2Ju/mcJvfn/8+PHomLR2SqWSzp49Kyn57FOb2cwNAPDDEoah/u///k/z8/PKZDJR7UzamDQIgmiH8PjRXJKipVzx67cK3O1q6U4Dd9L3SbO8Wr3XL37xCxUKBdXr9WjpVdL0+EqloqWlJTZZA3YZrVbgBatUKvrv//5v3bp1q2lX8Ph52+ZhQrfjODp+/LguXLiQGDiTRrp3Mm38aXSyscxOw7HdW5/0mcz3rY51AQAcbGEY6quvvoo2+LT3Fkmqf+VyOTpGrNXsr3YBOz4d/Hl+jlbsAN2OGdG3zyq3ZTIZOY4jz/M0Pj7+7DcN4KkRvIEXqFKp6P79+7p9+/aW0G2H7XivfRAEyufzeueddxKnsbmuq+np6ZZTzdvZSUOiVUOkXejeaThuNeVvu98BAA6+Wq2mTz75JNo01K6f8Zpjwmi5XNbw8HBiR3BPT49GR0e3/L5VB/bz0G5flfjyqmw2q9nZWW1ubiZe69KlSxocHGzaUC2+NCubzUbtBAC7h+ANvEALCwu6cuWKMpmMSqXSlkaD/TDTtO1p3Ek7mVcqFS0sLCgIgpZHkpivzzJlrl3vf/xr/D6WlpY62ggtn89rZGSk5X3ZDaGlpaXEaYMAgIOpWq1qcnJSN2/eVKFQ2FJDDfO92RulWCzqyJEjidd0XVcbGxuJ+4nY7CVUTzsC3mkNjt9Lq/1dJEXHdyZdK94hwVGcwO4ieAMvyOzsrL755hvNz88nhm7D7v02a8/MkVyHDh3acl3XdVWpVJqmqEvJG5vFe/BbaTeivNOGgyTNz8+37K23OY6j3t7exF77uM3NzWj3VgDAwbexsaFvvvlGa2trKhQKW+qeYdfQRqOhrq4ujYyMJF6zUqlER20ldYKbjnBzXfv6T6OTsG3/bI4Da3V8ZrlcluM42147DEM2JAV2GcEbeAE2Nzd19+5d3b17V93d3VEPtREv6qaxYHrrS6WSTpw4oZGRkS0NDM/ztLm52dSr3W66+XaNhU6np3fa8AjDUCsrK6rVats+17x/oVBo+R7xhhAA4ODzPE+Li4saHx9v2ohU2hqE7RljmUxGPT09LUe8a7Wa1tbW2o52S3qudaeTOmw/5ubmWnZeHz16VF1dXU0ngiRNXfd9n7O8gV1G8AZS5nmevv76az148EBBEETT45LO6zZfzbnd5oiQwcFBXbx4MfH6c3Nzunnz5pbgvdPQ/Szrpltd1zSMdrLWO5PJRNPNW103DEN2NAeAH5ClpaXoGMlCodDUCRt/mM5r3/fV29ubOFvMZgJ6q01K41+fx5rvTl5v6tzCwoI2NjaaNh81Tpw4ob6+PmUymS3HcRomeHOkGLC7aLkCKQrDUH/+85/1t7/9TcvLy9FmavHN0+JHcfm+r2q1qmq1qtHRUV26dElnzpxJfI9WgTvpsZ3tntPJFPX4+9mbx3XaULH/JvFRjLR2lwUA7E1hGOrhw4f69ttv1dXVFU2ttutEfImW67qq1Wo6c+aMzp8/3/LaGxsbevz4cWKtNPU1rtOa2ql2NdtsjjY7O6tHjx4lvv7VV1/VSy+91FQz7RoZ70igfgK7g+ANpCQMQ33zzTf66quv5LquisVi00h3/LmmseB5nlzXled56uvr01tvvaVLly61fB/TaIif3Wlr10B42sZDfCpbu+dlMhnNz89rbm5u2+tms1mdOnUqccqcfc3jx4+rq6vrqe4dALB/zMzMaGpqShsbG1EdNXXH1E7z1Txc19XAwIDGxsZ09OjRltdOmpXVKljvRmA1wdscmZbEtBvMZ48fy2nPDrhz586LunUAMQRvICVhGOr69evRmmV7MzX7OfH13KaXvq+vT7/+9a919uzZjqZVt9rVNakxkbSJy7NqdV0TvBcWFjpeX2YaDvHRDCM+Eg4AOLju3Lmjubm5xLXd5mtSHb148aJGR0e3rXGtZoe12y/lWXU6C80srVpcXNTCwkLi84aGhtTf39+2bj7v+wewc8ldZwCeSa1W08cff6z5+XnlcrktO5jH16GFYRg1GKrVqiqVit566y0dOXJExWKx5fusrKxodXW15c6uST8/S/HdycZm8caL+azbyWazOnr0aNPmOPEp5o1GY9u/DQBg/5uentajR49UrVbbbqpmL9Wq1+vq6enR2NiY+vr6Wl7b1NBOZoU9z/Ddaei2O69Nx0KS4eFhDQ0NRZ8/CILob9PJSD6AF4MRb+A58zxPCwsLunv3rhqNhnK5nHK5XBSO4wHS3kjN8zwVCgW9++67OnfuXOK53bZ6vS7XdXfUmx//udPXtvq3Tt+302KfzWY1NjbWFLxt9vEw9s7wAICD59tvv9X6+roymUxTB7Ydvu2ZY57nqV6v66WXXtLg4KAKhULLa5saup2k8N2qpm23+Vq7EfWka5rfm31fknR1denw4cM6cuRI04yxVhutAdgdjHgDz5k5Z9TzPHV3d29ZjyZtbSiYtVmFQkEvv/yy/v7v/76j4ri5ualKpfLMG6ftZMpbu9cn/btpNJjOhe2YEW8zmp10TIzpzQcAHGz379+X53lbaqkdcE3YNHWmq6tLFy9e3HYfELOfSqsZY9t1Ou80xCbV0fjv4j+bNd6rq6taWVlpee3e3l4dPXpUS0tLiZuTxu8dwIvHiDfwHHmep8ePH+vKlStN67qTdjG316L5vq9CoaBTp07pH/7hHzoujOPj47p//76k1sE4aV33diPRne5e3snvzVS35eVlLS8vd/S5JOnYsWPK5XKJZ7M2Go0dHVEGANhfwjCU7/taX19XGIZNM8fizzPh2xzB+frrr2tsbGzb5UiLi4taXFzccj3bdqPc2+2Z0mrteKcd3uYzr66uanl5ueV08/gsOvZCAfYeRryB5yQMQ924cUOfffaZyuWyisViU+i213WbXvlqtSrf93XixAm98cYbbXcvT2J6wpPu5Wmnhz8v9o7nJngvLS21vTfbyZMntbi42DSqYToXgiDQ2NiYuru70/4YAIBd8uDBgyh4ttonxezk7fu+wjDU0aNH9atf/aqjOrO4uKilpaWWHdDS9zWs3ckhz6LTvVOy2aw2NjY0OTmp06dPb/n3M2fOyHEc3b59e0vw7uQEEgDpY8QbeE6uXbumW7duqV6vq1QqNR0dFg/cruuqXq+rXq/r5Zdf1gcffKDXXnttx++5uLgYjSK3O996u03XnlWrUQDDdBCsr6/r4cOHHV2z3TneJ06cYH03ABxgYRjqL3/5S9Nod1JHdqPRkOu60frn9957r+P3WFpaajt9274XqfPa+azPi4+Om86H9fV1TU5Otrxeb2+v3nnnnaYN1uLXJXwDu4fgDTwHk5OTun37thYXF1UqlZTP56Pe+fjUcrPxS6VS0blz5/TBBx/o5MmTOy6Gk5OT2tzcjIpyqyPHdrq5mf2aTp/b7t/y+Xx0j5ubm5qZmenoumb0Iv4+ZsQbAHAw+b6v8fFxLS4uKpvNJh4hFq+rvb29+vGPf6yzZ8929B6Tk5Pa2Nh4qhrZiadZ/53E1PdWU+1tYRjK87xozXt8Z3OmngO7i+ANPAe3b9/WxsaG8vl8FLqlrQ0DE7prtZrGxsb03nvv6fDhw3Kcna/6mJ2djYJ3O3bh7aQhsN36bttOeuuz2ayq1arm5uY6uvbo6GjTGu/41EIAwMHkeZ6uXbsm6fvgGd/F3Ewxr9fryufzOnXqlH784x+rUCh0VOvM8ZY7Cd4vMrjG92LJZrNaXl7WN9980/I13d3d+slPftIUvO3rAdhdBG/gGYRhqGvXrmlqakqNRiNxpNtsoGamlzcaDb366qu6fPmyRkZG2h510s7c3Jyq1WpTsG1VWHcSpqXmAp10NMp209njjQXztVqtan5+vqN7KJfLLTfR6evroxEBAAdUEASanJyMjuNMWttt19fBwUGdPn1aQ0NDHb/H9PS0Njc3o/rZroZuN8rcTqc11LxPUuA237uuG61LT1IoFDQ8PKxDhw61PE4MwO4heANPqdFoaGFhQZ999llUvB3H2RK6zZrubDar/v5+nT59Wu+8847Onz//VCPdxsLCgmq1WlNR7nSX8qR1052sYdvJbufxhkM2m5Xv+1peXtbCwsK2n88E76RzvA8dOtRyaj0AYP8KgkCVSkW1Wq3pCLH4rCf76LCTJ0/qxIkTO3qfe/fuaXl5eUu4je9OnlT3kn7uJOC2Ole73U7o5mE6IYIg0NTUVMv3MEdyJm2uFoahKpXKtvcJIB20XIGnEASB1tfX9emnn6pSqURTzE0YNMea2KPcw8PDev/99/Uv//IvGhsbe6YRW8/ztLi4qFqttmXEO2mH1Kft9Y5P845fr1VYt+/Jvq9sNqtaraarV6+2fd9MJqOxsbGoY6LdxnEAgIOjWq1qcnIyOpIzPsXcBG9zdNjLL7+sV199VYcPH+74PTzPizqvk2ZnbTfry/59/HdJP3dSg5OCf7zz2gTvR48etbxONpvV6Oho0waldrtgenqaWgrsEoI3sENhGGp5eVl/+MMfdOvWLXV1dUVryuypb7VaTZubm6pWq9Eo949+9KPncg/3799XEASJ4TYevk1jxdx7UkMg/rukf4u/Ji7eeIn/zr7HTiUFe84mBYCDKQxDrays6OrVq1Fdjdcy+zjOgYEBvffeezp16tSO3md8fFxBEDTtlm5qlbS1diXV0Ph9J9VR87P9vKTvk2pm0kh8J+vRs9msTpw40XSkGDUT2Bs4xxvYoeXlZd25c0e3b99WX1+fisXiltBtHrVaTZcuXdLPfvYzHTly5Lnfi73TaasR70zmyc7i0pPQms1mtzQQzPOfZR1b0qh3EARbGjDb7cra7j1Nw4tGBAAcPMvLy3rw4IGWl5fV19fXdHyYGeU2s8l839e7776rvr6+Hb9PUqdwq3O6M5nMlmVhSTV0O9uNfrdaJhYf+fY8TxMTEy3fJ5vN6sKFC9F0/EajEW342u4EFADpI3gDO/Do0SN9+eWXunXrlnp7e1UqlaKA6XmearVatC7tzTff1AcffKB8Pt/xLqudmpyc7Ki3Pj4d3IRuO2zb/243INpNLW/1mjgzUmD35vu+3/YcUnP97777TplMpmm6obmHM2fOPNP6eADA3mOO5iwUCk1ru+3QXa/XVa1W9c///M+6cOGCisXiU71Xq/Ad30Hdfm681u2khsZ/l3Qvdm1udY++77cN3vEZAtt9BgAvDi1XoEMzMzP6/PPPNTU1JcdxopHuIAjkuq6q1arq9boKhYJeffVVvfvuu+rt7U2lyD1+/Fi+7zeNIrcbTW7VYIivBU9aG25/jX9vXhN/fbwBYt+fPfW9nc8//1zZbFbFYjHqrTcNMPtMVwDAwbC5uanV1VWVSqWmYznN9PJ6va4wDHX+/HmdO3cu6vzeqdnZ2S01tNVyLcOuo0kdzkk11fw+SavX20zNjE8976SGHjlyRL7vbwnc7daHA0gXwRvo0L179/T48eNoF1Uzmlur1aIe+HK5HOPc7AAAIABJREFUrNOnT+vNN9/U4OBgavdSr9clNW9iFm8sxDdpSRLvqbef20kP/XbsUXY7eLdSrVY1MTGhqakpPXr0SI7jRCHbPj5maGiI6XIAcIBMT09rYWGhaWp3/HSQfD6v0dFRvf3224lHTnbKrqGt1lRLaqoz8Q7spNHpeHh+munlSc+JTzcPgqDl84Mg0J07d7S5uRkNENj3V6vVWK4F7BKCN7CNMAw1MTGh8fFxua4bjcA2Go1og5dMJqOBgQGdPHlSFy5c2PFGLztVrVYlKXEjM/v7pLVqnaznbqddj3/Sc+P31Wg0Eo8zWVtb08TEhG7cuKHx8XEVi0UVi8WmUfIgCOQ4jgYHB6PREADA/nfv3j3Nzs4qn89vGe12XVdhGGpgYEBvvPGGXn755Wd6L1NDk0a5k+qo/bPRyYi1+X2Sdh3iSe/Tqq7bXNfV7OysPv30U62trWloaGjLUjEAu4fgDbQRhqHW1tb0X//1X9rY2IimtYVhKM/zogA5Ojqq8+fP6/Tp0xoaGkr1fnzf1/z8vLLZrPL5fMuGQKvgHX/e02p3jfiIu30/vu9rbm6u6fnValV3797VJ598orW1NZXLZTmO03ScWBAEajQaOnbsGNPMAeAA8X1fMzMzWl1djUayzf/vm2M5+/r6dPr06edyOog91bxV0I57HvVzJ2E9fl/2xmhhGEYzAMzzzBFpn3zyie7evauenp6ow9oembdHwQG8WARvIIHpHa7Vavr3f//3qBfeLoDValWVSkXvv/++PvzwQ3V1db2Q+7p///6WQJt0XFe7qebteu3t33fSU9/qeWZ6ub25WtJrwjDUp59+qq+//lq1Wk2HDh1q+lubKeb1el2+7+unP/0pjQYAOEAmJyfleV402m06mev1umq1mlzX1fHjx3Xu3Lnn8n6tAndSh7G9aan9+/hrW0maep70b3FJS8ZMTVxaWtK3336rCxcuRL+bmprSZ599pq+++io6cSU+YyybzerDDz+khgK7hOANJFhZWdG3336rK1euaHJyMto9vF6vy/O8aOrbz372M/3yl798YUWs0Wjo3r17krRlg7FW0+HiOn1eu9cnNR7ia8Sl5l5703hxXVfLy8u6deuWLly4oL/85S8aHx+Ppuubz2muaaYaZjIZDQ8P6+zZszu+ZwDA3vWb3/xGvu+rWCxG67nNLua1Wk2/+tWv9Nprrz23vVMmJyeVyWRUKpXanpltPO+ZY+3qaFINtZd4+b6/ZY33jRs39OWXX2p6elqDg4NN92jvCu/7PlPOgV1E8AZibty4oS+++ELffvttdO61GYE1xaurq0uXL1/WL37xi6cOsE/LBNhWu7BK7UN1J6PgrXS6Js3+XdJ91et13b17V1NTU7p9+7YkqVAoREefmMaCaTDU63X19PTo7bffpqceAA6IMAz1+9//XmEYKp/PR3un2MeGXb58WefPn9fAwMAz//9/o9GIZo2Zzut2D1vS0qm4511H42q1mlZWVtRoNHT37l2dO3dODx8+1PXr1zU1NaVSqaR8Pi/HcRQEQRSyzcyxTCajl156adt7BJAOgjdguXHjhq5du6aZmZko9PX19TWdf+15nrq7u/X++++rUCi80PsLw1CPHj1KHF02P7eb+taqIdGuBz/eC79d4yBpp3Tzsxm9zmazmpmZiXrts9msKpWKfN/X4OBgtGureTiOo8OHD2tsbKz9HwgAsK989dVXkhRNhzahu16va2hoSO+++25UF55Vo9HQgwcPWp780WkN3W7U266b8Tooba2jrc7/tv99Y2NDGxsbymazKpVKCoJADx480Mcff6z5+floQzrf99Xd3R11YNjTzPv7+6O9UwC8ePzXB+hJUbt3755u3bqlpaWlKBAWCoVoIxLXddVoNHTo0CGdPHkymhb9ormuu+00uHijIGmzs6TX2r+Ln8fdKuzbkhoP9vuajXIymYxWVlaUzWZVLpd17NgxVatVTU9Py3GcaHaBef6RI0d0+vTpF7KOHgCQvkajoYWFBVUqlWhWmb2Lue/7+sX/396dPcdxXXcc/3X37IPBPoMdIAlwsVZatkTKcZTY5Sq7Kv9K/p685Sl5zpNTlVTiqkRRYlOKyo4XSaYkUiC4gKIIEsQ6M909eWDd1p1Gz2AATgMc6PupQgGYrS9oq06fe88996c/1dTUlLLZbF+u2Wq1oon1Tkl0t8S70+uTnu/0vm6T10nP2dV2ZiI6l8tpfX1dv//973X37l21Wi0Vi0V5nifP85TNZqPFA9OYVFJfJi8AHB+JN77TgiDQ1taW1tbW9Nlnn+nhw4dRYui6btScJAxD5XI5ZTIZLSws9KWr6nG0Wi3t7u4eOFs0Saey88OS6MM+r5cxdrppMXu28/m8JCmTyahSqejChQva2dnRs2fP2o6RMd1YWe0GgLPF933dvHlTQRDI9/2oqZpZpT137pyuXr3a92Rxe3tbUvIk9VFWtLv9bh477Ozubr1R4pPX2WxWxWJRYRjK8zzdvXtXw8PD0Wkr1WpVIyMjWltbk6SoN430/F4nm81qcnKy678NgHSReOM7q9lsanNzU1988YX++Z//WeVyOWrs4vu+crmcPM+LGnvNz89rYmJCKysrWllZOfHxmuNDHj16pEKhkPiabqXn5jPir+u0N7uTTjcNSavd8RI7z/OiPWjlclnDw8O6ePGiZmdn9fjxYw0PD0fJuUm8h4eHNT09zQ0DAJwRYRhqb29Pv/3tb6NEOJPJRFu6hoeH9Ytf/KKvPT3M6RidzsmO/95t1bvTe4/ymsPKze3nstmsKpWKisWims2mdnZ2tLm5qStXrqhSqSgMQ128eFG1Wk137txRs9k8EPuLxeKpLRoAeI7EG985poHX/fv39fHHH2t1dVUjIyOSpEKhoP39/ShJNMnuu+++qx//+McqlUqnNu5Go6E7d+70dDMgJe9V6zQDf9jn9PrcYcePmQ6ye3t7evDggX7+859reXlZjx490t27d7W1tRXdRDQaDTUaDS0vL7PaDQBnyN7enr788kutrq6q0WhEzcDCMNTY2JiuXbum8+fP9/WajUZDX375pe7fv992ZKXRS/XYceJnL8dyJu0Hj7/WVOLZK+BXr15VrVbT+Pi4RkZG9PTpU125ckWrq6ttExmm5JyO5sDpIvHGd85XX30VlZX7vq/R0VGFYaidnR2tra2pWCyqUCgol8vpjTfe0M9+9rPTHrIkqV6vR0eJvYjj3Dz0+rnx3zuV35kbiJWVlegGyN6Xtru7q+3tbY2NjencuXOsdgPAGfLkyRO9//77UXWZOdUiDEMNDQ2lOtnaarUObNc6bJ/3iyTfvYhXjZlxdnu9ee25c+fa3tNoNKKE2/d9+b4fnZFOR3PgdJF44zvliy++0I0bN7S3txcF3mfPnkUz4DMzM6pWq7p8+bK+973vHTgr+zQ1m03du3dPUufV7E6SkmITmI+rW4l6UmM187g9s//nP/9Zly5dUiaTiRLwZrOphw8fynEcTU9Pq1gsvjT/GwAAXszt27f14Ycfan19XZLajr8Kw1DFYrEtmewXx3GiZqn2ynHS68z3eHzt1qm8H+OLN31LSsbNdqxGo9Hx+qbU3Pd9hWGocrncdr43gNNB4o3vhCAIdOPGDd25cyfqTr6zsxMdzTE2NqaLFy/qBz/4gYaHh6NV75dNq9WKmo91Sro7NTWLB/N44m3fTHRrCNOtHC+ecHf6HPNlyv7v3LkTdZrd2tqKSv5nZmZOtbwfANBf33zzjW7dutXW/Mus0pbLZU1OTkZxrp92d3f10UcfHZi4jk9iS51jalI/k05xtBfdys3jMT4+jt3d3Y6faRrW7ezsKJPJKJPJ0NEceAmQeOPMazQa+uqrr7S2tqatrS3V63Xt7OxEXVQzmYxee+01vfLKK5qZmenbsSX9FgSBNjc3JR1+fFinhi3mdzvBTpq5P2yvW9Jr4jcbSWOM30Ssr69rZGREX3/9tXZ2diQ9L6k3Z5TOzs6SeAPAGfHJJ5/os88+0/7+vnK5XHSKSLPZlCTVajUtLy+ncu0wDLW1tZVYTt4pATeS4l08liatUCfpFGcPi6nxMZuJ6/jzplLPNFRrNBoKgiCVyQwAR0PijTPr8ePH2traUrPZ1Oeff67NzU09e/Ys6l5eLBY1MTGharWqV199VTMzMy/1jLC5aUhqCiN1T5q7zcCbsrX4Z3RLsnstV+t2IyM93+e3ubmper0e3UQEQaB8Pq9SqaTR0VHlcrmergUAeHmtr6/r008/1cOHD1UoFOR5XtRp3HEcjY2N6cKFC6nu7z4syT4sATevkdqT76RzwZPe022S3HxmfCyH/Q22MAy1u7srz/PUaDRUKBSiKr+X+f4G+K4g8caZ1Gg0dOvWLd2+fVuO42hvby8KaMvLy1HZ1eTkpF577bVTHm1vTFJqVuSTkuOkIB4vfbMfM58ZLzHvtApgAnenmX37hiQ+JrOnzn6d4zhqNBrRDH29Xo+OcJuammKGHgDOiN/97nf6+uuvo+MkzXndOzs7KpVKevXVV/W9731P4+PjqVzfnBFuxyKpeyLbaSLb/oqvPCcl1XYctK/Zi27J9/7+vrLZbPS5zWZTGxsbymQyevLkibLZbHTud6/XA5AeEm+cSQ8ePNDTp0+j5iP5fF4XLlzQ9evXlckM3v/tTYC3G8L0MjNv3htPvk2ybbqexmftk75Mx3E76U46F9z83q1Mzv79yy+/lO/72t/f17NnzzQzM6PHjx/rL/7iL1QsFl/0nw4AcIpMnPniiy/k+75KpVL02Pr6ugqFghYWFvTWW29pYWEhtTE0Gg3dv3//QFzpJZYmxTM76bZjatLWKklt8btT7IzrVnlWKBR0+/ZtXblyJepJ47puVMJfr9fluq6CIDgw2QDgdAxeBgL0YHFxUYuLiwceH9TAs7W1pbW1tY4z6Ue9abBXu6Xns+S+7yeucJuk25SqHfWGodPKvOu6Wl1d1fDwcHSdQqGgZ8+eqdVqaWlpaSAnSQAA7f7+7/8+2kbk+762trb04MED5fN5FQoFXb16VaOjo6ldPymGuq574FixXtjbs6Tn8dPE0nh8tK/TawVXt8oxk7T7vq+RkZED7/V9XxsbGyqXy9rc3IwalFJmDrwcuKvFmTSoCXYvOiXanRJdW1J5XBiGbY3bTMM5U47veZ7q9bqGhoaiRNgk4PEbhF7HbX5/8OCBRkZG9ODBA3mep6mpKW1ubur73//+mf7fEAC+C5rNpv7t3/5Ne3t7ymazajab2tra0ubmpjKZjHK5nF5//XV973vfU7lcTn08nSq6etm2ZZjzxs2X4zhR8u26bvSY+YxMJhPFVZOwd1qBjq+GJ+35DsNQ29vbbSXm8c+o1+saHR2NOsfbk+YATg+JNzAgOt0cJJW0mZ/je7vtpNv+Mq+v1+vK5XKamppSPp/X7du3NTw8HK14mxuKTt3Q4793K0V3HEezs7MaHR3V3t6efN+XJBWLRV2/fp0ZegAYYPv7+7p9+7Zu3rypbDarIAj09OlT7ezsREeHvfnmm/rpT3+qUqmU+mTrYcm2/d38HI+h5rsdS81q997envb391WtVqNVfc/z5HmeisWistmsPM/reJxnpzEnPW9isb13e3NzU/fu3VMmk9Ho6Kjq9bqk51vt4qv7AE4H/xUCA6Bb19RONxDx98d/thNw89lhGOrixYtaWFhQJpM58Jqkr/jndxpb0k1PpVJRNptVpVKR67ra3t7WyMiIKpUKK94AMMB2d3f12WefRQnszs6O9vf3FYahyuWyrly5omvXrmlkZCT1pHB/f18bGxtdV7u7xZxO8c9OwH3fV71e1+uvv65Lly6pUChEjc3in2F+tz+/k/i4XNdVpVI58FyhUNDo6Kjy+byGhoZUKBS0u7sbbRMjpgKnj8QbGCCHJbLxGwg70CYlzOZxO7FeXl7W5cuXVavV5DiOgiDomHyb9/cy7qTHzOfm83nl83nlcrnUmusAAE6GSXQfPnyoTCajnZ0dNRoNZTIZDQ0NaXZ2Vm+99Zbm5uZOJCFsNBra3t5OLCM/LAFPipfxL/OacrmsxcVFLS0tRXvWTdIbj5tJ8dgeUyeu60bN1Gz5fF7j4+OqVqtyXVfZbDZKvAuFwomU8gPojsQbGABhGEYd2o9SKtdJp+NQcrmcJicndfHiRZ0/f76tiUy3FW/zvD2WpJuZ+Nj29/fVarXkeZ7Gx8e1srKilZWVY/4rAQBeBs+ePdPa2pp831cQBNre3pb0fFV2bm5Or732mi5fvnyiYzpsb7d5jf3dSJpwTjpObGZmRoVCQaVSScPDw11jqHksaZz2eOOP22OKKxQKWllZiXq0mOPTJicnNTs72/s/FoBUsMcbGAA7OztaX1+X43zbIfWwle5Ox5+Y70k3AbVaTfl8/sBr7EZspklLpz1qnRrCJD3veZ5KpZL29/e1sLCgt99+u6//bgCAk+X7vr755hutrq6q1Wrp6dOnyufzarVaymazunTpkn70ox+d+LjMlqpulWOHvb/TVxAECoJAs7OzKhaL2tnZifqWSO1HeMYTb7snSqfqsMP2gBulUkmvvPKKMpmMHjx4oHq9rkKhoMXFRV28eLH3fywAqSDxBgZEp9n6TjP0vTSGSWq2ZrNn8s177WZsRxmzpLZJA/N577zzDvvPAOCM+MMf/qCbN29qb29Pu7u7ajabajabqlarWlhYUK1WO/ExbW9v6969e1E8SprAlg5uz+r0c1LsDMNQc3Nz0Tnh9qS1XWpux9CjHNEZv77neVpeXlYul2t73vM8XblyRR988IGk53vt9/f3j/pPBiAFJN7AALEDdLcmZod9RtKX7/t67733NDk5KUkaHx/X9evX9dvf/jbq3Bpf7e40Q2/GER9vp5WFsbGx4/2DAABeKru7u1EjtadPn+qVV15Rq9WS67p65513tLi4eGpj66VHii1pS1XSxLVZ0TaJ9vj4uGZmZvTFF1+07QM/ysT1YX9DfKI87ic/+Yl+9rOfaXJykv3dwEuCxBsYIJ0S7sOaqcUfMz/Hm8PYK9zmc+ON1exzSDsl30ml7knjXlhYUC6XY7UbAM6AtbU1PXnyJOpJMjo6qkqlorW1NV27dk1jY2OneqyVXW5uHHWlOZ5w+76vVqul6elpLS8vq1wuq1gsanZ2tq2SzGzVin9et+snTQ6Y2HpY4r24uCjHcaKzvAGcPhJvYADs7e3p8ePHknqfqbcdtjfN3ByYxjDS86DebDYPJN12mXg8uU9KwLuNL5vNknQDwBlx//597ezsKAgCOY6jxcVFnTt3Tru7u1pcXFSpVDq1sSX1RnmRSjF7Uto0McvlclEZezabTeyTYq7XbfLavKZb87VqtdpxvI7jHChBB3D6SLyBAeD7vnZ3d6PfD2vAkhSszeNJNw6+76tSqWh4eFjZbFbS8yYt8/Pz+vjjj9uS7/jnmN/jpeWdxkeiDQBnS6vV0vr6up49e6ZGo6FCoaBqtapXX31V09PT8jxPY2NjUXw5ac1mUzs7O5I6dy7vFEO7lZqbVe8wDLW0tKRM5tvbanN0mlkRT4qh9ufGV+E7Jd3ma2hoiHgKDBhqT4AB0a0bazedOqnaJXC+7+vcuXNtNw2VSkVXrlxRLpc7UG4e/9zj/i0AgLNhY2NDQ0NDqlarWlpa0uuvv65XX31VExMTev3116MTM05DvV7X5ubmkSvGujUlNZ3MgyBQLpfT97///baJhXw+r1qtltik1P6503PdxtPLawG8fFjxBgbQUfeodWsIY5rCXL9+PbE76tzcnO7du9e15Nx8P2wcrHoDwNnjOI5effXV0x7GoQ7rk2LrlnSbxNv3fXmep2q1qvn5+ba91IVCQbVaTWtra23xM95Pxb5GL7HxRSa9AZwuVryBAdNrN/NOZW3x40+CIJDneVpZWTlQBuh5nhYWFg40YouvgB9WEpc0Npq9AABOWqe4dNjKd3yLlj15HQRB4nvi3c7jP5vXHcdhzdUAvHy48wUGwHFKzI1O+9N831ez2ZTjOJqenk78vEwmo6WlpQOJdvz6R70BMPvXmLUHAKStUzO1TnG0W0NS+/gwM4n8s5/97MBnVatV/fVf//WBhNsex3GSZ/MZy8vLVI8BA4bEGxgA5XJZMzMzR5qhlzo3U7NvGiTpr/7qrzom3ufPn48asNnvi5fLHSWJJuEGAJykThPW8aZqhzVTs79yuZzm5uZ04cKFxM91XbdjlVhS/5Wj/C0ABg+JNzAAXNdta3z2omeQmpsAz/M0NTWl5eXlxPeYI0lee+01ZTKZAzcena7Tq5mZmVPrcgsA+O4wsek4TdXMz/bxYc1mU+VyWW+++aZyuVzi52WzWb366qsHYu9xS83tvyFpMh7Ay43EGxgwSSVynfZ3H9ZUrVQq6fLly4ceSzI2Ntb2ub3s8e40bvtmo1wus9cbAHBijrpNK6mpmomBhUKh48S19HzSfGxsrGusPEoCbif/5XKZxBsYMNzxAt8B8WTZTrxfeeWVru91HCdKvJNKy4/bIIZycwDASenU0bybpK1VJvF2HEfFYlHj4+Md3++6rkZHRw80VTtq3Iw3SG21WiqVSj29F8DLg8QbGCDdbhJ6aRJj7/E2+9NmZmYOvea1a9eUz+cPNIixk3lzrU6SSvY8zzvsTwYA4MTZ8czEOd/3o+S7XC5rcnKy62fk83ldu3atrbdKUrl5r+z3LiwsUDEGDBj+iwUG0IuUypm92ZVKRbVaredrmv3YZrXc3uN9nHI33/d14cIF5fP5I78XAIBemUZnvZ4K0il2+r7flvi++eabPV0/Hn/NxPVhyXd8wtp8NZvNnq4L4OVC4g0MkOMkuJ1WvJeWlvTWW2/1/Dnz8/PKZrMHOrSaaxx1PJxBCgA4SS8SQ02y6/u+JiYmdOHCBc3Pz/d0zcXFxbZk207AjzIGScrlclpcXDzy3wHg9JF4AwPssK6snZJuczTYUezv77cl3aZsTmrf+90L9ncDAF5GSV3M7b3dvu+r0WgoCIKeE+eFhQU5jtP2Ob00XIs/dtyeKgBeDiTewIDq5SgU83O8I+vw8LBmZmZUrVZ7vt4Pf/hDVSqVxNK3o3JdV7VajY6sAIATEY9VSSeE2K9NSsDNVqsLFy5obm6u52vbndB7PQ0kKclutVryPO9I28QAvDxIvIEBl3SUmPkeD+zmBiKbzapYLKpQKPR8nWq1qmw223HPWTfxGwdJR7o2AAAvotsRnL00JzW/O46j+fn5I01cx4/kjF/jKFzXJX4CA4rEGxggnW4Oei05NzPuQ0NDxzqKpFKpdGyG1svNQ3z1HQCAk9BLU7VO7LiVz+c1Ojqqcrnc83WXlpbkeV7XpLvbyrsdM13X5SgxYECReAMD6KiJqx24fd/XysqKFhYWjnzdCxcuqFqtHmiO1svNDMk2AOC09Bp/4slufI93rVY70oqzaa5mjv6K7x0/ypjCMFQmk9H09HTP1wfw8iDxBs6gpJsGuwHa9PS0JiYmjv25x9mnZmNvNwDgtNmxqFtvFBP3giDQtWvXDj2/O4nneYnX7RUnggCDj8QbGCAvOmPfarU0Nzd37DK12dlZjY6OHhjPUTutOo6j5eVlEnAAwEspPtHsOI4ajYYajcaxkt/p6Wnlcrm2x8wqeKfrx8cSBIFc19XKysqRrw/g9JF4AwPisP1pSUlwvCmMfYzJcSTtNzvqnm1KzgEAJ62XhmqdVrqbzaZarZYWFhZ06dIlDQ0NHXsc8Yqx4xwLyqQ1MJhIvIEBYhLWTkl4t6TbnN39xhtvRB1Wj6parapSqRy4Rq+z/0ddGQcAoB9epGLMXvX2PK/rSvVhn2vH7iAIehqbeW+hUND4+PiRrw3g5UDiDQywpAQ8qWuqXaY2NDSkbDZ7rOsVCgVlMpkDCX78Ot1uIszqweTkJLP2AIATFT9W7LAjOe3E+9KlS8eOn/bnJ8XITnHTftzzvBe+PoDTQ+INDKhOZ5J2SoYdx1EYhiqVSm1NXo6q1wS72/uDINDY2BiJNwDgpdGtP0oYhlpaWjp24pu0Z7vXGGhe73keZ3gDA4zEGxggJlDbX/HnO90wSM9ny2dnZ1/oDFB79t8e02HJuP0cCTcA4LTEY2h8G1TSyR0v2k08m822lagfpzdKqVTiKDFggJF4AwMkfrPQba+3feNgmrHMz8+/0Gp30nXi10oac/w9hzWKAwCgn+KT1nYclQ5f7W42my90/fn5eRUKhbbr9XKetz2GcrmsmZmZFxoHgNND4g0MoG4JdzyIH6cJWjdjY2Oq1Wpt+8aDIOipBN1+juPEAAAnIR4zO8Ueu3O4+bKrulZWVpTJZI41hqTmor7v99xYzRxlBmBwkXgDA8Iu006atU9qeGa/97h7sruNxbCPKet19p5VbwBA2tbX13X//v0o7rium7hdq1OjUPO+xcXFvsQt+/2m9Lxb7HzRvioAXh7Hm7YDcKJMsDVB+rA93vGfzWz93NzcsWfr42NJKtOzv9vi4zRHqAAAkCazci21x62kEnN7ldte9e6HeGw0n5uU6Ce9t9VqqVwua2pqqi/jAXDyWPEGBkCr9fwcbul58t3LjL1h31zMz8+/cOJdLpejs7zN58ZvGOIrBfFxMGsPADgJSfu5O01a20m3Hcdc19Xk5OQLjyU+cd0t4U5aWfc8T/l8/oXHAeB0kHgDAyCTyUSdyO2kWzqYbCcl3ebxkZGRtq6qx5HP51UsFqPPtzuadyo3P+yscQAA0nBYeXhSIzXTkNQ877qu5ufnX6jMPGm7WK8r6va4AAwuEm9gAJRKJU1NTXU8SqzbjLlJjIMgOPb5ozY7+JtJAPN4L11aWfEGAJwke2+3LZ5wJ8Unk4QvLCy8UOJtnyhir7zb5e3xyjH7u/0+AIOJxBsYIN06sdrf7cft55aWll44+Q7DsG2PtuNWzMp3AAAe8klEQVQ4ymQybTcQzMoDAF4mZpuWdLAaLL6/O56ku66rc+fOvVDiOzc313acmOu6ymQy0cT4Ycn32NiYxsfHj319AKePxBsYEPZ+6U7PG/EVcdd1X3i23njllVf09ttvtyXY5sYhCAL5vh8dkdJpTCTmAICT8PXXX+vRo0fyPE+O40STz0lNzZLibDab7UtDs5WVFRWLxQON3sxxnHbin8S8BsDgIvEGBogpG5d664RqXuc4jubm5vqSeDuOo1KppOnp6ajhm/Q8mfZ9X2EYynXdaDzx8j37bwAAIE1mpdusYsdXlu0y8zjTG+Xdd9994XHY5e52THRdN0q+zTWTVr0BDD4Sb2DAJN00xJnHgyBQs9mUJC0tLfVtDIVCQbVarW2FwJTNmaS705fEPjUAwMkz+7U7JbmGiZ/5fF7T09M6d+5cX66/srKiWq3Wdo14I7ekCYFuK+EABgfneAMDxCS5dgDuttptVqFbrZYWFxf7lvDm83mNj49He73NuDqdjdrLyjwAAP22vb2t7e1tSe2rx/Hzuk3FlolTpqLLcRwVCoW+jMX0QwmCICp9N/vO46vc8YZvlUpFw8PDfRkHgNPBijcwIOKrxZ2S2KTni8WiRkdH+5Z4FwoFTU1NHdjDnXTjQOINADgtjUZD+/v7iRPW8cZq9nNBEKhQKPS1oVk+n5frum0ng9hbszrFzDAMlc/n+zYBAOB0kHgDAyTpuJPDEtpMJqOxsbG+jqNYLEal5vFGMUnlcYc1rwEAIA3xbuXx2GR+jh+/GYaharWaLl++3Lex1Go1DQ0NtSX59nWTyszN6juN1YDBR+INDJBGo9FxP5pd5m0nwfbv/WZuUMx1PM9raxQT//J9X81ms+08UwAA0rK5uaknT54krirbE8em5FuS6vW6fN/X0NCQZmZm+jaW2dlZjY6OHuh5Ep+4Nkm3mQCwjxsDMLhIvIEBsLW1pbW1tbaSNJvdLTWedBcKBS0uLvZ9TJlMRjMzMwqCoG1/t302qX2jY74HQRA9DwBAWsyktOd5ymS+bWtkxyxzDGb8jO/R0dG+76mOJ9b2Nc3j8co28zc8fvxY33zzTV/HA+BkkXgDA8BOrJMejyfdkqJztff29nTnzp2+j8ncQJgGa2Y80redY+3xtFotZbNZlctlSdKXX35J6RwAIDX37t3T1tZWlHjH45L0/Jzu+N5pe5W5n+7fv6+nT58qCAI1Gg1JapsYMFVjRqvV0t7eXsd7AACDhcQbGBDxJNZ+PCnxNgG80WikknjHr2eveGez2ej69uSAPUnAijcAIG12DDK/2/EqntD6vp+YoPeTfc14jLTH5ziO8vl8298CYHCReAMDxATlbnu94jcVklJZWQ6CQN98803b7Ly5ftLNQ/zG4uHDhyTfAIDUPHnyRPv7+9FEsH2EV3xft93kzHEcbW1taXd3t6/j+fLLL6O4acbSKVaaMvhOEwQABg+JNzAAms2mdnZ2DqxsxwO2/Zhh3mM3l+mHMAy1u7vbdTydbiharZa2t7dJvAEAqXn06JF2d3ej2GOXcsebkcb7o9TrddXr9b6OZ2NjI4rlvcRze8wk3sDgI/EGBkC9Xtfm5maUvMb3gRlJe8DNDP7q6mrfEl2zPy1eLtfpe1K5OQAAadrc3FSj0UisupJ0INHtNondD6YnSjxOdhpLLpdTJpMh6QbOCO5+gQERL4vr9jrz3f5aW1vr21j29vb04MGDrjcDnVa9Pc9TNpvt21gAAEiSVL4dT3Djr09zhfnJkyfa3d09MHmeFLftiQKSb+BsyBz+EgAvA/voEVtSo7KkFeY0zs5OWjkwP9tjsW8kzL41AADSZBJp83M8Ntk/x7do2ed894M5IiwpJsbHE/8b7LPHAQwuEm9ggCQl2PbPSbP7Jlm/fft234L21taWbt26dWDf3GEr4OZv4BgxAMBJOGyVO/46e2W8n9uiPv/8c+3t7bXFyaTSd/u7jcQbGHyUmgMDoFwua3p6Ovq928y9/bu9n/revXt9C9qO4yibzXbdM2ePo9P4AABIU1K87GWLVL9Luz3Pa+uuHl+J71QxJj1vZsqENTD4SLyBAeA4jjKZTJQ4d5r5TmoMY4J8P8u7G42GNjY2Evdw22OJs8vlpqenScABAKmJx8rDKrLMz2nEpnis7KXM3AiCQJVKRZVKpe/jAnBySLyBAWHPePdyU2CXnJvzQh8/ftyXBNz3fe3t7fV002CXmButVktDQ0Mk3gCAU5FUidVp5bkfnj17piAIEhu9dbqemTgIw1AjIyMaGxvr65gAnCwSb2AAeJ6nXC7XU6lZ0qy6CfJff/11X1e+k8rLe8VeNQDAyyatFe+NjQ0FQZC4l/ywZL/VaqlUKqlcLvd9XABODok3MABKpZJqtZrCMGwrnUsqOU8q/+73frWk8vJeG9iY8abRZR0AAKPXhmr274aJt2mMp5dYKbWvePe72RuAk8d/wcAAMMHX9/2ux4rYSXB8Nt0cj9LP8dhl7J1uIuzx2sepXLhwgZsIAEBqpqenNTQ0lBj7eomHaVRmxY8VO+x1ZgJgcnJS1Wq17+MBcHK46wUGwPDwsBYXF6NS815vBuyEuNVqKZvNptY0JunnODOGhYWFvo8BAACbnbge5TiuVqularWq8fHxvo/JPm2kl3GYZqRDQ0N9HwuAk0XiDQwAc3zX7OzsgSZl9ms6vdck3rdu3VKz2ezr2HpdNTA3QGEY9nWfOQAAnfi+L9/3e3ptr13QjyO+RazXMvNGo6EgCOiLApwBJN7AgMhms5qenj7SzL0d2D3PU7PZ7EvwLhQKqlarUTLdi1arpSAI+jYGAAC6mZiYUKlUOvJqd6vV0vDwcF9XmaemppTNZo+U3LdaLTWbTV28eFETExN9GwuA00HiDQwIx3FUKBSOXDJnuK6riYmJvjQ18zxP+Xy+4xjiNxLmdaZBzNLS0guPAQCAbgqFgjKZTPR7rxVaQRBobGxMIyMjfRvL7Oxsx7hpx8x4TxTf9zUyMqJCodC3sQA4HSTewAAxZdrdbh66NTqbmJjoS0Mz13W7Jt42u7wuDEN5nqfz58+/8BgAAOgmn8/Ldd3EE0GSmDjl+77GxsY0Ojrat7GMj4/L87yOk+dJE9Ym5scnEAAMJhJvYEA4jqNcLhcl3kdd8e7nvupSqaSZmZkDNzOdxLuzcpQYACBttVpNpVKp7WiweJ+U+FcQBPJ9X6Ojo31NvKXO1WBJ7GPEqtUqZ3gDZwCJNzAghoaG9N5776ler/dcbh6/obh48aJyudwLj6VcLmt+fj5qlnYYe6yFQkHnzp174TEAANAvduI9NTWlUqnU92tMTk6qXC4fGjdNbPV9XzMzM32J2wBOH4k3MGCCIDjS8Sjm+Ww2m8pY4tdJur6ZuS8Wi6rVan0fBwAAScbHxzU+Pt626t2J6Tbu+37PjUOPwlSsmZNGOo3BxMzd3V395V/+JUeJAWcEiTcwQBzH0ezsrCR1vYlISsir1Wpfj0bJZDKamZnpOflvNBpqNBp0NAcAnJh45Vc3zWZTQRDo7bffTmWSuFarqVKpdI3d9ph93ydmAmcIiTcwQBzHaUt2e2nSYp7vR1M1m+d5mpmZabtGEvsGYnR0VFeuXOnrOAAA6KRSqWh4eLhjYzU7jpry7vn5+VRWmU1s7iVmmsl1Ss2Bs4PEGxgw+Xw+KoPrFrzjq9tjY2N9HYdp9nZYp1j7piabzWp4eLiv4wAAoJNsNqtMJnNoqbmJU0NDQyqXy6l2Ee+1P8vw8LBKpVLfJ84BnA7+SwYGiOlu2mw2e76JMPvJZmdn+1pq7rrugW6xh40jk8lwFikA4MTkcjlls9mue7btOFUqlfoaK22FQiHqt3LYkWZBEOj8+fMk3cAZwn/NwADxPE9vvfVWWxlar0nvzMxMX28mPM/T1NRU1H31sHE4jqNyuaxqtdq3MQAA0I0pNbebgXaKheb4rrRMTEyoXC4fGrvDMFSj0dAbb7zB8ZvAGULiDQwgk+z2knybAJ5GgxbTmdV0ao2zE/+RkZG+n4kKAMBhzAryYfu7gyDQ/Px8KqeAxK/Z7Tmz17yXhnAABgeJNzCAZmdn5Xlex6CcdHPR7+Q7m81qZWXl0LO8TWLu+37bigMAAGmrVquq1WqJlVl2szPz/Pnz51NrZpZ0Pftx85z5WllZodQcOEP4rxkYQPPz81GzmCTxIJ5G4LZvIFzX7ToJ0Gg0tLy8rMuXL/d9HAAAdGLHqqTHzXNBEKjZbJ7ImJImrM14wjCU67qanp4+ME4Agy29lo0AUmVWj5MS3vgxY2kG7k4lfPY4fN+X4zjM3AMATlyhUNDExMSBmGhWnu3S7rm5udQ6msdX3TvFbvu4TgBnB3fBwAC6dOlStOLdrcxben6DYRqhpZmAd0r0zePlclmVSiW16wMAkMR13a77tk2cCoJAxWIx1Uliex+3YcfMIAgUhiGJN3AGkXgDA2hpaUmu60YrzUl71+xVb9d1NTk5eSIla0njyOfz0XmkAACcpEwmo3w+H/1ur3jbk8ZpH3dpXy9erWZXiIVhqNnZ2VTHAuDkkXgDAywpeCc9nmZX1E43EabjeavVUqVSSa1ZDQAA3RQKBY2NjSXGS/uUkImJiROrDDO/29czk+ie52l6epr93cAZwx5vYEBVq9WoJK2bkziOxD7WzL5W/NgzAABOWlJ5t3nc7O9uNBqHxtN+jSM+Ke44ThQn8/m8JicnUx0HgNPBijcwoKanp1UqlQ49lzTNGwnHcbS0tNR20xDn+75+8IMfUDYHADgVlUpFc3NzbfHQjpVBECgIAp0/fz7VVealpSWNj493nDQPw1DValXvvPNOamMAcHpIvIEBZW4UTFdxw066T2KV2R5HvIQuDEPt7+937HoOAEDaTDxKSnjtjuZmIjktZpK60zhMPCVeAmcTiTcwoK5evaqJiYmoEUtSkxbzeFpHo5hrJa2s24n31NSUhoaGUhsDAADdmLiYdH63iaOe553IOOyYbTd629/f5ygx4Awj8QYGVKVSkeu60Ux9UuJtOpqneYyXORu1U9OaSqWiYrGYavIPAMBhklaZ7W7iJ6HZbKrZbB5YWTcJueM4bR3YAZwdJN7AAMtms/I8r2323F7tNiVraR2R4jiOhoaGOnZWD8Mw9TNRAQA4qvhkdS/NSl/U9va29vb2ogTbXu2mCSlw9nE3DAywSqXSlvjGv+z912mwy8w7lbqzXw0AcJqCIFCj0Yh+T4pJJ5F4P3r0SLu7u8pkMm2Jt833fW1vb6c6DgCng8QbGGAjIyOqVCqJya2d+NrN1/rJ3Dh062hO0g0AOE3b29u6d+9e4gSxkXbSbZi46bquHMdpq1YLw1APHjzQhx9+eCJjAXCySLyBAWb2ipnEO77abZqb3b17N5Xr2/vikkrmms0mZeYAgFO1vb2t+/fvR3HK3tttJ75pN1dzHEee50Ur3ua6ZnLcPEdPFOBs4o4YGGC1Wk2Tk5MHSuTsm4qT2DcWX1EPw1B7e3uSpNXV1bYSPwAATtLOzo7W19eVy+UkfTtBbGu1Wrp582aq8dJOtiVFk+Zmy5brutrY2NB//Md/6Je//CUVY8AZQ+INDDBTpmZ/xRPwtMu9zaq2Wdk218pms8pkMsrlcvroo49079691MYAAECSGzdu6MaNGyoUCvI8L9oeZR+Dacq/0zzD21zHPsfbHGVmn9/tOI7q9bp+85vf6Fe/+tWJdVsHkD5qWYAzwNw82Al2UhlbmteOXysMQzUaDXmepz//+c/a29vT8vKyzp8/r1qtltp4AACQpM8//1x/+tOfotXu+CS1XS1m9lyn6cmTJ9rf35frugqC4MDEtaRo//fe3p4+/PBDNRoN/fjHP9bw8HCqYwOQPhJvYIDFZ+jNbHn85iEIAj158kSjo6N9v7FwXbdt9cCeBDA3E0+fPtXOzo4ePXqkhw8f6ty5c1pYWEhlPAAArK2t6eOPP9a9e/cUBEEUj3zfl+M40Sqz7/vyfV+e5+nZs2epVoh9/fXX2tnZkeM4ajQa0b5yz/MOxO9sNqtvvvlGH330kcIw1JUrVzQzM6NyuZza+ACki8QbGGD7+/uq1+sHSs7tsjnz3O3bt3X16tW+J7qe50U3MHaDmFarpXw+r52dHXmep3q9rq+++kqrq6uanJzUu+++q3PnzimXy2loaCjaeydJxWKxr2MEAHw3tFotPXz4UP/zP/+jP/7xjwqCQIVCQZlMJlppdhwnSrjr9brq9bpc19Xa2lqqiffu7q6azaZarVYUu+3tYHa380wmoyAI9OzZM/3yl7/U+vq6rl69qmq1qlwup9HRUUlSPp+niSkwIEi8gQH22Wef6ebNm22Jt82UsAVBoHv37unq1at9H4PrutENQqPRUDabjcYRhqHK5XJUcu66rvb393Xr1i198cUXyufzqtVqevvttzU3Nxetkl+8eLFt/JlMJvVuswCAwWV6mtTrdf3jP/6j7t69q0KhoGw2G8URu6lZs9lUo9GIEvFWq3XgyLF+m5qa0v3797Wzs9M2ZunbUvdcLifXddVoNKKJ61KppN///vf6+OOP5bqu5ubm9Itf/EKSdP78+Wj/uvkC8HIi8QYGVKvV0ubmpra2tjQxMSHXddtuGFzXjY4maTQaqZZ0u66rer0e7ZszKwtm1l76dibfHlMYhlpfX9c//dM/RXvt7FX7paUlXbhwQT/84Q/1xhtvUJYOAIjYMe/Bgwf6z//8T/3Xf/2X8vm8isWicrmccrlc1OzTxBiT8Mabq51E0hrf023HZ/PdvGZkZES7u7vR5ID5G7766iv93d/9XfRvUKvV9MYbb+jatWs6f/48sRJ4SZF4AwPqgw8+0KNHj5TP56PZeunbrqnSt83OwjDUrVu3UpvJT2rkZkrHzc1MvJTOrEA0m035vq9Go6H9/X0FQaD9/X0VCgU9fvxYW1tb+sMf/qCVlRX97d/+bSrjBwAMns8//1x37tzR7373O62urkp6XnptVrpNwh0/N9sk4PYWqUKhoNnZ2VTLtt955x1tbm7q/v37kp6veHuepzAM27aGmUTbnrx2XTcqSzeVZeb3R48e6d///d/1v//7v/rpT3+qv/mbv0ntbwBwfCTewAD6h3/4B62traleryubzUaz96aUzv4y7ty5k2ribSf80rc3FOY5s9ptfo4f7dJsNhUEQfRYpVJRvV5XsVjU1NSU5ufntbq6qqWlpVT+BgDAy293d1fvv/++fv3rX6vZbKrZbKper0cxx1Rb2fFHUtsWKNNUzTQ3y+VyymQyeu+991JNvF3X1fz8vFZWVnT37t2ousuUvZvX2Mm4KT3f39+X4zjKZrPRPnGTqJvtXq1WS4VCIbXxA3gxJN7AgGg2m7p//75+9atf6fbt2wrDMJrRNyvK8aNSTODOZrNqNBpaX1/X9PR0NIP+ohzH0dTUVHSTYB6zG9iY7rF20zezD81uCmPGa1YoarWawjDUhQsXNDMzo2Kx2LdxAwAGy9ramj799FPdunVL6+vrevjwYdvErtniZFa47ePBTHwxybaJjSam5PN5nT9/XsvLy6mXaS8uLiqbzWpubi66vomR5hjOJ0+e6ObNm8pkMm0r9yZB39nZiVbth4aGorO/wzDU5uZmquMHcHzcxQID4NmzZ/rqq6/0pz/9Sf/3f/+nXC6nQqEQzdLHS73NCrJZATAz4nfv3tXk5GRfE++lpSWNjIxoc3PzwNEo9gy8OQJla2tLu7u70c2SJI2OjkYl88PDwxobG9Pw8LAcx9Hs7CznlwLAd9zq6qp+/etf68GDB23xw2xdMv1Dcrlc20q3KSfPZrOq1WrR57VaLZVKJZXLZQ0PD2tubk5DQ0Op/x2VSkW5XE5jY2PRnnJ7Nb7RaGhjY0PS8+PHTHPSbDYbVbaZpnBBECiXyymfz6tSqTBBDbzk+K8TeMnt7Ozo1q1b+u///m/dunUr2rtmGsbYzWDss7TtMjrT8Ozu3bt67bXX+jY2s+J96dIlffHFF1EH1nw+H90klMtljY+Pa2JiQo7j6JtvvtHGxkZbid/c3JyGh4eVyWQ0MTGh6enpvo0RADDYtra2dPfuXd2/fz+KaXbibSZ8zeqwYSqtyuWypqamdPHixeixIAg0NjamarWqkZGRE/178vm8qtVqx+drtZqGh4f1xz/+Uaurq1F5vD3BYPdUMRPWk5OTWlhYOKk/A8ARkXgDL6l6vS7f9/XJJ5/oww8/1M2bN1UoFKLGMWa12+6Qandsje8TcxxH9+7diwJ1P/385z9XLpfTV199FXVYrdVqajQaun79+oEZ+DAMtba2JkmamZlpO8MbAADbp59+qnv37kWTtfYeZ7OdysREUz3l+74cx9H4+LjeeOMNvfvuu6f9Z/Qsm81qaWlJS0tLev/997WxsaHHjx9rd3dXrVZL5XJZnuepWCwqDENNTExoZmZG8/PzmpmZOe3hA+iAxBt4idjNz/7lX/5FN27c0MbGhlzXVbFYjPZEm/1edtLdqXGa3UU8zaYxP/nJT3p+reu6NEkDAPTk0aNH2traiiaWx8bGtLGxoUajEe2DLpVKKhQKUffvRqOh119/XT/84Q81Pz9/2n/Csb333nttv+/v72t1dVUPHz6U53na2NjQm2++qcXFxVMaIYBekXgDL5Ht7W3967/+q9bW1qIGamZ125Rv23u6u7Gbx9iz/wAADJInT56oVCppcXFRDx8+1N7enorFoqrVqjKZTBQnTdK9v7+vt956Sz/60Y80Ojp62sPvq3w+r0uXLunixYvRY8R2YDCQeAMvgU8//VQfffSRPvvsM+3u7kb7tE3pnEmek1at7S7mZt+apLYjRcxrrl+/zlEjAICB8vbbb2ttbU0PHz7U0NBQVG4uKToaTJJGRkY0PT2tt99+O9qWddaSUvP3nLW/C/guIPEGTtlnn32mGzdu6JNPPtGzZ8/ajkYxs/imcYzZt20fF2bv6TZnekqKEvRCoaCFhQUNDQ3p4sWLbY1nAAB42S0sLGhyclJ7e3va3t7WJ598opGREW1vb6ter2t2djbqF1IsFjU6OkpiCuClQ+INnKJbt27pxo0bunnzZpR0m5uFYrGoUqkUlc9Jis7K9n1fk5OTGh4eVqlUUrFYlOM4evr0afTZQ0NDyufzyuVympqaUqlUio7oAgBgUJRKJZVKJUlSo9GQ9PxYrr29Pfm+r1qtpvHx8dMcIgAcisQbOEW/+c1v9Pvf/17b29tRQuy6rnK5nMrlctS5VFJ0PvfQ0JDK5bIuXryoarWqiYkJjY6OKgxD3b9/P/rs8fHx6EYFAICzIJfL6cqVK6c9DAA4MhJv4BSY8zd/97vfaXt7W9K3q9mlUkljY2MqFottZeGO46jZbOr111/X9evX287vlp4n7IPcuRUAAAA4q0i8gVPy+eefq9FoqFgsKggCeZ4XlY4XCgVlMhm1Wi01m02Njo7q/PnzkqQf/ehHlIsDAAAAA4TEGzgFYRjqgw8+0PT0tIIgUDabjfZju66rvb09Xb58WXNzc5qdnW3bm03SDQAAAAwWEm/gFDiOo+XlZU1NTenZs2fyfV+5XE6VSkXFYlHXrl1TNptt62YOAAAAYDA55uihU3BqFwZOW6vV0pMnT6LzR1utlhzHUSaTked5GhsbY2UbAAAA6L9Tuckm8QYAAAAAfFecSuJN/SoAAAAAACki8QYAAAAAIEUk3gAAAAAApIjEGwAAAACAFJF4AwAAAACQIhJvAAAAAABSROINAAAAAECKSLwBAAAAAEgRiTcAAAAAACki8QYAAAAAIEUk3gAAAAAApIjEGwAAAACAFJF4AwAAAACQIhJvAAAAAABSROINAAAAAECKSLwBAAAAAEgRiTcAAAAAACki8QYAAAAAIEUk3gAAAAAApIjEGwAAAACAFJF4AwAAAACQIhJvAAAAAABSROINAAAAAECKSLwBAAAAAEgRiTcAAAAAACki8QYAAAAAIEUk3gAAAAAApIjEGwAAAACAFJF4AwAAAACQIhJvAAAAAABSROINAAAAAECKSLwBAAAAAEgRiTcAAAAAACki8QYAAAAAIEUk3gAAAAAApIjEGwAAAACAFJF4AwAAAACQIhJvAAAAAABSROINAAAAAECKSLwBAAAAAEgRiTcAAAAAACki8QYAAAAAIEUk3gAAAAAApIjEGwAAAACAFJF4AwAAAACQIhJvAAAAAABSlDnFazuneG0AAAAAAE4EK94AAAAAAKSIxBsAAAAAgBSReAMAAAAAkCISbwAAAAAAUkTiDQAAAABAiki8AQAAAABIEYk3AAAAAAApIvEGAAAAACBFJN4AAAAAAKSIxBsAAAAAgBSReAMAAAAAkCISbwAAAAAAUkTiDQAAAABAiki8AQAAAABIEYk3AAAAAAApIvEGAAAAACBFJN4AAAAAAKSIxBsAAAAAgBSReAMAAAAAkCISbwAAAAAAUkTiDQAAAABAiki8AQAAAABIEYk3AAAAAAApIvEGAAAAACBFJN4AAAAAAKSIxBsAAAAAgBSReAMAAAAAkCISbwAAAAAAUkTiDQAAAABAiv4fbdSyBBQYZS4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1200x1200 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images = render_smpl_params(bm, {'pose_body':all_pose_body}).reshape(1,2,1,400,400,3)\n",
    "img = imagearray2file(images)\n",
    "show_image(img[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The closer above two bodies look similar to each other, the more successful is VPoser is in reconstructing the original data space body pose parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate novel body poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poZ_body_sample.shape torch.Size([1, 32])\n",
      "pose_body.shape torch.Size([1, 63])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x1200 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Sample a 32 dimensional vector from a Normal distribution\n",
    "poZ_body_sample = torch.from_numpy(np.random.randn(1,32).astype(np.float32)).to('cuda')\n",
    "pose_body = vp.decode(poZ_body_sample)['pose_body'].contiguous().view(-1, 63)\n",
    "\n",
    "print('poZ_body_sample.shape', poZ_body_sample.shape)\n",
    "print('pose_body.shape', pose_body.shape)\n",
    "\n",
    "images = render_smpl_params(bm, {'pose_body':pose_body}).reshape(1,1,1,400,400,3)\n",
    "img = imagearray2file(images)\n",
    "show_image(img[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we drawn a sample from a 32 dimensional Normal distribution and decoded its value to a full 63 dimensional SMPL body pose vector. The generated image shows the corresponding rendered body. Fo an advanced tutorial on generating poses with VPoser refer to [Link](notebooks/vposer_sampling.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

D:\Projects\smplify-x\src\human-body-prior\tutorials\vposer_sampling.ipynb
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling Novel Body Poses from VPoser\n",
    "![alt text](https://github.com/nghorbani/human_body_prior/raw/845ad715b82bbd1a4e4772b23a0b7e4cd92203c6/github_data/latent_interpolation_1.gif \"Interpolation of novel poses on the smoother VPoser latent space.\")\n",
    "\n",
    "You can use VPoser to produce novel random poses. \n",
    "This generative model can be potentially used as a synthetic data generator to train other models.\n",
    "\n",
    "For this task, you would first need to obtain a trained VPoser from https://smpl-x.is.tue.mpg.de/downloads .\n",
    "Put the downloaded model in a folder, here we assume \n",
    "\n",
    "'GITHUB_CLONE_ROOT/human_body_prior/support_data/dowloads/vposer_vXX', \n",
    "\n",
    "and afterwards you can use the following code snippet to sample new poses: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from body_visualizer.tools.vis_tools import render_smpl_params\n",
    "from body_visualizer.tools.vis_tools import imagearray2file, show_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../support_data/dowloads/vposer_v2_05\n",
      "../support_data/dowloads/models/smplx/neutral/model.npz\n"
     ]
    }
   ],
   "source": [
    "#This tutorial requires 'vposer_v2_05'\n",
    "\n",
    "from os import path as osp\n",
    "support_dir = '../support_data/dowloads'\n",
    "expr_dir = osp.join(support_dir,'vposer_v2_05') #'TRAINED_MODEL_DIRECTORY'  in this directory the trained model along with the model code exist\n",
    "bm_fname =  osp.join(support_dir,'models/smplx/neutral/model.npz')#'PATH_TO_SMPLX_model.npz'  obtain from https://smpl-x.is.tue.mpg.de/downloads\n",
    "\n",
    "print(expr_dir)\n",
    "print(bm_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading SMPLx Body Model\n",
    "from human_body_prior.body_model.body_model import BodyModel\n",
    "\n",
    "bm = BodyModel(bm_fname=bm_fname).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Loading VPoser Body Pose Prior\n",
    "from human_body_prior.tools.model_loader import load_model\n",
    "from human_body_prior.models.vposer_model import VPoser\n",
    "vp, ps = load_model(expr_dir, model_code=VPoser,\n",
    "                              remove_words_in_model_weights='vp_model.',\n",
    "                              disable_grad=True)\n",
    "vp = vp.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x1200 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_poses = 9 # number of body poses in each batch\n",
    "\n",
    "sampled_pose_body = vp.sample_poses(num_poses=num_poses)['pose_body'].contiguous().view(num_poses, -1) # will a generate Nx1x21x3 tensor of body poses \n",
    "images = render_smpl_params(bm, {'pose_body':sampled_pose_body}).reshape(3,3,1,400,400,3)\n",
    "img = imagearray2file(images)\n",
    "show_image(np.array(img[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each column of the first row of the above image are the novel poses, and the following rows are varied rotations for\n",
    "demonstration purpose so that you can see the generated body from different angles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VPoser Latent Space Interpolation\n",
    "You can go one step further and investigate the smoothness of the latent space of VPoser.\n",
    "To do this you sample two poses poZ<sub>1</sub> and poZ<sub>2</sub> and then use &alpha; &in; [0.,1.] to change the influence \n",
    "of each on the interpolated poZ<sub>inp</sub>, e.g.\n",
    " \n",
    "poZ<sub>inp</sub> = &alpha; * poZ<sub>1</sub> + (1-&alpha;)* poZ<sub>2</sub>.\n",
    "\n",
    "The header animations show the video of body poses that are result of decoding corresponding varying poZ<sub>inp</sub> values.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

D:\Projects\smplify-x\src\human-body-prior\tutorials\__init__.py
# -*- coding: utf-8 -*-
#
# Copyright (C) 2019 Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG),
# acting on behalf of its Max Planck Institute for Intelligent Systems and the
# Max Planck Institute for Biological Cybernetics. All rights reserved.
#
# Max-Planck-Gesellschaft zur Förderung der Wissenschaften e.V. (MPG) is holder of all proprietary rights
# on this computer program. You can only use this computer program if you have closed a license agreement
# with MPG or you get the right to use the computer program from someone who is authorized to grant you that right.
# Any use of the computer program without a valid license is prohibited and liable to prosecution.
# Contact: ps-license@tuebingen.mpg.de
#
#
# If you use this code in a research publication please consider citing the following:
#
# Expressive Body Capture: 3D Hands, Face, and Body from a Single Image <https://arxiv.org/abs/1904.05866>
#
#
# Code Developed by:
# Nima Ghorbani <https://nghorbani.github.io/>
#
# 2018.01.02


D:\Projects\smplify-x\src\human-body-prior\__init__.py


